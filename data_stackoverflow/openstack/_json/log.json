[
    {
        "url": "https://stackoverflow.com/questions/10424074",
        "text": "trace nova traceback most recent call last : trace nova file line in module trace nova service.wait trace nova file line in wait trace nova _launcher.wait trace nova file line in wait trace nova service.wait trace nova file line in wait trace nova return self._exit_event.wait trace nova file line in wait trace nova return hubs.get_hub .switch trace nova file line in switch trace nova return self.greenlet.switch trace nova file line in main trace nova result function trace nova file line in run_server trace nova server.start trace nova file line in start trace nova self.manager.init_host trace nova file line in init_host trace nova self.reboot_instance context, instance trace nova file line in wrapped trace nova return f trace nova file line in decorated_function trace nova function self, context, instance_uuid, trace nova file line in decorated_function trace nova sys.exc_info trace nova file line in __exit__ trace nova self.gen.next trace nova file line in decorated_function trace nova return function self, context, instance_uuid, trace nova file line in reboot_instance trace nova network_info self._get_instance_nw_info context, instance trace nova file line in _get_instance_nw_info trace nova instance trace nova file line in get_instance_nw_info trace nova args trace nova file line in call trace nova return _get_impl .call context, topic, msg, timeout trace nova file line in call trace nova return rpc_amqp.call context, topic, msg, timeout, connection.pool trace nova file line in call trace nova rv list rv trace nova file line in __iter__ trace nova raise result trace nova remoteerror: remote error: networknotfound network could not be found. trace nova u traceback most recent call last , u file line in _process_data\\n rval node_func context=ctxt, \\n u file line in return func self, context, , u file line in get_instance_nw_info\\n network self._get_network_by_id context, vif u file line in _get_network_by_id\\n network_id \\n u file line in _get_network_by_id\\n return self.db.network_get context, network_id \\n u file line in network_get\\n return impl.network_get context, network_id \\n u file line in return f , u file line in network_get\\n raise exception.networknotfound network_id=network_id \\n u networknotfound: network could not be found.\\n"
    },
    {
        "url": "https://stackoverflow.com/questions/11161868",
        "text": "nova.rpc.common reconnecting to amqp server on localhost:5672 nova.rpc.common amqp server on localhost:5672 is unreachable: errno econnrefused. trying again in seconds. trace nova.rpc.common traceback most recent call last : trace nova.rpc.common file line in reconnect trace nova.rpc.common self._connect trace nova.rpc.common file line in _connect trace nova.rpc.common self.connection.connect trace nova.rpc.common file line in connect trace nova.rpc.common return self.connection trace nova.rpc.common file line in connection trace nova.rpc.common self._connection self._establish_connection trace nova.rpc.common file line in _establish_connection trace nova.rpc.common conn self.transport.establish_connection trace nova.rpc.common file line in establish_connection trace nova.rpc.common connect_timeout=conninfo.connect_timeout trace nova.rpc.common file line in __init__ trace nova.rpc.common super connection, self .__init__ trace nova.rpc.common file line in __init__ trace nova.rpc.common self.transport create_transport host, connect_timeout, ssl trace nova.rpc.common file line in create_transport trace nova.rpc.common return tcptransport host, connect_timeout trace nova.rpc.common file line in __init__ trace nova.rpc.common raise socket.error, msg trace nova.rpc.common error: errno econnrefused trace nova.rpc.common"
    },
    {
        "url": "https://stackoverflow.com/questions/12293008",
        "text": "exit code: stdout: stderr: bad argument occurred at line: restore h or iptables restore help for more information.\\n trace nova traceback most recent call last : trace nova file line in module trace nova service.wait trace nova file line in wait trace nova _launcher.wait trace nova file line in wait trace nova service.wait trace nova file line in wait trace nova return self._exit_event.wait trace nova file line in wait trace nova return hubs.get_hub .switch trace nova file line in switch trace nova return self.greenlet.switch trace nova file line in main trace nova result function trace nova file line in run_server trace nova server.start trace nova file line in start trace nova self.manager.init_host trace nova file line in init_host trace nova self.l3driver.initialize trace nova file line in initialize trace nova linux_net.init_host trace nova file line in init_host trace nova add_snat_rule ip_range trace nova file line in add_snat_rule trace nova iptables_manager.apply trace nova file line in inner trace nova retval f trace nova file line in apply trace nova trace nova file line in _execute trace nova return utils.execute *cmd, trace nova file line in execute trace nova .join cmd trace nova processexecutionerror: unexpected while running command. trace nova command: sudo nova rootwrap iptables restore trace nova exit code: trace nova stdout: trace nova stderr: bad argument occurred at line: restore h or iptables restore help for more information.\\n trace nova"
    },
    {
        "url": "https://stackoverflow.com/questions/12441561",
        "text": "debug keystone.common.wsgi request environ debug keystone.common.wsgi script_name debug keystone.common.wsgi webob.adhoc_attrs response at ok debug keystone.common.wsgi request_method post debug keystone.common.wsgi path_info tokens debug keystone.common.wsgi server_protocol http debug keystone.common.wsgi content_length debug keystone.common.wsgi eventlet.posthooks debug keystone.common.wsgi raw_path_info debug keystone.common.wsgi remote_addr swift_proxy_ip debug keystone.common.wsgi eventlet.input eventlet.wsgi.input object at debug keystone.common.wsgi wsgi.url_scheme http debug keystone.common.wsgi server_port debug keystone.common.wsgi wsgi.input cstringio.stringi object at debug keystone.common.wsgi openstack.context none, false debug keystone.common.wsgi http_host keystone_host_ip:5000 debug keystone.common.wsgi wsgi.multithread true debug keystone.common.wsgi openstack.params u u u u u u u u debug keystone.common.wsgi wsgi.version debug keystone.common.wsgi server_name swift_proxy_ip debug keystone.common.wsgi gateway_interface cgi debug keystone.common.wsgi wsgi.run_once false debug keystone.common.wsgi wsgi.errors open file stderr , mode at debug keystone.common.wsgi wsgi.multiprocess false debug keystone.common.wsgi webob.is_body_seekable true debug keystone.common.wsgi content_type application json debug keystone.common.wsgi http_accept_encoding identity debug keystone.common.wsgi debug keystone.common.wsgi request body debug keystone.common.wsgi debug keystone.common.wsgi debug routes.middleware matched post tokens debug routes.middleware route path: defaults: keystone.service.publicrouter object at debug routes.middleware match dict: keystone.service.publicrouter object at , debug routes.middleware matched post tokens debug routes.middleware route path: defaults: u keystone.service.tokencontroller object at debug routes.middleware match dict: u keystone.service.tokencontroller object at debug keystone.common.wsgi arg_dict: debug root token_ref datetime.datetime 2012, , u u u true, u u u u u none, u true, u u u u u debug keystone.common.wsgi response headers debug keystone.common.wsgi content type application json debug keystone.common.wsgi vary x auth token debug keystone.common.wsgi content length debug keystone.common.wsgi debug keystone.common.wsgi response body debug keystone.common.wsgi null, true, , , debug eventlet.wsgi.server host_ip post http"
    },
    {
        "url": "https://stackoverflow.com/questions/14442005",
        "text": "trace nova.manager traceback most recent call last : trace nova.manager file line in periodic_tasks trace nova.manager task self, context trace nova.manager file line in _heal_instance_info_cache trace nova.manager context, self.host trace nova.manager file line in instance_get_all_by_host trace nova.manager return impl.instance_get_all_by_host context, host trace nova.manager file line in wrapper trace nova.manager return f trace nova.manager file line in instance_get_all_by_host trace nova.manager return _instance_get_all_query context .filter_by host=host .all trace nova.manager file line in all trace nova.manager return list self trace nova.manager file line in __iter__ trace nova.manager return self._execute_and_instances context trace nova.manager file line in _execute_and_instances trace nova.manager result conn.execute querycontext.statement, self._params trace nova.manager file line in execute trace nova.manager params trace nova.manager file line in _execute_clauseelement trace nova.manager compiled_sql, distilled_params trace nova.manager file line in _execute_context trace nova.manager context trace nova.manager file line in _execute_context trace nova.manager context trace nova.manager file line in do_execute trace nova.manager cursor.execute statement, parameters trace nova.manager operationalerror: operationalerror socket not open"
    },
    {
        "url": "https://stackoverflow.com/questions/17681226",
        "text": "screen s stack p key x stuff cd config file log config d debug touch echo waiting for keystone to start... waiting for keystone to start... timeout sh c while ! http_proxy= curl s do sleep done die keystone did not start local set xtrace . stack.sh:311 keystone did not start"
    },
    {
        "url": "https://stackoverflow.com/questions/17831758",
        "text": "tail f wed jul file line in connect wed jul return dialect.connect *cargs, wed jul wed jul file line in connect wed jul return self.dbapi.connect *cargs, wed jul wed jul operationalerror: operationalerror unable to open database file none none wed jul wed jul client mod_wsgi : exception occurred processing wsgi script wed jul client typeerror: expected byte string object for header value, value of type int found"
    },
    {
        "url": "https://stackoverflow.com/questions/18806930",
        "text": "sat sep debug:openstack_auth.backend:beginning user authentication for user sat sep req: curl i x post h content type: application h user agent: python keystoneclient sat sep body: sat sep sat sep starting new http connection : sat sep debug:openstack_auth.backend:authorization failed: attribute of objects http unable to establish connection to sat sep warning:openstack_auth.forms:login failed for user"
    },
    {
        "url": "https://stackoverflow.com/questions/19002218",
        "text": "cinder.volume.manager none none updating volume status in systemexit: none none critical cinder an integer is required trace cinder traceback most recent call last : trace cinder file line in trace cinder launcher.wait trace cinder file line in wait trace cinder self._start_child wrap trace cinder file line in _start_child trace cinder os._exit status trace cinder typeerror: an integer is required trace cinder cinder.service child exited with status class , delattr , dict , doc , format , getattribute , hash , init , module , new , reduce , reduce_ex , repr , setattr , sizeof , str , subclasshook , weakref , set cinder.service wait wrap.failed false cinder.service forking too fast, sleeping"
    },
    {
        "url": "https://stackoverflow.com/questions/19103269",
        "text": "bosh upload stemcell ... unable to connect to the openstack compute api. check task debug log for details. ... e, t task:1 : no route to host connect errno::ehostunreach excon::errors::socketerror"
    },
    {
        "url": "https://stackoverflow.com/questions/19324066",
        "text": "sudo chown r stack eq cd git checkout requirements.txt test requirements.txt setup.py error: pathspec did not match any file s known to git. error: pathspec did not match any file s known to git. failed local jobs p kill set xtrace stack.sh failed: full log in"
    },
    {
        "url": "https://stackoverflow.com/questions/19668277",
        "text": "e, t : unable to connect to the openstack compute api. check task debug log for details. i, t : no existing deployments found will save to i, t : loading existing deployment data from: i, t : bosh registry is ready on port i, t : loading yaml from e, t : expected actual unauthorized response excon::response:0x0000000382cdc0 to authenticate user with credentials provided.\\ tue, oct gmt , accept, accept encoding, x auth token , repose repose :remote_ip= to authenticate user with credentials provided.\\ tue, oct gmt , accept, accept encoding, x auth token , repose repose excon::errors::unauthorized levels in create_stemcell in create_stemcell"
    },
    {
        "url": "https://stackoverflow.com/questions/20614829",
        "text": "passwd: user does not exist run parts: exited with return code cc_scripts_user.py : failed to run parts in __init__.py : traceback most recent call last : file line in run_cc_modules cc.handle name, run_args, file line in handle name, self.cfg, self.cloud, cloudinit.log, args file line in sem_and_run func file line in handle util.runparts runparts_path file line in runparts raise subprocess.calledprocesserror sp.returncode,cmd calledprocesserror: command returned non zero exit status __init__.py : config handling of scripts user, none, failed"
    },
    {
        "url": "https://stackoverflow.com/questions/21251303",
        "text": "nova.wsgi stopping wsgi server nova.openstack.common.service caught sigherm stopping children nova.openstack.common.service child exited with status"
    },
    {
        "url": "https://stackoverflow.com/questions/22616974",
        "text": "nova.osapi_compute.wsgi.server wsgi starting up critical nova errno address already in use nova.openstack.common.service parent process has died unexpectedly, exiting nova.openstack.common.service parent process has died unexpectedly, exiting nova.wsgi stopping wsgi server. nova.wsgi stopping wsgi server."
    },
    {
        "url": "https://stackoverflow.com/questions/22948334",
        "text": "tue apr client mod_wsgi : exception occurred processing wsgi script tue apr client typeerror: expected byte string object for header value, value of type int found"
    },
    {
        "url": "https://stackoverflow.com/questions/23195976",
        "text": "nova.virt.driver unable to load the virtualization driver trace nova.virt.driver traceback most recent call last : trace nova.virt.driver file line in load_compute_driver trace nova.virt.driver virtapi trace nova.virt.driver file line in import_object_ns trace nova.virt.driver return import_class import_str *args, trace nova.virt.driver file line in import_class trace nova.virt.driver __import__ mod_str trace nova.virt.driver importerror: no module named docker trace nova.virt.driver"
    },
    {
        "url": "https://stackoverflow.com/questions/23479120",
        "text": "message: expected actual bad request response excon::response:0x00000001eed698 get_version_v2 got an unexpected keyword argument bad mon, may gmt , :remote_ip= openstackserverip , :local_port= :local_address= get_version_v2 got an unexpected keyword argument bad mon, may gmt , openstackserverip ,"
    },
    {
        "url": "https://stackoverflow.com/questions/23911862",
        "text": "may account server: __call__ with put : most recent call last file line in __call__#012 res method file line in return func file packages line in _timing_stats#012 resp func ctrl, file line in req.headers file packages line in put_container#012 raise databaseconnectionerror self.db_file, db doesn t exist db connection doesn t exist"
    },
    {
        "url": "https://stackoverflow.com/questions/24369303",
        "text": "starting new http connection : debug post http _make_request starting new http connection : debug get http _make_request keystoneclient.middleware.auth_token verify error: command returned non zero exit status debug keystoneclient.middleware.auth_token token validation failure. _validate_user_token trace keystoneclient.middleware.auth_token traceback most recent call last : trace keystoneclient.middleware.auth_token file line in _validate_user_token trace keystoneclient.middleware.auth_token verified self.verify_signed_token user_token, token_ids trace keystoneclient.middleware.auth_token file line in verify_signed_token trace keystoneclient.middleware.auth_token if self.is_signed_token_revoked token_ids : trace keystoneclient.middleware.auth_token file line in is_signed_token_revoked trace keystoneclient.middleware.auth_token if self._is_token_id_in_revoked_list token_id : trace keystoneclient.middleware.auth_token file line in _is_token_id_in_revoked_list trace keystoneclient.middleware.auth_token revocation_list self.token_revocation_list trace keystoneclient.middleware.auth_token file line in token_revocation_list trace keystoneclient.middleware.auth_token self.token_revocation_list self.fetch_revocation_list trace keystoneclient.middleware.auth_token file line in fetch_revocation_list trace keystoneclient.middleware.auth_token return self.cms_verify data trace keystoneclient.middleware.auth_token file line in cms_verify trace keystoneclient.middleware.auth_token return verify trace keystoneclient.middleware.auth_token file line in verify trace keystoneclient.middleware.auth_token .decode trace keystoneclient.middleware.auth_token file line in cms_verify trace keystoneclient.middleware.auth_token raise e trace keystoneclient.middleware.auth_token calledprocesserror: command returned non zero exit status trace keystoneclient.middleware.auth_token debug keystoneclient.middleware.auth_token marking token as unauthorized in cache _cache_store_invalid keystoneclient.middleware.auth_token authorization failed for token keystoneclient.middleware.auth_"
    },
    {
        "url": "https://stackoverflow.com/questions/24530833",
        "text": "main debug jclouds.wire` html not found p the resource could not be found. p html null main debug o.j.rest.internal.invokehttpmethod invoking container:list main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request get http main debug jclouds.headers get http main debug jclouds.headers accept: application json main debug jclouds.headers x auth token:"
    },
    {
        "url": "https://stackoverflow.com/questions/24652932",
        "text": "nova.scheduler.filter_scheduler admin demo instance: from last host: node : u traceback most recent call last , u file line in _build_instance\\n set_access_ip=set_access_ip \\n u file line in decorated_function\\n return function self, context, , u file line in log.exception instance failed to , , u file line in exit six.reraise self.type_, self.value, self.tb \\n u file line in block_device_info \\n , u file line in write_to_disk=true \\n u file line in to_xml\\n disk_info, rescue, block_device_info \\n , u file line in get_guest_config\\n flavor , u file line in get_config\\n _ unexpected vif_type=%s vif_type \\n u novaexception: unexpected vif_type=binding_failed\\n"
    },
    {
        "url": "https://stackoverflow.com/questions/24806438",
        "text": "jul object server container update failed with saving for async update later : timeout txn: jul swift with object server re: trying to delete timeout txn: jul container replicator reading http response from : timeout"
    },
    {
        "url": "https://stackoverflow.com/questions/24806438",
        "text": "jul controller proxy server with object server re: trying to get final status of put to timeout txn: jul controller proxy server with object server re: expect: continue on connectiontimeout txn:"
    },
    {
        "url": "https://stackoverflow.com/questions/25832153",
        "text": ". stack.sh warning: this script has not been tested on trusty apt::acquire::retries using mysql database backend echo_summary installing package prerequisites t true ... install_package bridge utils pylint python setuptools screen unzip wget psmisc git lsof openssh server openssl vim nox locate python virtualenv python iputils ping wget curl tcpdump tar gcc dev python dev python eventlet python routes python greenlet python sqlalchemy python wsgiref python pastedeploy python xattr python python dev python lxml python pastescript python pastedeploy python paste python python sqlalchemy python mysqldb python webob python greenlet python routes dev dev python dateutil msgpack python dnsmasq base dnsmasq utils kpartx parted iputils arping python mysqldb python xattr python lxml gawk iptables ebtables sudo libjs jquery tablesorter vlan curl genisoimage socat python mox python paste python migrate python gflags python greenlet python python routes python numpy python pastedeploy python eventlet python cheetah python carrot python tempita python sqlalchemy python suds python lockfile python python boto python kombu python feedparser python nbd client open iscsi genisoimage sysfsutils utils tgt qemu utils libpq dev python dev open iscsi python numpy python beautifulsoup python dateutil python paste python pastedeploy python anyjson python routes python xattr python sqlalchemy python webob python kombu pylint python eventlet python nose python sphinx python mox python kombu python coverage python python migrate nodejs dev is_ubuntu z deb ... sudo debian_frontend=noninteractive http_proxy= https_proxy= no_proxy= apt get option dpkg::options::= force assume yes update err trusty inrelease err trusty updates inrelease err trusty backports inrelease err trusty security inrelease err trusty release.gpg could not resolve err trusty updates release.gpg could not resolve err trusty security release.gpg could not resolve err trusty backports release.gpg could not resolve reading package lists... w: failed to fetch w: failed to fetch w: failed to fetch w: failed to fetch w: failed to fetch could not resolve w: failed to fetch could not resolve w: failed to fetch could not resolve w: failed to fetch could not resolve w: some index files failed to download. they have been ignored, or old ones used instead. no_update_repos=true ... reading package lists... building dependency tree... reading state information... bridge utils is already the newest version. curl is already the newest version. gawk is already the newest version. git is already the newest version. iptables is already the newest version. lsof is already the newest version. is already the newest version. parted is already the newest version. psmisc is already the newest version. python is already the newest version. screen is already the newest version. sudo is already the newest version. tar is already the newest version. tcpdump is already the newest version. wget is already the newest version. openssh server is already the newest version. openssh server set to manually installed. the following extra packages will be installed: binutils build essential comerr dev cpp cpp docutils common docutils doc dpkg dev fakeroot fontconfig config fonts dejavu core gcc javascript common multidev libalgorithm diff perl libalgorithm diff xs perl libalgorithm merge perl libboost libboost libc libc dev bin dev libcloog libconfig general perl libdpkg perl dev libfakeroot libfile fcntllock perl libgcc libgssapi libjpeg libjs jquery libjs jquery hotkeys libjs jquery isonscreen libjs jquery metadata libjs sphinxdoc libjs underscore libnetfilter nssdb libpaper utils libpython dev libssl dev libssl doc libtidy libyaml linux libc dev make manpages dev mysql common python amqp python amqplib python astroid python colorama python decorator python dingus python distlib python dns python docutils python egenix mxdatetime python egenix mxtools python formencode python python python librabbitmq python logilab common python markupsafe python openid python pastedeploy tpl python pbr python pil python pip python pygments python repoze.lru python requestbuilder python roman python scgi python sqlalchemy ext python stompy python tk python utidylib python yaml ruby sharutils sphinx common sphinx doc common dev ... upgraded, newly installed, to remove and not upgraded. need to get mb of archives. after this operation, mb of additional disk space will be used. err trusty updates main could not resolve err trusty main could not resolve err trusty main could not resolve err trusty main could not resolve err trusty updates main libboost could not resolve err trusty updates main libboost could not resolve err trusty main could not resolve ... e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: failed to fetch could not resolve e: unable to fetch some archives, maybe run apt get update or try with fix missing? failed local jobs p kill stack.sh failed: full log in"
    },
    {
        "url": "https://stackoverflow.com/questions/26065822",
        "text": "serf: eventmemberjoin: vagrant one err agent: failed to sync remote state: no known consul servers"
    },
    {
        "url": "https://stackoverflow.com/questions/26065822",
        "text": "agent.rpc: accepted client: agent: lan joining: consul server hostname serf: eventmemberjoin: agent two serf: eventmemberjoin: agent one agent: lan joined: err: nil consul: adding server agent one addr: dc: memberlist: suspect agent one has failed, no acks received memberlist: suspect agent two has failed, no acks received memberlist: suspect agent one has failed, no acks received memberlist: suspect agent two has failed, no acks received memberlist: suspect agent two has failed, no acks received memberlist: marking agent one as failed, suspect timeout reached serf: eventmemberfailed: agent one consul: removing server agent one addr: dc: memberlist: suspect agent one has failed, no acks received memberlist: marking agent two as failed, suspect timeout reached serf: eventmemberfailed: agent two memberlist: suspect agent two has failed, no acks received err agent: failed to sync remote state: rpc error: failed to get conn: dial tcp i o timeout serf: attempting reconnect to agent one err agent: failed to sync remote state: no known consul servers"
    },
    {
        "url": "https://stackoverflow.com/questions/26067481",
        "text": "nova.virt.driver unable to load the virtualization driver trace nova.virt.driver traceback most recent call last : trace nova.virt.driver file line in load_compute_driver trace nova.virt.driver virtapi trace nova.virt.driver file line in import_object_ns trace nova.virt.driver return import_class import_str *args, trace nova.virt.driver file line in import_class trace nova.virt.driver __import__ mod_str trace nova.virt.driver file line in module trace nova.virt.driver from novadocker.virt.docker import driver trace nova.virt.driver file line in module trace nova.virt.driver from nova.i18n import _ trace nova.virt.driver importerror: no module named"
    },
    {
        "url": "https://stackoverflow.com/questions/26067481",
        "text": "nova.virt.driver unable to load the virtualization driver trace nova.virt.driver traceback most recent call last : trace nova.virt.driver file line in load_compute_driver trace nova.virt.driver virtapi trace nova.virt.driver file line in import_object_ns trace nova.virt.driver return import_class import_str *args, trace nova.virt.driver file line in import_class trace nova.virt.driver __import__ mod_str trace nova.virt.driver file line in module trace nova.virt.driver from novadocker.virt.docker import driver trace nova.virt.driver file line in module trace nova.virt.driver from novadocker.virt.docker import client as docker_client trace nova.virt.driver importerror: cannot import name client"
    },
    {
        "url": "https://stackoverflow.com/questions/27212465",
        "text": "warn running task batch: no public_key file defined. at line at line"
    },
    {
        "url": "https://stackoverflow.com/questions/27212465",
        "text": "warn running task batch: wrong username password or wrong key on or root is not permitted to login over ssh. at line"
    },
    {
        "url": "https://stackoverflow.com/questions/27277279",
        "text": "nova.compute.manager instance: instance failed to spawn trace nova.compute.manager instance: traceback most recent call last : trace nova.compute.manager instance: file line in _spawn trace nova.compute.manager instance: block_device_info trace nova.compute.manager instance: file line in spawn trace nova.compute.manager instance: image self._pull_missing_image context, image_meta, instance trace nova.compute.manager instance: file line in _pull_missing_image trace nova.compute.manager instance: fileutils.ensure_tree snapshot_directory trace nova.compute.manager instance: file line in ensure_tree trace nova.compute.manager instance: os.makedirs path trace nova.compute.manager instance: file line in makedirs trace nova.compute.manager instance: makedirs head, mode trace nova.compute.manager instance: file line in makedirs trace nova.compute.manager instance: mkdir name, mode trace nova.compute.manager instance: oserror: errno permission denied: trace nova.compute.manager instance: nova.compute.manager instance: error: errno permission denied: trace nova.compute.manager instance: traceback most recent call last : trace nova.compute.manager instance: file line in _build_instance trace nova.compute.manager instance: set_access_ip=set_access_ip trace nova.compute.manager instance: file line in decorated_function trace nova.compute.manager instance: return function self, context, trace nova.compute.manager instance: file line in _spawn trace nova.compute.manager instance: log.exception _ instance failed to spawn , trace nova.compute.manager instance: file line in __exit__ trace nova.compute.manager instance: six.reraise self.type_, self.value, self.tb trace nova.compute.manager instance: file line in _spawn trace nova.compute.manager instance: block_device_info trace nova.compute.manager instance: file line in spawn trace nova.compute.manager instance: image self._pull_missing_image context, image_meta, instance trace nova.compute.manager instance: file line in _pull_missing_image trace nova.compute.manager instance: fileutils.ensure_tree snapshot_directory trace nova.compute.manager instance: file line in ensure_tree trace nova.compute.manager instance: os.makedirs path trace nova.compute.manager instance: file line in makedirs trace nova.compute.manager instance: makedirs head, mode trace nova.compute.manager instance: file line in makedirs trace nova.compute.manager instance: mkdir name, mode trace nova.compute.manager instance: oserror: errno permission denied: trace nova.compute.manager instance:"
    },
    {
        "url": "https://stackoverflow.com/questions/27516705",
        "text": "failure creating private subnet for"
    },
    {
        "url": "https://stackoverflow.com/questions/27556935",
        "text": "enabled_services=g reg,key,n api,n crt,n obj,n cpu,n net,n cond,n sch,n novnc,n xvnc,n cauth,c sch,c api,c vol,h eng,h api,h api cfn,h api cw,horizon,rabbit,tempest,mysql,ceilometer acompute,ceilometer acentral,ceilometer anotification,ceilometer collector,ceilometer alarm evaluator,ceilometer alarm notifier,ceilometer api is_service_enabled gantt return for i in r source stack post config is_service_enabled marconi server return for i in r source stack post config is_service_enabled sahara return for i in r source stack post config is_service_enabled trove return for i in r source stack post config is_service_enabled odl server odl compute return is_service_enabled odl server return is_service_enabled odl compute return for i in r source stack post config is_service_enabled tempest return stack stack post config stack post config create_tempest_accounts is_service_enabled tempest return openstack project create alt_demo error: openstack authentication failure: an unexpected prevented the server from fulfilling your request: no module named backends.sql disable debug mode to suppress these details. http exit_trap local jobs p"
    },
    {
        "url": "https://stackoverflow.com/questions/27653228",
        "text": "remote error: operationalerror operationalerror column cannot be null update instance_extra set updated_at=%s, instance_uuid=%s where instance_extra.id datetime.datetime 2014, , none, trace oslo.messaging.rpc.dispatcher u traceback most recent call last , u file line in _object_dispatch\\n return getattr target, method context, , u file line in return fn self, ctxt, , u file line in columns_to_join=_expected_cols expected_attrs , u file line in instance_update_and_get_original\\n columns_to_join=columns_to_join \\n u file line in return f , u file line in instance_update_and_get_original\\n columns_to_join=columns_to_join \\n u file line in _instance_update\\n session.add instance_ref \\n , u file line in __exit__\\n self.rollback \\n u file line in __exit__\\n compat.reraise exc_type, exc_value, exc_tb \\n u file line in __exit__\\n self.commit \\n u file line in self._prepare_impl \\n , u file line in _prepare_impl\\n self.session.flush \\n , u file line in self._flush objects \\n u file line in transaction.rollback _capture_exception=true \\n u file line in __exit__\\n compat.reraise exc_type, exc_value, exc_tb \\n u file line in flush_context.execute \\n , u file line in rec.execute self \\n u file line in , u file line in save_obj\\n mapper, table, update , u file line in _emit_update_statements\\n execute statement, params , u file line in return meth self, multiparams, params , u file line in _execute_on_connection\\n return connection._execute_clauseelement self, multiparams, params , u file line in _execute_clauseelement\\n compiled_sql, distilled_params\\n u file line in _execute_context\\n context , u file line in _handle_dbapi_exception\\n util.raise_from_cause newraise, exc_info \\n u file line in raise_from_cause\\n reraise type exception , exception, \\n u file line in _execute_context\\n context , u file line in do_execute\\n cursor.execute statement, parameters , u file line in self.errorhandler self, exc, value , u file line in raise errorclass, , u operationalerror: operationalerror column cannot be null update instance_extra set updated_at=%s, instance_uuid=%s where instance_extra.id datetime.datetime 2014, , none, ."
    },
    {
        "url": "https://stackoverflow.com/questions/27703592",
        "text": "glance.store.filesystem specify at least or option glance.store.base failed to configure store correctly: store filesystem could not be configured correctly. reason: specify at least or option disabling add method. glance.api.v1.upload_utils failed to upload image trace glance.api.v1.upload_utils traceback most recent call last : trace glance.api.v1.upload_utils file line in upload_data_to_store"
    },
    {
        "url": "https://stackoverflow.com/questions/27731012",
        "text": "fatal error: needed to prompt for a connection or sudo password host: but abort on prompts was set to true aborting. log manager manager_6ce27.configure error: exception raised on operation fabric_plugin.tasks.run_task invocation traceback most recent call last : file line in wrapper result func file line in run_task _run_task task, task_properties, fabric_env file line in _run_task task file string , line in configure file string , line in _copy_openstack_configuration_to_manager file line in host_prompting_wrapper return func file line in put ftp sftp env.host_string file line in __init__ self.ftp connections host_string .open_sftp file line in __getitem__ self.connect key file line in connect self key connect user, host, port, file line in connect password prompt_for_password text file line in prompt_for_password handle_prompt_abort a connection or sudo password file line in handle_prompt_abort abort reason abort on prompts was set to true file line in abort raise env.abort_exception msg fabrictaskerror: needed to prompt for a connection or sudo password host: but abort on prompts was set to true cfy manager manager_6ce27.configure task failed needed to prompt for a connection or sudo password host: but abort on prompts was set to true attempt cfy manager workflow execution failed: workflow failed: task failed needed to prompt for a connection or sudo password host: but abort on prompts was set to true bootstrap failed!"
    },
    {
        "url": "https://stackoverflow.com/questions/27731012",
        "text": "log manager manager_db1f7.configure info: environment prepared successfully log manager manager_db1f7.configure error: exception raised on operation fabric_plugin.tasks.run_task invocation traceback most recent call last : file line in wrapper result func file line in run_task _run_task task, task_properties, fabric_env file line in _run_task task file string , line in configure file string , line in _copy_openstack_configuration_to_manager file line in host_prompting_wrapper return func file line in put ftp sftp env.host_string file line in __init__ self.ftp connections host_string .open_sftp file line in open_sftp return self._transport.open_sftp_client file line in open_sftp_client return sftpclient.from_transport self file line in from_transport return cls chan file line in __init__ server_version self._send_version file line in _send_version t, data self._read_packet file line in _read_packet raise sftperror garbage packet received sftperror: garbage packet received cfy manager manager_db1f7.configure task failed garbage packet received attempt"
    },
    {
        "url": "https://stackoverflow.com/questions/28052729",
        "text": "glance.wsgi.server removing dead child glance.wsgi.server started child debug glance.wsgi.server exited wait_on_children glance.wsgi.server wsgi starting up on glance.wsgi.server child exiting normally glance_store._drivers.filesystem specify either or option glance_store.driver failed to configure store correctly: none disabling add method. glance_store._drivers.filesystem specify either or option glance_store.driver failed to configure store correctly: none disabling add method. glance_store._drivers.filesystem specify either or option glance_store.driver failed to configure store correctly: none disabling add method. glance_store._drivers.filesystem specify either or option glance_store.driver failed to configure store correctly: none disabling add method."
    },
    {
        "url": "https://stackoverflow.com/questions/28422914",
        "text": "config file config file debug nova.servicegroup.api servicegroup driver defined as an instance of db from __new__ nova.openstack.common.periodic_task skipping periodic task _periodic_update_dns because its interval is negative nova.virt.driver loading compute driver nova.virt.driver unable to load the virtualization driver trace nova.virt.driver traceback most recent call last : trace nova.virt.driver file line in load_compute_driver trace nova.virt.driver virtapi trace nova.virt.driver file line in import_object_ns trace nova.virt.driver return import_class import_str *args, trace nova.virt.driver file line in __init__ trace nova.virt.driver self.containers containers.containers virtapi trace nova.virt.driver file line in __init__ trace nova.virt.driver vif_class importutils.import_class conf.lxc.vif_driver trace nova.virt.driver file line in import_class trace nova.virt.driver __import__ mod_str trace nova.virt.driver file line in module trace nova.virt.driver from nova import processutils trace nova.virt.driver importerror: cannot import name processutils trace nova.virt.driver importerror: cannot import name processutils."
    },
    {
        "url": "https://stackoverflow.com/questions/28460311",
        "text": "thread o.j.h.h.backofflimitedretryhandler cannot retry after server error, command has exceeded retry limit abstract org.jclouds.openstack.keystone.v2_0.domain.access org.jclouds.openstack.keystone.v2_0.authenticationapi.authenticatewithtenantnameandcredentials java.lang.string,org.jclouds.openstack.keystone.v2_0.domain.passwordcredentials admin, passwordcredentials , http"
    },
    {
        "url": "https://stackoverflow.com/questions/28534990",
        "text": "config file sahara.main starting sahara all in one sahara.utils.rpc notifications enabled sahara.plugins.base plugin loaded sahara.plugins.hdp.ambariplugin:ambariplugin sahara.plugins.base plugin loaded sahara.plugins.vanilla.plugin:vanillaprovider sahara.plugins.base plugin loaded sahara.plugins.fake.plugin:fakepluginprovider debug sahara.main logging of request response exchange could be enabled using flag log exchange from make_app keystonemiddleware.auth_token starting keystone auth_token middleware keystonemiddleware.auth_token using as cache directory for signing certificate keystonemiddleware.auth_token signing_dir mode is instead of critical sahara typeerror: unsupported type for timedelta seconds component: str trace sahara traceback most recent call last : trace sahara file line in module trace sahara load_entry_point trace sahara file line in main trace sahara app server.make_app trace sahara file line in make_app trace sahara app.wsgi_app acl.wrap app.wsgi_app, conf trace sahara file line in wrap trace sahara return auth_token.authprotocol app, trace sahara file line in __init__ trace sahara trace sahara typeerror: unsupported type for timedelta seconds component: str trace sahara sahara failed to start"
    },
    {
        "url": "https://stackoverflow.com/questions/28670908",
        "text": "trace designate file line in module trace designate _authtokenplugin.register_conf_options conf, _authtoken_group trace designate attributeerror: type object has no attribute trace designate"
    },
    {
        "url": "https://stackoverflow.com/questions/28987303",
        "text": "try to get system from: my host b,c .example.com my host b,c .example.com 24249 debug setting read timeout to none my host b,c .example.com 24249 debug get http my host b,c .example.com mylogger host_name"
    },
    {
        "url": "https://stackoverflow.com/questions/29065477",
        "text": "oslo.messaging._drivers.impl_rabbit delaying reconnect for seconds... oslo.messaging._drivers.impl_rabbit connecting to amqp server on privatecloud:5672 oslo.messaging._drivers.impl_rabbit amqp server on privatecloud:5672 is unreachable: errno econnrefused. trying again in seconds."
    },
    {
        "url": "https://stackoverflow.com/questions/29571057",
        "text": "url_helper.py : calling failed : unexpected object has no attribute"
    },
    {
        "url": "https://stackoverflow.com/questions/29699450",
        "text": "critical root runtimeerror: manager class not registered for datastore manager none trace root traceback most recent call last : trace root file line in module trace root sys.exit main trace root file line in main trace root raise runtimeerror msg trace root runtimeerror: manager class not registered for datastore manager none"
    },
    {
        "url": "https://stackoverflow.com/questions/29779476",
        "text": "openstackclient.shell exception error: openstackclient.shell exception raised: six requirement.parse six set admin_tenant= openstack user create admin project email password openstack grep id get_field read data error: openstackclient.shell exception raised: six requirement.parse six set admin_user= openstack role create admin grep id get_field read data error: openstackclient.shell exception raised: six requirement.parse six set admin_role= openstack role add project user error: openstackclient.shell exception raised: six requirement.parse six set exit_trap local jobs p n exit"
    },
    {
        "url": "https://stackoverflow.com/questions/29865287",
        "text": "execute_user_data_script c:\\program files debug cloudbaseinit.plugins.windows.userdatautils user_data stderr: the term ? is not recognized as the name of a cmdlet, function, script file, or operable program. check the spelling of the name, or if a path was included, verify that the path is correct and try again. at c:\\users\\cloudbase char:24 regex ? snapshot categoryinfo : objectnotfound: ? :string , commandnotfou ndexception fullyqualifiederrorid : commandnotfoundexception"
    },
    {
        "url": "https://stackoverflow.com/questions/29963316",
        "text": "org.cloudifysource.shell.commands.abstractgscommand setting security profile to org.cloudifysource.shell.commands.abstractgscommand bootstrapping cloud openstack havana. this may take a few minutes. org.cloudifysource.esc.driver.provisioning.baseprovisioningdriver setup network configuration for managers org.cloudifysource.esc.driver.provisioning.baseprovisioningdriver using management network : cloudify management network org.cloudifysource.esc.shell.listener.cliagentlessinstallerlistener attempting to access management vm org.cloudifysource.esc.shell.listener.cliagentlessinstallerlistener uploading files to com.jcraft.jsch permanently added rsa to the list of known hosts. org.cloudifysource.esc.shell.installer.cloudgridagentbootstrapper failed accessing management vm reason: failed to set up file transfer: unknown message with code could not determine the type of file caused by: org.cloudifysource.esc.installer.installerexception: failed to set up file transfer: unknown message with code could not determine the type of file org.cloudifysource.esc.driver.provisioning.openstack.openstackcloudifydriver deleting floating ip: floatingip severe org.cloudifysource.shell.commands.abstractgscommand failed to set up file transfer: unknown message with code could not determine the type of file : org.cloudifysource.esc.installer.installerexception: failed to set up file transfer: unknown message with code could not determine the type of file at org.cloudifysource.esc.installer.filetransfer.vfsfiletransfer.initialize vfsfiletransfer.java:206 at org.cloudifysource.esc.installer.agentlessinstaller.uploadfilestoserver agentlessinstaller.java:306 at org.cloudifysource.esc.installer.agentlessinstaller.installonmachinewithip agentlessinstaller.java:210 at org.cloudifysource.esc.shell.installer.cloudgridagentbootstrapper$1.call cloudgridagentbootstrapper.java:865 at org.cloudifysource.esc.shell.installer.cloudgridagentbootstrapper$1.call cloudgridagentbootstrapper.java:860 at java.util.concurrent.futuretask.run futuretask.java:262 at java.util.concurrent.threadpoolexecutor.runworker threadpoolexecutor.java:1145 at java.util.concurrent.threadpoolexecutor$worker.run threadpoolexecutor.java:615 at java.lang.thread.run thread.java:744 caused by: org.apache.commons.vfs2.filesystemexception: unknown message with code could not determine the type of file at org.apache.commons.vfs2.provider.sftp.sftpfileobject.refresh sftpfileobject.java:95 at org.apache.commons.vfs2.provider.abstractfilesystem.resolvefile abstractfilesystem.java:366 at org.apache.commons.vfs2.provider.abstractfilesystem.resolvefile abstractfilesystem.java:317 at org.apache.commons.vfs2.provider.abstractoriginatingfileprovider.findfile abstractoriginatingfileprovider.java:85 at org.apache.commons.vfs2.provider.abstractoriginatingfileprovider.findfile abstractoriginatingfileprovider.java:65 at org.apache.commons.vfs2.impl.defaultfilesystemmanager.resolvefile defaultfilesystemmanager.java:693 at org.apache.commons.vfs2.impl.defaultfilesystemmanager.resolvefile defaultfilesystemmanager.java:621 at org.cloudifysource.esc.installer.filetransfer.vfsfiletransfer.resolvetargetdirectory vfsfiletransfer.java:218 at org.cloudifysource.esc.installer.filetransfer.vfsfiletransfer.initialize vfsfiletransfer.java:203 ... more caused by: org.apache.commons.vfs2.filesystemexception: could not determine the type of file at org.apache.commons.vfs2.provider.abstractfileobject.gettype abstractfileobject.java:505 at org.apache.commons.vfs2.provider.sftp.sftpfileobject.refresh sftpfileobject.java:91 ... more caused by: org.apache.commons.vfs2.filesystemexception: could not connect to sftp server at at org.apache.commons.vfs2.provider.sftp.sftpfilesystem.getchannel sftpfilesystem.java:153 at org.apache.commons.vfs2.provider.sftp.sftpfileobject.statself sftpfileobject.java:151 at org.apache.commons.vfs2.provider.sftp.sftpfileobject.dogettype sftpfileobject.java:114 at org.apache.commons.vfs2.provider.abstractfileobject.gettype abstractfileobject.java:496 ... more caused by: com.jcraft.jsch.jschexception: java.io.ioexception: pipe closed at com.jcraft.jsch.channelsftp.start channelsftp.java:288 at com.jcraft.jsch.channel.connect channel.java:152 at com.jcraft.jsch.channel.connect channel.java:145 at org.apache.commons.vfs2.provider.sftp.sftpfilesystem.getchannel sftpfilesystem.java:130 ... more caused by: java.io.ioexception: pipe closed at java.io.pipedinputstream.read pipedinputstream.java:308 at java.io.pipedinputstream.read pipedinputstream.java:378 at com.jcraft.jsch.channelsftp.fill channelsftp.java:2665 at com.jcraft.jsch.channelsftp.header channelsftp.java:2691 at com.jcraft.jsch.channelsftp.start channelsftp.java:257 ... more"
    },
    {
        "url": "https://stackoverflow.com/questions/30115309",
        "text": "heat heat engine heat.common.config ht the option in heat.conf is deprecated and will be removed in the juno release. edt heatcritical : importerror: no module named my_heat_plugin.client traceback most recent call last : file line in module srv engine.engineservice cfg.conf.host, rpc_api.engine_topic file line in __init__ resources.initialise file line in initialise _load_global_environment global_env file line in _load_global_environment _load_global_resources env file line in _load_global_resources manager plugin_manager.pluginmanager __name__ file line in __init__ self.modules list modules file line in load_modules module _import_module importer, module_name, package file line in _import_module module loader.load_module module_name file line in load_module mod imp.load_module fullname, self.file, self.filename, self.etc file line in module from abc_heat_plugin.client import constants as const importerror: no module named abc_heat_plugin.client"
    },
    {
        "url": "https://stackoverflow.com/questions/30125093",
        "text": "util.py : getting data from class failed stopping read required files in advance for other mountpoints ok stopping read required files in advance for other mountpoints ok stopping read required files in advance for other mountpoints ok url_helper.py : calling failed : request httpconnectionpool : max retries exceeded with url: caused by class : errno no route to host"
    },
    {
        "url": "https://stackoverflow.com/questions/30259064",
        "text": "debug keystonemiddleware.auth_token removing headers from request environment: x service catalog,x identity status,x service identity status,x roles,x service roles,x domain name,x service domain name,x project id,x service project id,x project domain name,x service project domain name,x user id,x service user id,x user name,x service user name,x project name,x service project name,x user domain id,x service user domain id,x domain id,x service domain id,x user domain name,x service user domain name,x project domain id,x service project domain id,x role,x user,x tenant name,x tenant id,x tenant _remove_auth_headers debug keystonemiddleware.auth_token authenticating user token __call__ debug keystoneclient.auth.identity.v2 making authentication request to get_auth_ref debug keystoneclient.session req: curl g i x get h x subject token: h user agent: python keystoneclient h accept: application h x auth token: _http_log_request debug keystoneclient.session resp: _http_log_response debug keystoneclient.session request returned failure status: request keystonemiddleware.auth_token bad response code while validating token: keystonemiddleware.auth_token identity response: you are not authorized to perform the requested action: identity:validate_token disable debug mode to suppress these details. , debug keystonemiddleware.auth_token token validation failure. _validate_token trace keystonemiddleware.auth_token traceback most recent call last : trace keystonemiddleware.auth_token file line in _validate_token trace keystonemiddleware.auth_token expires _get_token_expiration data trace keystonemiddleware.auth_token file line in _get_token_expiration trace keystonemiddleware.auth_token raise exc.invalidtoken _ token authorization failed trace keystonemiddleware.auth_token invalidtoken: token authorization failed trace keystonemiddleware.auth_token debug keystonemiddleware.auth_token marking token as unauthorized in cache store_invalid keystonemiddleware.auth_token authorization failed for token keystonemiddleware.auth_token invalid user token rejecting request sahara.cli.sahara_all get http debug sahara.openstack.common.periodic_task running periodic task saharaperiodictasks.update_job_statuses run_periodic_tasks debug sahara.service.periodic updating job statuses update_job_statuses"
    },
    {
        "url": "https://stackoverflow.com/questions/30640077",
        "text": "esm org.openspaces.grid.gsm.strategy.scalestrategyprogresseventstate tommy.tomcat machines sla enforcement is in progress.; caused by: org.openspaces.grid.gsm.machines.exceptions.expectedmachinewithmorememoryexception: machines sla enforcement is in progress: expected machine with more memory. machine public_ip public_ip has been started with not enough memory. actual total memory is which is less than reserved container"
    },
    {
        "url": "https://stackoverflow.com/questions/30731034",
        "text": "nova.scheduler.filter_scheduler attempting to build instance s uuids: u nova.scheduler.filters.compute_filter ram:3257 disk:87040 io_ops:0 instances:1 has not been heard from in a while nova.scheduler.filters.compute_filter network, network ram:3257 disk:59392 io_ops:0 instances:0 has not been heard from in a while nova.scheduler.filters.compute_filter thinkcentre thinkcentre ram:3257 disk:87040 io_ops:0 instances:0 has not been heard from in a while nova.scheduler.filter_scheduler choosing host weighedhost host: weight: for instance oslo.messaging._drivers.impl_rabbit connected to amqp server on nova.scheduler.filter_scheduler attempting to build instance s uuids: u nova.scheduler.filter_scheduler instance: from last host: node : u traceback most recent call last , u file line in _build_instance\\n set_access_ip=set_access_ip \\n u file line in decorated_function\\n return function self, context, , u file line in log.exception _ instance failed to , , u file line in __exit__\\n six.reraise self.type_, self.value, self.tb \\n u file line in block_device_info \\n , u file line in write_to_disk=true \\n u file line in to_xml\\n disk_info, rescue, block_device_info \\n , u file line in get_guest_config\\n flavor , u file line in get_config\\n _ unexpected vif_type=%s vif_type \\n u novaexception: unexpected vif_type=binding_failed\\n nova.scheduler.filters.compute_filter ram:3257 disk:87040 io_ops:0 instances:1 has not been heard from in a while nova.scheduler.filters.compute_filter network, network ram:3257 disk:59392 io_ops:0 instances:0 has not been heard from in a while nova.scheduler.filters.compute_filter thinkcentre thinkcentre ram:3257 disk:87040 io_ops:0 instances:0 has not been heard from in a while nova.filters filter computefilter returned hosts nova.scheduler.driver instance: setting instance to state."
    },
    {
        "url": "https://stackoverflow.com/questions/30975069",
        "text": "nova.compute.manager instance: instance failed to trace nova.compute.manager instance: traceback most recent call last : trace nova.compute.manager instance: file line in _build_resources trace nova.compute.manager instance: yield resources trace nova.compute.manager instance: file line in _build_and_run_instance trace nova.compute.manager instance: block_device_info=block_device_info trace nova.compute.manager instance: file line in spawn trace nova.compute.manager instance: trace nova.compute.manager instance: cannot create trace nova.compute.manager instance:"
    },
    {
        "url": "https://stackoverflow.com/questions/31160977",
        "text": "localrc credentials database_password=devstack admin_password=devstack service_password=devstack service_token=devstack rabbit_password=devstack guest_password=devstack mysql_host=127.0.0.1 mysql_user=root mysql_password=devstack rabbit_host=127.0.0.1 libvirt_type=qemu glance_hostport=127.0.0.1:9292 service_password=devstack enabled_services=key,n crt,n obj,n cpu,n net,n cond,n sch,n novnc,n xvnc,n cauth enabled_services+=,g reg enabled_services+=,cinder,c api,c vol,c bak enabled_services+=,manila,m sch,m shr enabled_services+=,horizon,rabbit,mysql,tempest needs for manila disable_service n net enable_service q svc,q agt,q dhcp,q l3,q meta,neutron devstack.log screen_logdir=$dest api_rate_limit=false q_plugin=ml2 enable_tenant_vlans=true physical_network=physnet1 ovs_physical_bridge=br ex q_ml2_plugin_mechanism_drivers=openvswitch q_ml2_plugin_type_drivers=flat,vlan,vxlan share_backing_file_size=8g flat_interface=eno1 manila_repo_root=openstack enable_plugin manila installation errors : user_role_id=de7a755f4a604c3c843f912d3daa8bc2 echo stack stack stack stack run_plugins stack post config local local config local local plugin for plugin in local f source stack post config is not in global requirements.txt on exit"
    },
    {
        "url": "https://stackoverflow.com/questions/31203760",
        "text": "here are my config files: glance api.conf glance registry.conf keystone_authtoken auth_uri identity_uri admin_tenant_name service admin_user glance admin_password mypass keystone.conf b here is my api.log: keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token http connection exception: unable to establish connection to keystonemiddleware.auth_token authorization failed for token keystonemiddleware.auth_token invalid user token deferring reject downstream glance.wsgi.server post http keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token retrying on http connection exception: unable to establish connection to keystonemiddleware.auth_token http connection exception: unable to establish connection to keystonemiddleware.auth_token authorization failed for token keystonemiddleware.auth_token invalid user token deferring reject downstream glance.wsgi.server post http"
    },
    {
        "url": "https://stackoverflow.com/questions/31675063",
        "text": "critical nova operationalerror: operationalerror unknown database none none trace nova traceback most recent call last : trace nova file line in module trace nova sys.exit main trace nova file line in main trace nova ret fn trace nova file line in sync trace nova return migration.db_sync version trace nova file line in db_sync trace nova return impl.db_sync version=version trace nova file line in db_sync trace nova current_version db_version trace nova file line in db_version trace nova return versioning_api.db_version get_engine , repository trace nova file line in get_engine trace nova facade _create_facade_lazily trace nova file line in _create_facade_lazily trace nova _engine_facade db_session.enginefacade.from_config conf trace nova file line in from_config trace nova retry_interval=conf.database.retry_interval trace nova file line in __init__ trace nova trace nova file line in create_engine trace nova _test_connection engine, max_retries, retry_interval trace nova file line in _test_connection trace nova return exc_filters.handle_connect_error engine trace nova file line in handle_connect_error trace nova handler ctx trace nova file line in handler trace nova context.is_disconnect trace nova file line in _raise_operational_errors_directly_filter trace nova raise operational_error trace nova operationalerror: operationalerror unknown database none none trace nova command failed, please check log for more exit_trap local jobs p n kill_spinner z ne echo on exit on exit z d exit"
    },
    {
        "url": "https://stackoverflow.com/questions/32200851",
        "text": "trace nova.compute.manager instance: _ unexpected vif_type=%s vif_type trace nova.compute.manager instance: novaexception: unexpected vif_type=binding_failed"
    },
    {
        "url": "https://stackoverflow.com/questions/32226108",
        "text": "aug master kube apiserver : panic: runtime error: invalid memory address or nil pointer dereference aug master kube apiserver : signal aug master kube apiserver : goroutine running : aug master kube apiserver : aug master kube apiserver : aug master kube apiserver : aug master kube apiserver : aug master kube apiserver : main.main aug master kube apiserver : aug master kube apiserver : goroutine chan receive : aug master kube apiserver : github.com aug master kube apiserver : aug master kube apiserver : created by github.com aug master kube apiserver : aug master kube apiserver : goroutine syscall : aug master kube apiserver : os signal.loop aug master kube apiserver : aug master kube apiserver : created by os signal.init.1 aug master kube apiserver : aug master kube apiserver : goroutine runnable :"
    },
    {
        "url": "https://stackoverflow.com/questions/32234541",
        "text": ":. stack.sh:575+echo waiting for keystone to start... waiting for keystone to start... :. stack.sh:579+wait_for_service :. stack.sh:340+local :. stack.sh:341+local :. stack.sh:342+timeout sh c while ! curl g k noproxy s do sleep done :. stack.sh:584+is_service_enabled tls proxy :. stack.sh:1738+return :. :. stack.sh:978+is_service_enabled tls proxy :. stack.sh:1738+return :. stack.sh:985+export os_token=password :. stack.sh:985+os_token=password :. stack.sh:986+export :. :. stack.sh:988+create_keystone_accounts ::. stack.sh:376+get_or_create_project admin ::. stack.sh:729+local os_cmd=openstack ::. stack.sh:730+local ::. stack.sh:731+ ! z :::. stack.sh:740+openstack project create admin or show f value c id error: openstack the resource could not be found. http request id: ::. stack.sh:738+local project_id= ::. stack.sh:739+echo :. stack.sh:376+local admin_tenant= ::. stack.sh:377+get_or_create_user admin password ::. stack.sh:700+ ! z ::. stack.sh:703+local ::. stack.sh:705+local os_cmd=openstack ::. stack.sh:706+local ::. stack.sh:707+ ! z :::. stack.sh:723+openstack user create admin password password or show f value c id error: openstack"
    },
    {
        "url": "https://stackoverflow.com/questions/32808809",
        "text": "cloud init : cloud init v. running at sun, sep up seconds. cloud init : ci info: device cloud init : ci info: cloud init : ci info: device up address mask hw address cloud init : ci info: cloud init : ci info: lo: true . cloud init : ci info: true . . fa:16:3e:20:97:7b cloud init : ci info: cloud init : ci info: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!route failed!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! centos linux core kernel on an localhost login: cloud init : util.py : failed to set the hostname to xxxxxxxxx xxxx cloud init : util.py : running set_hostname module from failed cloud init : util.py : failed to write hostname xxxxxxxxx to"
    },
    {
        "url": "https://stackoverflow.com/questions/33097631",
        "text": "oslo_messaging._drivers.impl_rabbit connecting to amqp server on os svr:5672 oslo_messaging._drivers.impl_rabbit connected to amqp server on os svr:5672 nova.virt.bluecube.driver .......... nova.virt.bluecube.driver u u u false, u u u u u u u u u true, u none, u , u u u u u u u u u u u nova.virt.bluecube.vmops instance: spawning new instance nova.compute.manager instance failed network setup after attempt s nova.compute.manager traceback most recent call last : nova.compute.manager file line in _allocate_network_async nova.compute.manager dhcp_options=dhcp_options nova.compute.manager file line in wrapped nova.compute.manager return func self, context, nova.compute.manager file line in wrapper nova.compute.manager res f self, context, nova.compute.manager file line in allocate_for_instance nova.compute.manager nw_info self.network_rpcapi.allocate_for_instance context, nova.compute.manager file line in allocate_for_instance nova.compute.manager macs nova.compute.manager file line in call nova.compute.manager nova.compute.manager file line in _send nova.compute.manager nova.compute.manager file line in send nova.compute.manager nova.compute.manager file line in _send nova.compute.manager raise result nova.compute.manager oserror: errno permission deined"
    },
    {
        "url": "https://stackoverflow.com/questions/33234128",
        "text": "keystoneclient.middleware.auth_token unable to find authentication token in headers keystoneclient.middleware.auth_token invalid user token rejecting request eventlet.wsgi get http"
    },
    {
        "url": "https://stackoverflow.com/questions/34012841",
        "text": "controller sahara all config file sahara.main starting sahara all in one sahara.utils.rpc notifications disabled sahara.plugins.base plugin loaded sahara.plugins.vanilla.plugin:vanillaprovider sahara.plugins.base plugin loaded sahara.plugins.hdp.ambariplugin:ambariplugin sahara.plugins.base plugin loaded sahara.plugins.spark.plugin:sparkprovider debug sahara.main logging of request response exchange could be enabled using flag log exchange make_app keystonemiddleware.auth_token starting keystone auth_token middleware keystonemiddleware.auth_token using as cache directory for signing certificate sahara.main loading ops debug sahara.service.periodic starting periodic tasks with initial delay seconds setup sahara.openstack.common.periodic_task skipping periodic task check_for_zombie_proxy_users because its interval is negative sahara.main loading infrastructure engine sahara.main loading remote critical sahara error: errno address already in use trace sahara traceback most recent call last : trace sahara file line in module trace sahara sys.exit main trace sahara file line in main trace sahara wsgi.server eventlet.listen cfg.conf.host, cfg.conf.port , , trace sahara file line in listen trace sahara sock.bind addr trace sahara file line in meth trace sahara return getattr self._sock,name *args trace sahara error: errno address already in use trace sahara controller"
    },
    {
        "url": "https://stackoverflow.com/questions/34192462",
        "text": "keystone.middleware.core rbac: invalid token keystone.common.wsgi the request you have made requires authentication."
    },
    {
        "url": "https://stackoverflow.com/questions/34312403",
        "text": "glance.api.v1.upload_utils failed to upload image trace glance.api.v1.upload_utils raise notimplementederror trace glance.api.v1.upload_utils notimplementederror glance.api.v2.image_data failed to upload image data due to internal trace glance.api.v2.image_data self.notifier.error msg self.notifier.error msg glance.api.v2.image_data failed to upload image data due to internal"
    },
    {
        "url": "https://stackoverflow.com/questions/34446815",
        "text": "dec localhost kubelet: manager.go:158 machine: numcores:1 cpufrequency:2992788 memorycapacity:1929773056 machineid:19d82c8902374e7caa655973bd7a6e6a systemuuid:b75ef5e2 bootid:7bf3d099 filesystems: device: capacity:21456445440 diskmap:map 252:0: major:252 minor:0 size:107374182400 scheduler:none major:253 minor:0 size:21474836480 scheduler:none networkdevices: name:eth0 macaddress:fa:16:3e:5d:3c:38 speed:0 mtu:1500 name:flannel0 macaddress: speed:10 mtu:1472 topology: id:0 memory:2147074048 cores: id:0 threads: caches: size:32768 type:data level:1 size:32768 type:instruction level:1 size:4194304 type:unified level:2 caches: cloudprovider:unknown instancetype:unknown dec localhost docker: get dec localhost kubelet: manager.go:165 version: kernelversion:3.10.0 containerosversion:centos linux core dockerversion:1.8.2 cadvisorversion:0.16.0 dec localhost kubelet: panic: runtime error: invalid memory address or nil pointer dereference dec localhost kubelet: signal dec localhost kubelet: goroutine running : dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: main.main dec localhost kubelet: dec localhost kubelet: goroutine syscall, locked to thread : dec localhost kubelet: runtime.goexit dec localhost kubelet: dec localhost kubelet: goroutine chan receive : dec localhost kubelet: github.com dec localhost kubelet: dec localhost kubelet: created by github.com dec localhost kubelet: dec localhost kubelet: goroutine syscall : dec localhost kubelet: os signal.loop dec localhost kubelet: dec localhost kubelet: created by os signal.init.1 dec localhost kubelet: dec localhost kubelet: goroutine sleep : dec localhost kubelet: time.sleep 0x12a05f200 dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: created by dec localhost kubelet: dec localhost kubelet: goroutine io wait : dec localhost kubelet: net.runtime_pollwait 0x7f19addac638, dec localhost kubelet: dec localhost kubelet: net. .wait dec localhost kubelet: dec localhost kubelet: net. .waitread dec localhost kubelet: dec localhost kubelet: net. .accept dec localhost kubelet: dec localhost kubelet: net. .accepttcp dec localhost kubelet: dec localhost kubelet: net http.tcpkeepalivelistener.accept 0xc8200264d8, dec localhost kubelet: dec localhost kubelet: net http. .serve dec localhost kubelet: dec localhost kubelet: net http. .listenandserve dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: dec localhost kubelet: created by dec localhost kubelet: dec localhost systemd: kubelet.service: main process exited, invalidargument dec localhost systemd: unit kubelet.service entered failed state. dec localhost systemd: kubelet.service holdoff time over, scheduling restart. dec localhost systemd: stopping kubernetes kubelet server... dec localhost systemd: starting kubernetes kubelet server... dec localhost systemd: kubelet.service start request repeated too quickly, refusing to start. dec localhost systemd: failed to start kubernetes kubelet server. dec localhost systemd: unit kubelet.service entered failed state."
    },
    {
        "url": "https://stackoverflow.com/questions/34892918",
        "text": "debug eventlet.wsgi.server accepted from server"
    },
    {
        "url": "https://stackoverflow.com/questions/34892918",
        "text": "neutron.db.agents_db none none agent healthcheck: found dead agents out of type last heartbeat host dhcp agent gonzalo devstack ss agent gonzalo devstack ss metadata agent gonzalo devstack ss open vswitch agent gonzalo devstack ss oslo.messaging._drivers.impl_rabbit amqp server closed the connection. check login credentials: socket closed"
    },
    {
        "url": "https://stackoverflow.com/questions/34892918",
        "text": "oslo.messaging._drivers.impl_rabbit amqp server closed the connection. check login credentials: socket closed"
    },
    {
        "url": "https://stackoverflow.com/questions/35439056",
        "text": "debug jclouds.wire main sensitive data in payload, use property_logger_wire_log_sensitive_info override to enable logging this data. debug jclouds.headers main post http debug jclouds.headers main accept: application json debug jclouds.headers main content type: application json debug jclouds.headers main content length: debug jclouds.headers main http ok debug jclouds.headers main x distribution: ubuntu debug jclouds.headers main connection: keep alive debug jclouds.headers main vary: x auth token debug jclouds.headers main date: wed, feb gmt debug jclouds.headers main content type: application json debug jclouds.headers main content length: debug jclouds.wire main true, , , , , , , , , , , , , , debug jclouds.headers main get http debug jclouds.headers main accept: application json debug jclouds.headers main x auth token: debug jclouds.headers main http ok debug jclouds.headers main content type: text plain; debug jclouds.headers main connection: keep alive debug jclouds.headers main date: wed, feb gmt debug jclouds.headers main content type: text plain; debug jclouds.headers main content length: debug jclouds.wire main debug jclouds.wire main"
    },
    {
        "url": "https://stackoverflow.com/questions/35439056",
        "text": "debug org.jclouds.rest.internal.invokehttpmethod main invoking authenticationapi.authenticatewithtenantnameandcredentials debug org.jclouds.http.internal.javaurlhttpcommandexecutorservice main sending request post http debug org.jclouds.http.internal.javaurlhttpcommandexecutorservice main receiving response http ok debug org.jclouds.openstack.keystone.v2_0.suppliers.regionidtourifromaccessfortypeandversion main endpoints for apitype object store and version us east endpoint east debug org.jclouds.rest.internal.invokehttpmethod main invoking container:list debug org.jclouds.http.internal.javaurlhttpcommandexecutorservice main sending request get http debug org.jclouds.http.internal.javaurlhttpcommandexecutorservice main receiving response http ok org.jclouds.http.functions.parsejson main parsing input: java.lang.illegalstateexception: expected begin_array but was string at line column path com.google.gson.jsonsyntaxexception: java.lang.illegalstateexception: expected begin_array but was string at line column path at com.google.gson.gson.fromjson gson.java:817 at com.google.gson.gson.fromjson gson.java:770 at com.google.gson.gson.fromjson gson.java:719 at org.jclouds.json.internal.gsonwrapper.fromjson gsonwrapper.java:42 at org.jclouds.http.functions.parsejson.apply parsejson.java:83 at org.jclouds.http.functions.parsejson.apply parsejson.java:77 at org.jclouds.http.functions.parsejson.apply parsejson.java:62 at org.jclouds.http.functions.parsejson.apply parsejson.java:42 at org.jclouds.rest.internal.invokehttpmethod.invoke invokehttpmethod.java:90 at org.jclouds.rest.internal.invokehttpmethod.apply invokehttpmethod.java:73 at org.jclouds.rest.internal.invokehttpmethod.apply invokehttpmethod.java:44 at org.jclouds.reflect.functionalreflection$functionalinvocationhandler.handleinvocation functionalreflection.java:117 at com.google.common.reflect.abstractinvocationhandler.invoke abstractinvocationhandler.java:87 at com.sun.proxy.$proxy57.list unknown source at com.ceph.jcloudsswift.listcontainers jcloudsswift.java:58 at com.ceph.jcloudsswift.main jcloudsswift.java:22 caused by: java.lang.illegalstateexception: expected begin_array but was string at line column path at com.google.gson.stream.jsonreader.beginarray jsonreader.java:351 at org.jclouds.json.internal.nullfilteringtypeadapterfactories$iterabletypeadapter.readandbuild nullfilteringtypeadapterfactories.java:88 at org.jclouds.json.internal.nullfilteringtypeadapterfactories$iterabletypeadapter.read nullfilteringtypeadapterfactories.java:82 at org.jclouds.json.internal.nullfilteringtypeadapterfactories$fluentiterabletypeadapter.read nullfilteringtypeadapterfactories.java:239 at org.jclouds.json.internal.nullfilteringtypeadapterfactories$fluentiterabletypeadapter.read nullfilteringtypeadapterfactories.java:225 at com.google.gson.gson.fromjson gson.java:805 ... common frames omitted"
    },
    {
        "url": "https://stackoverflow.com/questions/35910437",
        "text": "keystoneclient.auth.identity.generic.base discovering versions from the identity service failed when creating the password plugin. attempting to determine version from url. cinder.api.middleware.fault caught error: could not determine a suitable url for the plugin cinder.api.middleware.fault returned with http eventlet.wsgi.server get http"
    },
    {
        "url": "https://stackoverflow.com/questions/36855856",
        "text": ": appeared during puppet run: x.x.x.x_api_nova.pp error: could not start service nova api : execution of start openstack nova api returned job for openstack nova api.service failed because the control process exited with code. see systemctl status openstack nova api.service and journalctl xe for details. you will find full trace in log apr localhost.localdomain systemd : unit openstack nova api.service entered failed state. apr localhost.localdomain systemd : openstack nova api.service failed. apr localhost.localdomain setroubleshoot : selinux is preventing from getattr access on the file for comple apr localhost.localdomain python : selinux is preventing from getattr access on the file plugin catchall confidence suggests if you believe that should be allowed getattr access on the rpm file by default. then you should report this as a bug. you can generate a local policy module to allow this access. do allow this access for now by executing: grep nova novncproxy m mypol semodule i mypol.pp apr localhost.localdomain systemd : openstack nova api.service holdoff time over, scheduling restart. apr localhost.localdomain systemd : starting openstack nova api server... subject: unit openstack nova api.service has begun start up defined by: systemd support: unit openstack nova api.service has begun starting up. apr localhost.localdomain : detected unhandled python exception in apr localhost.localdomain nova api : error: cannot open packages database in apr localhost.localdomain nova api : traceback most recent call last : apr localhost.localdomain nova api : file line in module apr localhost.localdomain nova api : sys.exit main apr localhost.localdomain nova api : file line in main apr localhost.localdomain nova api : config.parse_args sys.argv apr localhost.localdomain nova api : file line in parse_args apr localhost.localdomain nova api : default_config_files=default_config_files apr localhost.localdomain nova api : file line in __call__ apr localhost.localdomain nova api : self._namespace._files_permission_denied apr localhost.localdomain nova api : oslo_config.cfg.configfilespermissiondeniederror: failed to open some config files: apr localhost.localdomain setroubleshoot : selinux is preventing from getattr access on the file for comple apr localhost.localdomain python : selinux is preventing from getattr access on the file plugin catchall confidence suggests if you believe that should be allowed getattr access on the rpm file by default. then you should report this as a bug. you can generate a local policy module to allow this access. do allow this access for now by executing: grep nova novncproxy m mypol semodule i mypol.pp apr localhost.localdomain systemd : openstack nova api.service: main process exited, failure apr localhost.localdomain systemd : failed to start openstack nova api server. subject: unit openstack nova api.service has failed defined by: systemd support: unit openstack nova api.service has failed. the result is failed. apr localhost.localdomain systemd : unit openstack nova api.service entered failed state. apr localhost.localdomain systemd : openstack nova api.service failed."
    },
    {
        "url": "https://stackoverflow.com/questions/37190934",
        "text": "spark command: cp xx:maxpermsize=256m org.apache.spark.deploy.master.master ip port webui port using spark s profile: org master: registered signal handlers for term, hup, int warn utils: your hostname, spark master resolves to a loopback address: using instead on interface warn utils: set spark_local_ip if you need to bind to another address warn nativecodeloader: unable to load native hadoop library for your platform... using builtin java classes where applicable securitymanager: changing view acls to: ubuntu securitymanager: changing modify acls to: ubuntu securitymanager: securitymanager: authentication disabled; ui acls disabled; users with view permissions: set ubuntu ; users with modify permissions: set ubuntu warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port exception in thread java.net.bindexception: cannot assign requested address: service failed after retries! at sun.nio.ch.net.bind0 native method at sun.nio.ch.net.bind net.java:463 at sun.nio.ch.net.bind net.java:455 at sun.nio.ch.serversocketchannelimpl.bind serversocketchannelimpl.java:223 at sun.nio.ch.serversocketadaptor.bind serversocketadaptor.java:74 at io.netty.channel.socket.nio.nioserversocketchannel.dobind nioserversocketchannel.java:125 at io.netty.channel.abstractchannel$abstractunsafe.bind abstractchannel.java:485 at io.netty.channel.defaultchannelpipeline$headcontext.bind defaultchannelpipeline.java:1089 at io.netty.channel.abstractchannelhandlercontext.invokebind abstractchannelhandlercontext.java:430 at io.netty.channel.abstractchannelhandlercontext.bind abstractchannelhandlercontext.java:415 at io.netty.channel.defaultchannelpipeline.bind defaultchannelpipeline.java:903 at io.netty.channel.abstractchannel.bind abstractchannel.java:198 at io.netty.bootstrap.abstractbootstrap$2.run abstractbootstrap.java:348 at io.netty.util.concurrent.singlethreadeventexecutor.runalltasks singlethreadeventexecutor.java:357 at io.netty.channel.nio.nioeventloop.run nioeventloop.java:357 at io.netty.util.concurrent.singlethreadeventexecutor$2.run singlethreadeventexecutor.java:111 at java.lang.thread.run thread.java:745"
    },
    {
        "url": "https://stackoverflow.com/questions/37190934",
        "text": "your hostname, spark master resolves to a loopback address: using instead on interface warn utils: set spark_local_ip if you need to bind to another address"
    },
    {
        "url": "https://stackoverflow.com/questions/37190934",
        "text": "warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port warn utils: service could not bind on port attempting port exception in thread java.net.bindexception: cannot assign requested address: service failed after retries!"
    },
    {
        "url": "https://stackoverflow.com/questions/37208269",
        "text": "keystone.cmd.cli created domain keystone.cmd.cli created project admin debug passlib.registry registered handler: class register_crypt_handler keystone.cmd.cli created user admin keystone.cmd.cli created role admin keystone.cmd.cli granted admin on admin to user admin. keystone.cmd.cli created region regionone keystone.cmd.cli created admin endpoint keystone.cmd.cli created internal endpoint keystone.cmd.cli created public endpoint stack.sh:main:1007 is_service_enabled tls proxy common:is_service_enabled:2055 return stack.sh:main:1016 cat stack.sh:main:1031 source export os_identity_api_version=3 os_identity_api_version=3 export export os_username=admin os_username=admin export os_user_domain_id=default os_user_domain_id=default export os_password=nomoresecret os_password=nomoresecret export os_project_name=admin os_project_name=admin export os_project_domain_id=default os_project_domain_id=default export os_region_name=regionone os_region_name=regionone stack.sh:main:1033 create_keystone_accounts keystone:create_keystone_accounts:372 local admin_tenant keystone:create_keystone_accounts:373 openstack project show admin f value c id discovering versions from the identity service failed when creating the password plugin. attempting to determine version from url. could not determine a suitable url for the plugin keystone:create_keystone_accounts:373 admin_tenant= keystone:create_keystone_accounts:1 exit_trap stack.sh:exit_trap:474 local stack.sh:exit_trap:475 jobs p stack.sh:exit_trap:475 stack.sh:exit_trap:478 n stack.sh:exit_trap:484 kill_spinner stack.sh:kill_spinner:370 z stack.sh:exit_trap:486 ne stack.sh:exit_trap:487 echo on exit on exit stack.sh:exit_trap:488 generate subunit fail stack.sh:exit_trap:489 z stack.sh:exit_trap:492 d stack.sh:exit_trap:498 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/37340461",
        "text": "puppet apply detailed exitcodes notice: scope class tripleo::firewall::post : at this stage, all network traffic is blocked. warning: scope class swift : swift_hash_suffix has been deprecated and should be replaced with swift_hash_path_suffix, this will be removed warning: scope class nova::keystone::auth : note that service_name parameter value will be changed to compute service according future release. in case you use different value, please update your manifests accordingly. warning: scope class nova::keystone::auth : note that service_name_v3 parameter value will be changed to compute service acco in a future release. in case you use different value, please update your manifests accordingly. warning: scope class glance::api : the known_stores parameter is deprecated, use stores instead warning: scope class glance::api : default_store not provided, it will be automatically set to glance.store.filesystem.store warning: scope class nova::api : in n cycle, enabled_apis will have to be an array of apis to enable. warning: scope class neutron::server : identity_uri, auth_tenant, auth_user, auth_password, auth_region configuration options are deprecateted options warning: scope class neutron::agents::dhcp : the dhcp_domain parameter is deprecated and will be removed in future releases warning: scope class heat : value for rabbit_heartbeat_timeout_threshold parameter is different from openstack project defaults warning: scope class heat : configuration options are deprecated in favor of auth_plugi warning: scope class nova::network::neutron : neutron_auth_plugin parameter is deprecated and will be removed in a future release, use neut error: could not find class ::ironic::drivers::deploy for instack on node instack error: could not find class ::ironic::drivers::deploy for instack on node instack set e echo puppet apply exited with exit code puppet apply exited with exit code a exit os refresh config during configure phase. command status os refresh config aborting... traceback most recent call last : file string , line in module file line in install _run_orc instack_env file line in _run_orc _run_live_command args, instack_env, file line in _run_live_command raise runtimeerror failed. see log for details. name runtimeerror: os refresh config failed. see log for details. command returned non zero exit status"
    },
    {
        "url": "https://stackoverflow.com/questions/37896895",
        "text": "oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying... oslo_messaging._drivers.impl_rabbit the exchange to reply to reply_9270c332921a4fadb6b370fec2ecce16 doesn t exist yet, retrying..."
    },
    {
        "url": "https://stackoverflow.com/questions/37995117",
        "text": "sahara.service.ops instance: none, cluster: during operating on cluster reason: heat stack failed with status resource create failed: resourceinerror: resources.vanilla master floating.resources 0 .resources.inst: went to status due to message: no valid host was found. there are not enough hosts available., code: id: sahara.service.ops instance: none, cluster: traceback most recent call last : sahara.service.ops instance: none, cluster: file line in wrapper sahara.service.ops instance: none, cluster: f cluster_id, sahara.service.ops instance: none, cluster: file line in _provision_cluster sahara.service.ops instance: none, cluster: infra.create_cluster cluster sahara.service.ops instance: none, cluster: file line in create_cluster sahara.service.ops instance: none, cluster: self._launch_instances cluster, target_count, create_stages sahara.service.ops instance: none, cluster: file line in _launch_instances sahara.service.ops instance: none, cluster: cluster, target_count, update_stack, disable_rollback sahara.service.ops instance: none, cluster: file line in handler sahara.service.ops instance: none, cluster: add_fail_event instance, e sahara.service.ops instance: none, cluster: file line in __exit__ sahara.service.ops instance: none, cluster: self.force_reraise sahara.service.ops instance: none, cluster: file line in force_reraise sahara.service.ops instance: none, cluster: six.reraise self.type_, self.value, self.tb sahara.service.ops instance: none, cluster: file line in handler sahara.service.ops instance: none, cluster: value func sahara.service.ops instance: none, cluster: file line in _create_instances sahara.service.ops instance: none, cluster: last_updated_time=stack.last_updated_time sahara.service.ops instance: none, cluster: file line in wait_stack_completion sahara.service.ops instance: none, cluster: raise ex.heatstackexception stack.stack_status_reason sahara.service.ops instance: none, cluster: heatstackexception: heat stack failed with status resource create failed: resourceinerror: resources.vanilla master floating.resources 0 .resources.inst: went to status due to message: no valid host was found. there are not enough hosts available., code: sahara.service.ops instance: none, cluster: id: sahara.service.ops instance: none, cluster: sahara.service.heat.heat_engine instance: none, cluster: cluster creation rollback reason: heat stack failed with status resource create failed: resourceinerror: resources.vanilla master floating.resources 0 .resources.inst: went to status due to message: no valid host was found. there are not enough hosts available., code: id: sahara.utils.cluster instance: none, cluster: cluster status has been changed. new"
    },
    {
        "url": "https://stackoverflow.com/questions/38046214",
        "text": "nova.osapi_compute.wsgi.server get http status: len: time: nova.osapi_compute.wsgi.server get http status: len: time: nova.osapi_compute.wsgi.server get http status: len: time: nova.osapi_compute.wsgi.server get http status: len: time:"
    },
    {
        "url": "https://stackoverflow.com/questions/38046214",
        "text": "tail keystone.common.wsgi get oslo_log.versionutils deprecated: get_users of the api is deprecated as of mitaka in favor of a similar function in the api and may be removed in q. keystone.common.wsgi get keystone.common.wsgi post keystone.common.wsgi get keystone.common.wsgi get keystone.common.wsgi post keystone.common.wsgi acfb get keystone.common.wsgi get keystone.common.wsgi get"
    },
    {
        "url": "https://stackoverflow.com/questions/38249032",
        "text": "error: rpmdb: thread process failed: thread died in berkeley db library error: from dbenv failchk: db_runrecovery: fatal error, run database recovery error: cannot open packages index using error: cannot open packages database in critical:yum.main: error: rpmdb open failed"
    },
    {
        "url": "https://stackoverflow.com/questions/38495905",
        "text": "obtaining horizon from from r line cloning git: to . fatal: unable to connect to github.com: github.com 0: refused command git clone q git: failed with code in none python:pip_install:1 exit_trap stack.sh:exit_trap:474 local stack.sh:exit_trap:475 jobs p stack.sh:exit_trap:475 stack.sh:exit_trap:478 n stack.sh:exit_trap:484 kill_spinner stack.sh:kill_spinner:370 z stack.sh:exit_trap:486 ne stack.sh:exit_trap:487 echo on exit on exit stack.sh:exit_trap:488 generate subunit fail stack.sh:exit_trap:489 z stack.sh:exit_trap:492 d stack.sh:exit_trap:498 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/38648369",
        "text": "internal server error: thu jul traceback most recent call last : thu jul file line in get_response thu jul response wrapped_callback request, thu jul file line in dec thu jul return view_func request, thu jul file line in dec thu jul return view_func request, thu jul file line in dec thu jul return view_func request, thu jul file line in dec thu jul return view_func request, thu jul file line in dec thu jul return view_func request, thu jul file line in view thu jul return self.dispatch request, thu jul file line in dispatch thu jul return handler request, thu jul file line in get thu jul handled self.construct_tables thu jul file line in construct_tables thu jul handled self.handle_table table thu jul file line in handle_table thu jul data self._get_data_dict thu jul file line in _get_data_dict thu jul self._data self.table_class._meta.name: self.get_data thu jul file line in get_data thu jul data super globaloverview, self .get_data thu jul file line in get_data thu jul self.usage.get_limits thu jul file line in get_limits thu jul self.get_neutron_limits thu jul file line in get_neutron_limits thu jul api.neutron.is_security_group_extension_supported self.request thu jul file line in is_security_group_extension_supported thu jul return is_extension_supported request, thu jul file line in wrapped thu jul value cache key func thu jul file line in is_extension_supported thu jul extensions list_extensions request thu jul file line in wrapped thu jul value cache key func thu jul file line in list_extensions thu jul extensions_list neutronclient request .list_extensions thu jul file line in with_params thu jul ret self.function instance, thu jul file line in list_extensions thu jul return self.get self.extensions_path, thu jul file line in get thu jul thu jul file line in get thu jul thu jul file line in retry_request thu jul thu jul file line in do_request thu jul self._handle_fault_response status_code, replybody thu jul file line in _handle_fault_response thu jul exception_handler_v20 status_code, des_error_body thu jul file line in exception_handler_v20 thu jul thu jul neutronclientexception: service unavailable thu jul no server is available to handle this request. thu jul"
    },
    {
        "url": "https://stackoverflow.com/questions/38648369",
        "text": "neutron.common.config config paste file: neutron.manager loading core plugin: neutron_plugin_contrail.plugins.opencontrail.contrail_plugin.neutronplugincontrailcorev2 neutron.manager service is supported by the core plugin neutron.manager loading plugin: neutron_plugin_contrail.plugins.opencontrail.loadbalancer.plugin.loadbalancerplugin starting new http connection :"
    },
    {
        "url": "https://stackoverflow.com/questions/38719373",
        "text": "common:service_check:1633 local service common:service_check:1634 local failures common:service_check:1635 screen_name=stack common:service_check:1636 service_dir= common:service_check:1639 ! d common:service_check:1646 ls common:service_check:1646 common:service_check:1648 for service in common:service_check:1649 basename common:service_check:1649 cpu.failure common:service_check:1650 cpu common:service_check:1651 echo error: service n cpu is not running error: service n cpu is not running common:service_check:1654 n common:service_check:1655 die more details about the above errors can be found with screen common:die:186 local call trace . stack.sh:1346:service_check more details about the above errors can be found with screen on exit"
    },
    {
        "url": "https://stackoverflow.com/questions/38770138",
        "text": "cinder.openstack.common.scheduler.base_filter filter availabilityzonefilter returned host s cinder.scheduler.flows.create_volume failed to run task cinder.scheduler.flows.create_volume.schedulecreatevolumetask;volume:create: no valid host was found. no weighed hosts available debug cinder.volume.flows.common updating volume: with due to: no valid host was found. no weighed hosts available _update_object"
    },
    {
        "url": "https://stackoverflow.com/questions/38922184",
        "text": "nova.virt.libvirt.driver instance: migration operation has aborted nova.virt.libvirt.driver instance: live migration failure: internal error: unable to execute qemu command migration disabled: failed to allocate shared memory"
    },
    {
        "url": "https://stackoverflow.com/questions/38922184",
        "text": ": qemumonitorjsongetmigrationstatsreply:2443 : internal error: migration reply was missing return status : virnetclientprogramdispatcherror:177 : cannot open log file: device or resource busy"
    },
    {
        "url": "https://stackoverflow.com/questions/39272874",
        "text": "wed aug core:notice command line: d foreground wed aug mpm_prefork:notice caught sigwinch, shutting down gracefully wed aug core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 wed aug suexec:notice suexec mechanism enabled wrapper: wed aug so:warn module is already loaded, skipping wed aug so:warn module wsgi_module is already loaded, skipping wed aug auth_digest:notice generating secret for digest authentication ... wed aug mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations wed aug core:notice command line: d foreground wed aug mpm_prefork:notice caught sigwinch, shutting down gracefully wed aug core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 wed aug suexec:notice suexec mechanism enabled wrapper: wed aug so:warn module is already loaded, skipping wed aug so:warn module wsgi_module is already loaded, skipping wed aug auth_digest:notice generating secret for digest authentication ... wed aug mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations wed aug core:notice command line: d foreground thu sep core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 thu sep suexec:notice suexec mechanism enabled wrapper: thu sep so:warn module is already loaded, skipping thu sep so:warn module wsgi_module is already loaded, skipping thu sep auth_digest:notice generating secret for digest authentication ... thu sep mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations thu sep core:notice command line: d foreground thu sep mpm_prefork:notice caught sigwinch, shutting down gracefully thu sep core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 thu sep suexec:notice suexec mechanism enabled wrapper: thu sep so:warn module is already loaded, skipping thu sep so:warn module wsgi_module is already loaded, skipping thu sep auth_digest:notice generating secret for digest authentication ... thu sep mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations thu sep core:notice command line: d foreground thu sep mpm_prefork:notice caught sigwinch, shutting down gracefully thu sep core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 thu sep suexec:notice suexec mechanism enabled wrapper: thu sep so:warn module is already loaded, skipping thu sep so:warn module wsgi_module is already loaded, skipping thu sep auth_digest:notice generating secret for digest authentication ... thu sep mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations thu sep core:notice command line: d foreground thu sep mpm_prefork:notice caught sigwinch, shutting down gracefully thu sep core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 thu sep suexec:notice suexec mechanism enabled wrapper: thu sep so:warn module is already loaded, skipping thu sep so:warn module wsgi_module is already loaded, skipping thu sep auth_digest:notice generating secret for digest authentication ... thu sep mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations thu sep core:notice command line: d foreground thu sep mpm_prefork:notice caught sigwinch, shutting down gracefully thu sep core:notice selinux policy enabled; httpd running as context system_u:system_r:httpd_t:s0 thu sep suexec:notice suexec mechanism enabled wrapper: thu sep so:warn module is already loaded, skipping thu sep so:warn module wsgi_module is already loaded, skipping thu sep auth_digest:notice generating secret for digest authentication ... thu sep mpm_prefork:notice apache centos php mod_wsgi 3.4 python configured resuming normal operations thu sep core:notice command line: d foreground thu sep mpm_prefork:notice caught sigwinch, shutting down gracefully"
    },
    {
        "url": "https://stackoverflow.com/questions/39383334",
        "text": "warning: could not import horizon dependencies. this is normal during installation. warning:root:no local_settings file found. traceback most recent call last : file line in module execute_from_command_line sys.argv file line in execute_from_command_line utility.execute file line in execute settings.installed_apps file line in __getattr__ self._setup name file line in _setup self._wrapped settings settings_module file line in __init__ self.settings_module, e importerror: could not import settings is it on sys.path? is there an import in the settings file? : no module named angular_fileupload exit_trap"
    },
    {
        "url": "https://stackoverflow.com/questions/39533460",
        "text": "keystone.cmd.cli created domain keystone.cmd.cli created project admin debug passlib.registry registered handler: class register_crypt_handler keystone.cmd.cli created user admin keystone.cmd.cli created role admin keystone.cmd.cli granted admin on admin to user admin. keystone.cmd.cli created region regionone keystone.cmd.cli created admin endpoint keystone.cmd.cli created internal endpoint keystone.cmd.cli created public endpoint stack.sh:main:1038 cat stack.sh:main:1053 is_service_enabled tls proxy common:is_service_enabled:2078 return stack.sh:main:1057 source export os_identity_api_version=3 os_identity_api_version=3 export export os_username=admin os_username=admin export os_password=secret os_password=secret export os_project_name=admin os_project_name=admin export os_region_name=regionone os_region_name=regionone stack.sh:main:1059 create_keystone_accounts keystone:create_keystone_accounts:384 local admin_project keystone:create_keystone_accounts:385 openstack project show admin f value c id discovering versions from the identity service failed when creating the password plugin. attempting to determine version from url. could not determine a suitable url for the plugin keystone:create_keystone_accounts:385 admin_project= keystone:create_keystone_accounts:1 exit_trap stack.sh:exit_trap:487 local stack.sh:exit_trap:488 jobs p stack.sh:exit_trap:488 stack.sh:exit_trap:491 n stack.sh:exit_trap:497 kill_spinner stack.sh:kill_spinner:383 z stack.sh:exit_trap:499 ne stack.sh:exit_trap:500 echo on exit on exit stack.sh:exit_trap:501 generate subunit fail stack.sh:exit_trap:502 z stack.sh:exit_trap:505 devstack d stack.sh:exit_trap:511 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/39535873",
        "text": "cinder.scheduler.flows.create_volume failed to run task cinder.scheduler.flows.create_volume.schedulecreatevolumetask;volume:create: no valid host was found. no weighed hosts available"
    },
    {
        "url": "https://stackoverflow.com/questions/39597296",
        "text": "keystonemiddleware.auth_token fetch revocation list failed, fallback to online validation. keystonemiddleware.auth_token bad response code while validating token: keystonemiddleware.auth_token identity response: you are not authorized to perform the requested action, identity:validate_token. , critical keystonemiddleware.auth_token unable to validate token: failed to fetch token data from identity server"
    },
    {
        "url": "https://stackoverflow.com/questions/39631307",
        "text": "oslo.messaging._drivers.impl_rabbit reconnecting to amqp server on localhost:5672 oslo.messaging._drivers.impl_rabbit delaying reconnect for seconds... oslo.messaging._drivers.impl_rabbit amqp server on localhost:5672 is unreachable: errno econnrefused. trying again in seconds..."
    },
    {
        "url": "https://stackoverflow.com/questions/39826272",
        "text": "collecting hacking from r line retrying retry after connection broken by pip._vendor.requests.packages.urllib3.connection.verifiedhttpsconnection object at : failed to establish a new connection: errno temporary failure in name resolution retrying retry after connection broken by pip._vendor.requests.packages.urllib3.connection.verifiedhttpsconnection object at : failed to establish a new connection: errno temporary failure in name resolution retrying retry after connection broken by pip._vendor.requests.packages.urllib3.connection.verifiedhttpsconnection object at : failed to establish a new connection: errno temporary failure in name resolution retrying retry after connection broken by pip._vendor.requests.packages.urllib3.connection.verifiedhttpsconnection object at : failed to establish a new connection: errno temporary failure in name resolution retrying retry after connection broken by pip._vendor.requests.packages.urllib3.connection.verifiedhttpsconnection object at : failed to establish a new connection: errno temporary failure in name resolution could not find a version that satisfies the requirement hacking from r line from versions: no matching distribution found for hacking from r line python:pip_install:1 exit_trap stack.sh:exit_trap:487 local stack.sh:exit_trap:488 jobs p stack.sh:exit_trap:488 stack.sh:exit_trap:491 n stack.sh:exit_trap:497 kill_spinner stack.sh:kill_spinner:383 z stack.sh:exit_trap:499 ne stack.sh:exit_trap:500 echo on exit on exit stack.sh:exit_trap:501 generate subunit fail stack.sh:exit_trap:502 z stack.sh:exit_trap:505 d stack.sh:exit_trap:511 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/40023981",
        "text": "n api.log: debug nova.compute.api admin admin instance: block_device_mapping blockdevicemapping boot_index=0,connection_info=none,created_at=,delete_on_termination=true,deleted=,deleted_at=,destination_type= blockdevicemapping boot_index= from _bdm_validate_set_size_and_instance n api.log: debug nova.compute.api admin admin instance: fetching instance by uuid from get n api.log: debug neutronclient.v2_0.client admin admin get call to neutron for used request id from _append_request_id n api.log: nova.osapi_compute.wsgi.server admin admin get http status: len: time: n api.log: debug neutronclient.v2_0.client admin admin get call to neutron for device_id=d6c67c2f eba852ed0cc1 used request id from _append_request_id n api.log: debug neutronclient.v2_0.client admin admin get call to neutron for device_id=d6c67c2f eba852ed0cc1 used request id from _append_request_id n cond.log: nova.scheduler.utils admin admin instance: setting instance to state. n cond.log: debug nova.network.neutronv2.api admin admin instance: deallocate_for_instance from deallocate_for_instance n cond.log: debug neutronclient.v2_0.client admin admin get call to neutron for used request id from _append_request_id n cond.log: debug nova.network.neutronv2.api admin admin instance: instance cache missing network info. from _get_preexisting_port_ids n cond.log: debug nova.network.base_api admin admin instance: updating instance_info_cache with network_info: from update_instance_cache_with_nw_info grep: n dhcp.log: no such file or directory n sch.log: debug nova.filters admin admin filtering removed all hosts for the request with instance id filter results: u u u u u u none from get_filtered_objects n sch.log: nova.filters admin admin filtering removed all hosts for the request with instance id filter results: retryfilter: start: end: , availabilityzonefilter: start: end: , ramfilter: start: end: , diskfilter: start: end: q svc.log: neutron.wsgi admin get http q svc.log: neutron.wsgi admin get http q svc.log: neutron.wsgi admin get device_id=d6c67c2f eba852ed0cc1 http q svc.log: neutron.wsgi admin get device_id=d6c67c2f eba852ed0cc1 http"
    },
    {
        "url": "https://stackoverflow.com/questions/40417479",
        "text": "nova.servicegroup.drivers.db dbconnectionerror: pymysql.err.operationalerror connect to mysql server on errno econnrefused oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno econnrefused. trying again in"
    },
    {
        "url": "https://stackoverflow.com/questions/40865067",
        "text": "cinder.volume.manager bfca none none determined volume db was empty at startup. debug cinder.volume.manager bfca none none cinder volume db check: vol_db_empty=true from __init__ cinder.keymgr.conf_key_mgr bfca none none this key manager is insecure and is not recommended for production deployments cinder.cmd.volume bfca none none volume service iscsi failed to start. trace cinder.cmd.volume traceback most recent call last : trace cinder.cmd.volume file line in main trace cinder.cmd.volume trace cinder.cmd.volume file line in create trace cinder.cmd.volume trace cinder.cmd.volume file line in __init__ trace cinder.cmd.volume trace cinder.cmd.volume file line in __init__ trace cinder.cmd.volume active_backend_id=curr_active_backend_id trace cinder.cmd.volume file line in import_object trace cinder.cmd.volume return import_class import_str *args, trace cinder.cmd.volume file line in __init__ trace cinder.cmd.volume self.configuration.ixsystems_iqn_prefix trace cinder.cmd.volume file line in __getattr__ trace cinder.cmd.volume return getattr local_conf, value trace cinder.cmd.volume file line in __getattr__ trace cinder.cmd.volume return self._conf._get name, self._group trace cinder.cmd.volume file line in _get trace cinder.cmd.volume value self._do_get name, group, namespace trace cinder.cmd.volume file line in _do_get trace cinder.cmd.volume if opt.mutable and namespace is none: trace cinder.cmd.volume attributeerror: object has no attribute trace cinder.cmd.volume cinder.cmd.volume bfca none none no volume service s started successfully, terminating. c vol failed to start"
    },
    {
        "url": "https://stackoverflow.com/questions/40867000",
        "text": "slave went offline during the build error: connection was broken: java.io.ioexception: unexpected termination of the channel at hudson.remoting.synchronouscommandtransport$readerthread.run synchronouscommandtransport.java:50 caused by: java.io.eofexception at java.io.objectinputstream$peekinputstream.readfully objectinputstream.java:2351 at java.io.objectinputstream$blockdatainputstream.readshort objectinputstream.java:2820 at java.io.objectinputstream.readstreamheader objectinputstream.java:804 at java.io.objectinputstream. init objectinputstream.java:302 at hudson.remoting.objectinputstreamex. init objectinputstreamex.java:48 at hudson.remoting.abstractsynchronousbytearraycommandtransport.read abstractsynchronousbytearraycommandtransport.java:34 at hudson.remoting.synchronouscommandtransport$readerthread.run synchronouscommandtransport.java:48 build step execute shell marked build as failure"
    },
    {
        "url": "https://stackoverflow.com/questions/40934239",
        "text": "recoverable error: httpconnectionpool : max retries exceeded with url: caused by newconnectionerror requests.packages.urllib3.connection.httpconnection object at : failed to establish a new connection: errno connection refused , debug:keystoneauth.session:req: curl g i x get h user agent: python keystoneclient h accept: application h x auth token: debug:keystoneauth.session:resp: date: fri, dec gmt server: apache ubuntu vary: x auth token x openstack request id: content length: keep alive: connection: keep alive content type: application json resp body: null, null , false, bootstrap project for initializing the cloud. , true, false, true, false, true, call to list enabled services failed. this is likely due to a problem communicating with the neutron endpoint. load balancers panel will not be displayed. call to list enabled services failed. this is likely due to a problem communicating with the neutron endpoint. firewalls panel will not be displayed. call to list enabled services failed. this is likely due to a problem communicating with the neutron endpoint. vpn panel will not be displayed. call to list supported extensions failed. this is likely due to a problem communicating with the nova endpoint. host aggregates panel will not be displayed"
    },
    {
        "url": "https://stackoverflow.com/questions/41064739",
        "text": "migrate.versioning.api none none migrate.versioning.api none none done migrate.versioning.api none none migrate.versioning.api none none done migrate.versioning.api none none migrate.versioning.api none none done no hosts found to map to cell, exiting. no hosts found to map to cell, exiting. nova:create_cell:1 exit_trap stack.sh:exit_trap:487 local stack.sh:exit_trap:488 jobs p stack.sh:exit_trap:488 stack.sh:exit_trap:491 n stack.sh:exit_trap:497 kill_spinner stack.sh:kill_spinner:383 z stack.sh:exit_trap:499 ne stack.sh:exit_trap:500 echo on exit on exit stack.sh:exit_trap:501 generate subunit fail stack.sh:exit_trap:502 z stack.sh:exit_trap:505 d world dumping... see for details stack.sh:exit_trap:511 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/41444866",
        "text": ".. complete! info: running install_centos_stable_post info: running install_centos_check_services info: running install_centos_restart_daemons info: running daemons_running info: salt installed! cc_scripts_user.py : failed to run module scripts user scripts in util.py : running scripts user module from failed cloud init v. finished at mon, jan datasource datasourceopenstack net,ver=2 up seconds"
    },
    {
        "url": "https://stackoverflow.com/questions/41456786",
        "text": "stack.sh:main:1033 create_keystone_accounts keystone:create_keystone_accounts:372 local admin_tenant keystone:create_keystone_accounts:373 openstack project show admin f value c id traceback most recent call last : file line in from openstackclient.shell import main file line in from openstackclient.common import clientmanager file line in file line in get_plugin_modules import ep.module_name file line in from openstack import connection file line in from openstack import session as _session file line in default_user_agent openstack.version attributeerror: object has no attribute keystone:create_keystone_accounts:373 admin_tenant= keystone:create_keystone_accounts:1 exit_trap stack.sh:exit_trap:474 local stack.sh:exit_trap:475 jobs p stack.sh:exit_trap:475 stack.sh:exit_trap:478 n stack.sh:exit_trap:484 kill_spinner stack.sh:kill_spinner:370 z stack.sh:exit_trap:486 ne stack.sh:exit_trap:487 echo on exit on exit stack.sh:exit_trap:488 generate subunit fail stack.sh:exit_trap:489 z stack.sh:exit_trap:492 d stack.sh:exit_trap:498 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/41548799",
        "text": "keystone.notifications failed to construct notifier trace keystone.notifications traceback most recent call last : trace keystone.notifications file line in _get_notifier trace keystone.notifications transport messaging.get_transport conf trace keystone.notifications file line in get_transport trace keystone.notifications invoke_kwds=kwargs trace keystone.notifications file line in __init__ trace keystone.notifications verify_requirements=verify_requirements, trace keystone.notifications file line in __init__ trace keystone.notifications verify_requirements trace keystone.notifications file line in _load_plugins trace keystone.notifications self._on_load_failure_callback self, ep, err trace keystone.notifications file line in _default_on_load_failure trace keystone.notifications raise err trace keystone.notifications importerror: cannot import name _uuid_generate_random trace keystone.notifications keystone.common.wsgi authorization failed. the request you have made requires authentication. from eventlet.wsgi.server post http"
    },
    {
        "url": "https://stackoverflow.com/questions/42270910",
        "text": "stack.sh:main:1349 b m x stack.sh:main:1350 b m echo running user script running user script stack.sh:main:1351 b m line no such file or directory line no such file or directory line no such file or directory line no such file or directory line no such file or directory line no such file or directory line no such file or directory line no such file or directory line no such file or directory warning: setting legacy os_tenant_name to support cli tools. warning: setting legacy os_tenant_name to support cli tools. field value os flv disabled:disabled false os flv ext data:ephemeral disk id name os flavor access:is_public true properties ram rxtx_factor swap vcpus more than one securitygroup exists with the name more than one securitygroup exists with the name stack.sh:main:1351 b m err_trap stack.sh:err_trap:517 b m local stack.sh failed: full log in on exit"
    },
    {
        "url": "https://stackoverflow.com/questions/42298423",
        "text": "create: templateresource stack heat.engine.resource traceback most recent call last : heat.engine.resource file line in _action_recorder heat.engine.resource yield heat.engine.resource file line in _do_action heat.engine.resource yield self.action_handler_task action, heat.engine.resource file line in wrapper heat.engine.resource step next subtask heat.engine.resource file line in action_handler_task heat.engine.resource done check handler_data heat.engine.resource file line in check_create_complete heat.engine.resource return self._check_status_complete self.create heat.engine.resource file line in _check_status_complete heat.engine.resource heat.engine.resource resourcefailure: resources : stack create cancelled heat.engine.resource heat.engine.stack stack create failed swarm cluster swarm_masters ogdsyvluvmfn : resource create failed: resources : stack create cancelled heat.engine.stack failed to set state of stack swarm cluster swarm_masters ogdsyvluvmfn with traversal id to create_failed"
    },
    {
        "url": "https://stackoverflow.com/questions/42298423",
        "text": "oslo.service.loopingcall fixed interval looping call failed oslo.service.loopingcall traceback most recent call last : oslo.service.loopingcall file line in _run_loop oslo.service.loopingcall result func oslo.service.loopingcall file line in poll_and_check oslo.service.loopingcall stack self.openstack_client.heat .stacks.get self.cluster.stack_id oslo.service.loopingcall file line in get oslo.service.loopingcall resp self.client.get stack_id, oslo.service.loopingcall file line in get oslo.service.loopingcall return self.client_request url, oslo.service.loopingcall file line in client_request oslo.service.loopingcall resp, body self.json_request method, url, oslo.service.loopingcall file line in json_request oslo.service.loopingcall resp self._http_request url, method, oslo.service.loopingcall file line in _http_request oslo.service.loopingcall resp.content oslo.service.loopingcall httpunauthorized: error: authentication failed: the request you have made requires authentication. , oslo.service.loopingcall"
    },
    {
        "url": "https://stackoverflow.com/questions/42316142",
        "text": "openstack cat grep heat api feb controller openstack heat api: option from group is deprecated for removal. its value may be silently ignored in the future. feb controller openstack heat api: oslo_config.cfg option from group is deprecated for removal. its value may be silently ignored in the future. feb controller openstack heat api: error: unable to locate config file feb controller openstack systemd: openstack heat api.service: main process exited, failure feb controller openstack systemd: unit openstack heat api.service entered failed state. feb controller openstack systemd: openstack heat api.service failed. feb controller openstack heat api cfn: option from group is deprecated for removal. its value may be silently ignored in the future. feb controller openstack heat api cfn: oslo_config.cfg option from group is deprecated for removal. its value may be silently ignored in the future. feb controller openstack heat api cfn: error: unable to locate config file feb controller openstack systemd: openstack heat api cfn.service: main process exited, failure feb controller openstack systemd: unit openstack heat api cfn.service entered failed state. feb controller openstack systemd: openstack heat api cfn.service failed."
    },
    {
        "url": "https://stackoverflow.com/questions/42316142",
        "text": "openstack cat grep rpc rpc_backend=heat.openstack.common.rpc.impl_qpid openstack cat ... critical heat api cfn driverloadfailure: failed to load transport driver no driver found, looking for heat api cfn traceback most recent call last : heat api cfn file line in module heat api cfn sys.exit main heat api cfn file line in main heat api cfn messaging.setup heat api cfn file line in setup heat api cfn cfg.conf, url, allowed_remote_exmods=exmods, heat api cfn file line in get_transport heat api cfn raise driverloadfailure url.transport, ex heat api cfn driverloadfailure: failed to load transport driver no driver found, looking for heat api cfn ..."
    },
    {
        "url": "https://stackoverflow.com/questions/42460706",
        "text": "task: rabbitmq fail hostname has to resolve to ip address of api_interface failed: localhost u u u u u u , u u u u stream dgram raw , u false, u u , u u u , u getent desktop , u stream desktop , u dgram , u raw , u u true, false, , getent desktop , , , stream dgram raw , stream desktop , dgram , raw , msg: hostname has to resolve to ip address of api_interface fatal: all hosts have already failed aborting play recap to retry, use: limit localhost :"
    },
    {
        "url": "https://stackoverflow.com/questions/42726169",
        "text": "crit cinder dberror: pymysql.err.internalerror u unknown column in field list sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder dberror: pymysql.err.internalerror u unknown column in field list sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder dberror: pymysql.err.internalerror u unknown column in field list sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder bcac programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__ crit cinder programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select services.created_at as services_created_at, services.updated_at as services_updated_at, services.deleted_at as services_deleted_at, services.deleted as services_deleted, services.id as services_id, services.host as services_host, services.`binary` as services_binary, services.topic as services_topic, services.report_count as services_report_count, services.disabled as services_disabled, services.availability_zone as services_availability_zone, services.disabled_reason as services_disabled_reason, services.modified_at as services_modified_at, services.rpc_current_version as services_rpc_current_version, services.object_current_version as services_object_current_version, services.replication_status as services_replication_status, services.active_backend_id as services_active_backend_id, services.frozen as services_frozen services services.deleted false and services.host and services.`binary` parameters: cinder traceback most recent call last : cinder file line in module cinder sys.exit main cinder file line in main cinder server service.service.create binary= cinder file line in create cinder service_name=service_name cinder file line in __init__"
    },
    {
        "url": "https://stackoverflow.com/questions/42799761",
        "text": "keystone.common.wsgi get keystone.federation.controllers is not a trusted dashboard host keystone.common.wsgi authorization failed. the request you have made requires authentication. from"
    },
    {
        "url": "https://stackoverflow.com/questions/42799761",
        "text": "debug keystone.federation.utils cannot find in configuration group mapped. trying location in group federation. get_remote_id_parameter"
    },
    {
        "url": "https://stackoverflow.com/questions/42828482",
        "text": "murano.cmd.test_runner command failed: object has no attribute traceback most recent call last : file line in main exit_code test_runner.run_tests file line in run_tests ks_opts self._validate_keystone_opts self.args file line in _validate_keystone_opts ks_opts param ks_opts param .replace attributeerror: object has no attribute command failed: object has no attribute"
    },
    {
        "url": "https://stackoverflow.com/questions/42871576",
        "text": ":info::shell::94::root:: localhost executing script: ....... yum update y puppet hiera openssh clients tar nc puppet aodh puppet apache puppet barbican puppet cassandra puppet ceilometer puppet ceph puppet certmonger puppet cinder puppet collectd puppet concat puppet congress puppet contrail puppet corosync puppet datacat puppet puppet elasticsearch puppet firewall puppet fluentd puppet git puppet glance puppet gnocchi puppet haproxy puppet heat puppet horizon puppet inifile puppet ipaclient puppet ironic puppet java puppet kafka puppet keystone puppet puppet kmod puppet manila puppet memcached puppet midonet puppet mistral puppet module data puppet keepalived puppet mongodb puppet mysql puppet vsm puppet neutron puppet nova puppet nssdb puppet ntp puppet opendaylight puppet openstack_extras puppet openstacklib puppet oslo puppet ovn puppet pacemaker puppet panko puppet rabbitmq puppet redis puppet remote puppet rsync puppet sahara puppet sensu puppet snmp puppet ssh puppet staging puppet stdlib puppet swift puppet sysctl puppet systemd puppet tacker puppet tempest puppet timezone puppet tomcat puppet tripleo puppet trove puppet uchiwa puppet vcsrepo puppet vlan puppet vswitch puppet xinetd puppet zaqar puppet zookeeper puppet ..... rpm q whatprovides puppet vcsrepo rpm q whatprovides puppet vlan rpm q whatprovides puppet vswitch rpm q whatprovides puppet xinetd rpm q whatprovides puppet zaqar rpm q whatprovides puppet zookeeper rpm q whatprovides puppet rpm q whatprovides rpm q whatprovides ......... rpm: no arguments given for query t exit"
    },
    {
        "url": "https://stackoverflow.com/questions/42955733",
        "text": ": fluent engine.rb:126:block in configure: gem version : fluent supervisor.rb:369:rescue in main_process: config unknown output plugin run gem search rd fluent plugin to find plugins"
    },
    {
        "url": "https://stackoverflow.com/questions/43091643",
        "text": "glance.common.wsgi starting workers glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on"
    },
    {
        "url": "https://stackoverflow.com/questions/43091643",
        "text": "glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on keystonemiddleware.auth_token identity response: the request you have made requires authentication. , keystonemiddleware.auth_token identity response: the request you have made requires authentication. , critical keystonemiddleware.auth_token unable to validate token: identity server rejected authorization necessary to fetch token data eventlet.wsgi.server get http"
    },
    {
        "url": "https://stackoverflow.com/questions/43091643",
        "text": "wed mar wsgi:warn mod_wsgi: compiled for python wed mar wsgi:warn mod_wsgi: runtime using python wed mar mpm_event:notice apache ubuntu mod_wsgi 4.3.0 python configured resuming normal operations wed mar core:notice command line: wed mar mpm_event:notice caught sigterm, shutting down wed mar wsgi:warn mod_wsgi: compiled for python wed mar wsgi:warn mod_wsgi: runtime using python wed mar mpm_event:notice apache ubuntu mod_wsgi 4.3.0 python configured resuming normal operations wed mar core:notice command line: wed mar mpm_event:notice caught sigterm, shutting down wed mar wsgi:warn mod_wsgi: compiled for python wed mar wsgi:warn mod_wsgi: runtime using python wed mar mpm_event:notice apache ubuntu mod_wsgi 4.3.0 python configured resuming normal operations wed mar core:notice command line: wed mar mpm_event:notice caught sigterm, shutting down thu mar wsgi:warn mod_wsgi: compiled for python thu mar wsgi:warn mod_wsgi: runtime using python thu mar mpm_event:notice apache ubuntu mod_wsgi 4.3.0 python configured resuming normal operations thu mar core:notice command line:"
    },
    {
        "url": "https://stackoverflow.com/questions/43091643",
        "text": "glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on keystonemiddleware.auth_token identity response: the request you have made requires authentication. , keystonemiddleware.auth_token identity response: the request you have made requires authentication. , critical keystonemiddleware.auth_token unable to validate token: identity server rejected authorization necessary to fetch token data eventlet.wsgi.server get http"
    },
    {
        "url": "https://stackoverflow.com/questions/43091643",
        "text": "glance.common.wsgi starting workers glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on glance.common.wsgi started child eventlet.wsgi.server wsgi starting up on"
    },
    {
        "url": "https://stackoverflow.com/questions/43217761",
        "text": "nova.compute.manager instance: an occurred while refreshing the network cache. nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _heal_instance_info_cache nova.compute.manager instance: self.network_api.get_instance_nw_info context, instance nova.compute.manager instance: file line in get_instance_nw_info nova.compute.manager instance: nova.compute.manager instance: file line in get_instance_nw_info nova.compute.manager instance: result self._get_instance_nw_info context, instance, nova.compute.manager instance: file line in _get_instance_nw_info nova.compute.manager instance: nw_info self.network_rpcapi.get_instance_nw_info context, nova.compute.manager instance: file line in get_instance_nw_info nova.compute.manager instance: project_id=project_id nova.compute.manager instance: file line in call nova.compute.manager instance: nova.compute.manager instance: file line in _send nova.compute.manager instance: nova.compute.manager instance: file line in send nova.compute.manager instance: nova.compute.manager instance: file line in _send nova.compute.manager instance: result self._waiter.wait msg_id, timeout nova.compute.manager instance: file line in wait nova.compute.manager instance: message self.waiters.get msg_id, nova.compute.manager instance: file line in get nova.compute.manager instance: to message id msg_id nova.compute.manager instance: messagingtimeout: timed out waiting for a reply to message id"
    },
    {
        "url": "https://stackoverflow.com/questions/43217761",
        "text": "apr ip nova compute : debug oslo_messaging._drivers.amqpdriver call msg_id: exchange topic _send"
    },
    {
        "url": "https://stackoverflow.com/questions/43738250",
        "text": "may mm kube slave kubelet : reconciler.go:231 verifycontrollerattachedvolume operation started for volume spec.name: pod uid: may mm kube slave kubelet : nestedpendingoperations.go:262 operation for failed. no retries permitted until utc durationbeforeretry . error: volume spec.name: pod uid: is not yet attached according to node status may mm kube slave kubelet : kubelet.go:1549 unable to mount volumes for pod timeout expired waiting for volumes to attach mount for pod list of unattached unmounted mysql persistent storage ; skipping pod may mm kube slave kubelet : pod_workers.go:182 syncing pod skipping: timeout expired waiting for volumes to attach mount for pod list of unattached unmounted mysql persistent storage"
    },
    {
        "url": "https://stackoverflow.com/questions/43869659",
        "text": "swift upload object put failed: service unavailable first chars of response html service unavailable p the server is currently tail f ... may raspberrypi object server: stderr: accepted may raspberrypi object server: __call__ with put : most recent call last file line in __call__#012 res getattr self, req.method #012 file line in _timing_stats#012 resp func ctrl, file line in writer.put metadata #012 file line in super diskfilewriter, self ._put metadata, true file line in tpool_reraise self._finalize_put, metadata, target_path, cleanup file line in tpool_reraise#012 raise errno permission denied txn: may raspberrypi object server: stderr: put http txn:"
    },
    {
        "url": "https://stackoverflow.com/questions/43869659",
        "text": "may raspberrypi proxy server: expect: continue from object server txn: client_ip:"
    },
    {
        "url": "https://stackoverflow.com/questions/43869659",
        "text": "may raspberrypi object server: __call__ with put : most recent call last file line in __call__#012 res getattr self, req.method #012 file line in _timing_stats#012 resp func ctrl, file line in writer.put metadata #012 file line in super diskfilewriter, self ._put metadata, true file line in tpool_reraise self._finalize_put, metadata, target_path, cleanup file line in tpool_reraise#012 raise errno permission denied txn:"
    },
    {
        "url": "https://stackoverflow.com/questions/43870381",
        "text": "hello create container main debug o.j.rest.internal.invokehttpmethod invoking authenticationapi.authenticatewithtenantnameandcredentials main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request post http main debug jclouds.wire sensitive data in payload, use jclouds.wire.log.sensitive override to enable logging this data. main debug jclouds.headers post http main debug jclouds.headers accept: application json main debug jclouds.headers content type: application json main debug jclouds.headers content length: main debug o.j.h.i.javaurlhttpcommandexecutorservice receiving response http ok main debug jclouds.headers http ok main debug jclouds.headers keep alive: main debug jclouds.headers server: apache ubuntu main debug jclouds.headers connection: keep alive main debug jclouds.headers x openstack request id: main debug jclouds.headers vary: x auth token main debug jclouds.headers date: tue, may gmt main debug jclouds.headers content type: application json main debug jclouds.headers content length: main debug jclouds.wire bootstrap project for initializing the cloud. , true, , , , , , , , , , , , main debug o.j.o.k.v.s.regionidtourifromaccessfortypeandversion endpoints for apitype object store and version endpoint main debug o.j.rest.internal.invokehttpmethod invoking container:create main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main debug o.j.h.h.backofflimitedretryhandler retry delaying for ms: server error: abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main debug o.j.h.h.backofflimitedretryhandler retry delaying for ms: server error: abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main debug o.j.h.h.backofflimitedretryhandler retry delaying for ms: server error: abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main debug o.j.h.h.backofflimitedretryhandler retry delaying for ms: server error: abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main debug o.j.h.h.backofflimitedretryhandler retry delaying for ms: server error: abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http main debug o.j.h.i.javaurlhttpcommandexecutorservice sending request put http main debug jclouds.headers put http main debug jclouds.headers accept: application json main debug jclouds.headers x container meta main debug jclouds.headers x container meta main debug jclouds.headers x auth token: main o.j.h.h.backofflimitedretryhandler cannot retry after server error, command has exceeded retry limit abstract boolean org.jclouds.openstack.swift.v1.features.containerapi.create java.lang.string,org.jclouds.openstack.swift.v1.options.createcontaineroptions jclouds example, createcontaineroptions , x container meta , x container meta , , http"
    },
    {
        "url": "https://stackoverflow.com/questions/44092107",
        "text": "rally utility container rally verify create verifier type tempest name tempest verifier rally.api creating verifier rally.verification.manager cloning verifier repo from rally.verification.manager creating virtual environment. it may take a few minutes. rally.verification.utils failed cmd: rally.verification.utils output: obtaining file: collecting pbr!=2.1.0, from could not find a version that satisfies the requirement pbr!=2.1.0, from from versions: no matching distribution found for pbr!=2.1.0, from command failed, please check log for more"
    },
    {
        "url": "https://stackoverflow.com/questions/44127894",
        "text": "may poc desktop kube proxy : server.go:215 using iptables proxier. may poc desktop kube proxy : server.go:468 failed to retrieve node info: get dial tcp may poc desktop kube proxy : proxier.go:249 invalid nodeip, initialize kube proxy with as nodeip may poc desktop kube proxy : proxier.go:254 clustercidr not specified, unable to distinguish between internal and external traffic"
    },
    {
        "url": "https://stackoverflow.com/questions/44280173",
        "text": "httpd.service the apache http server loaded: loaded enabled; vendor preset: disabled active: failed result: exit code since wed cst; ago docs: man:httpd 8 man:apachectl 8 process: winch mainpid failure process: dforeground failure main pid: failure may systemd : starting the apache http server... may httpd : httpd: syntax on line of syntax on line of may systemd : httpd.service: main process exited, failure may kill : kill: cannot find process may systemd : httpd.service: control process exited, may systemd : failed to start the apache http server. may systemd : unit httpd.service entered failed state. may systemd : httpd.service failed. hint: some lines were ellipsized, use l to show in full."
    },
    {
        "url": "https://stackoverflow.com/questions/44412188",
        "text": "jun localhost proxy server: started child jun localhost proxy server: stderr: wsgi starting up on jun localhost proxy server: stderr: accepted jun localhost proxy server: stderr: get http txn: jun localhost proxy server: stderr: accepted jun localhost proxy server: insufficient storage txn: jun localhost proxy server: stderr: put http txn:"
    },
    {
        "url": "https://stackoverflow.com/questions/44412188",
        "text": "jun localhost account server: started child jun localhost container server: started child jun localhost object server: started child jun localhost object server: stderr: wsgi starting up on jun localhost account server: stderr: wsgi starting up on jun localhost container server: stderr: wsgi starting up on jun localhost object server: stderr: accepted jun localhost object server: stderr: error:root:filesystem at does not support most recent call last file line in write_metadata#012 metastr :xattr_size #012 file init .py , line in return xattr f .set attr, value, file init .py , line in return self._call _setxattr, _fsetxattr, name, value, options self.options #012 file init .py , line in return fd_func self.value, file line in raise file line in raise ioerror errno, strerror errno operation not supported jun localhost container server: stderr: accepted jun localhost object server: stderr: put http txn: jun localhost container server: stderr: put http txn:"
    },
    {
        "url": "https://stackoverflow.com/questions/44743183",
        "text": "fri jun :error remote mod_wsgi : exception occurred processing wsgi script fri jun :error remote traceback most recent call last : fri jun :error remote file line in __call__ fri jun :error remote self.load_middleware fri jun :error remote file line in load_middleware fri jun :error remote mw_instance mw_class fri jun :error remote file line in __init__ fri jun :error remote for url_pattern in get_resolver none .url_patterns: fri jun :error remote file line in url_patterns fri jun :error remote patterns getattr self.urlconf_module, self.urlconf_module fri jun :error remote file line in urlconf_module fri jun :error remote self._urlconf_module import_module self.urlconf_name fri jun :error remote file line in import_module fri jun :error remote __import__ name fri jun :error remote file line in module fri jun :error remote url r include fri jun :error remote file line in include fri jun :error remote urlconf_module import_module urlconf_module fri jun :error remote file line in import_module fri jun :error remote __import__ name fri jun :error remote file line in module fri jun :error remote from openstack_dashboard.api import base fri jun :error remote importerror: cannot import name base"
    },
    {
        "url": "https://stackoverflow.com/questions/45095918",
        "text": "critical neutron exception: failed to spawn rootwrap process. stderr: traceback most recent call last : file line in module sys.exit daemon file line in daemon return main run_daemon=true file line in main daemon_mod.daemon_start config, filters file line in daemon_start server manager.get_server file line in get_server self._authkey, self._serializer file line in __init__ self.listener listener file line in __init__ self._socket.bind address file line in meth return getattr self._sock,name *args socket.error: errno permission denied neutron traceback most recent call last : neutron file line in module neutron sys.exit main neutron file packages line neutron file line in module neutron sys.exit main neutron file line in main neutron agent_main.main neutron file line in main neutron manager linuxbridgemanager bridge_mappings, interface_mappings neutron file line in __init__ neutron self.check_vxlan_support neutron file line in check_vxlan_support neutron if self.vxlan_ucast_supported : neutron file line in vxlan_ucast_supported neutron test_iface self.ensure_vxlan seg_id neutron file line in ensure_vxlan"
    },
    {
        "url": "https://stackoverflow.com/questions/45177999",
        "text": "glance.db.sqlalchemy.migrate_repo.schema creating table image_members migrate.versioning.api done migrate.versioning.api migrate.versioning.api done migrate.versioning.api migrate.versioning.api done migrate.versioning.api migrate.versioning.api done migrate.versioning.api critical glance argumenterror: column type varchar on column is not compatible with"
    },
    {
        "url": "https://stackoverflow.com/questions/45297623",
        "text": "pre nova.compute.claims instance: attempting claim: memory mb, disk gb, vcpus cpu nova.compute.claims instance: total memory: mb, used: mb nova.compute.claims instance: memory limit: mb, free: mb nova.compute.claims instance: total disk: gb, used: gb nova.compute.claims instance: disk limit: gb, free: gb nova.compute.claims instance: total vcpu: vcpu, used: vcpu nova.compute.claims instance: vcpu limit not specified, defaulting to unlimited nova.compute.claims instance: claim successful nova.scheduler.client.report unable to refresh my resource provider record nova.network.neutronv2.api instance: updating port with attributes u nova.virt.osinfo cannot find os information reason: no configuration information found for operating system empty nova.virt.osinfo cannot find os information reason: no configuration information found for operating system empty nova.virt.libvirt.driver instance: creating image nova.virt.osinfo cannot find os information reason: no configuration information found for operating system empty os_vif successfully plugged vif vifbridge ? ,preserve_on_delete=false,vif_name= nova.virt.libvirt.driver connection event reason connection to libvirt lost: root original exception being dropped: traceback most recent call last , file line in return self._domain.createwithflags flags \\n , file line in result proxy_call self._autowrap, f, , file line in proxy_call\\n rv execute f, , file line in six.reraise c, e, tb , file line in rv meth , file line in if ret raise libvirterror virdomaincreatewithflags , , libvirterror: end of file while reading data: input output nova.virt.libvirt.driver instance: failed to start libvirt guest os_vif successfully unplugged vif vifbridge ? ,preserve_on_delete=false,vif_name= nova.virt.libvirt.driver instance: deleting instance files nova.virt.libvirt.driver instance: deletion of complete nova.virt.libvirt.driver connection event reason failed to connect to libvirt nova.virt.libvirt.host connection to libvirt failed: failed to connect socket to connection refused nova.virt.libvirt.host traceback most recent call last : nova.virt.libvirt.host file line in get_connection nova.virt.libvirt.host conn self._get_connection nova.virt.libvirt.host file line in _get_connection nova.virt.libvirt.host wrapped_conn self._get_new_connection nova.virt.libvirt.host file line in _get_new_connection nova.virt.libvirt.host wrapped_conn self._connect self._uri, self._read_only nova.virt.libvirt.host file line in _connect nova.virt.libvirt.host libvirt.openauth, uri, auth, flags nova.virt.libvirt.host file line in proxy_call nova.virt.libvirt.host rv execute f, nova.virt.libvirt.host file line in execute nova.virt.libvirt.host six.reraise c, e, tb nova.virt.libvirt.host file line in tworker nova.virt.libvirt.host rv meth nova.virt.libvirt.host file line in openauth nova.virt.libvirt.host if ret is none:raise libvirterror virconnectopenauth failed nova.virt.libvirt.host libvirterror: failed to connect socket to connection refused nova.virt.libvirt.host root original exception being dropped: traceback most recent call last , file line in _create_domain_and_network\\n post_xml_callback=post_xml_callback \\n file line in _create_domain\\n guest.launch pause=pause file line in self._encoded_xml, file line in _encoded_xml\\n return encodeutils.safe_decode self._domain.xmldesc 0 \\n , file line in result proxy_call self._autowrap, f, , file line in proxy_call\\n rv execute f, , file line in six.reraise c, e, tb , file line in rv meth , file line in if ret is none: raise libvirterror virdomaingetxmldesc , , libvirterror: internal error: client socket is nova.compute.manager instance: instance failed to spawn nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _build_resources nova.compute.manager instance: yield resources nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: block_device_info=block_device_info nova.compute.manager instance: file line in spawn nova.compute.manager instance: destroy_disks_on_failure=true nova.compute.manager instance: file line in _create_domain_and_network nova.compute.manager instance: destroy_disks_on_failure nova.compute.manager instance: file line in _cleanup_failed_start nova.compute.manager instance: destroy_disks=destroy_disks nova.compute.manager instance: file line in cleanup nova.compute.manager instance: self._undefine_domain instance nova.compute.manager instance: file line in _undefine_domain nova.compute.manager instance: guest self._host.get_guest instance nova.compute.manager instance: file line in get_guest nova.compute.manager instance: self.get_domain instance nova.compute.manager instance: file line in get_domain nova.compute.manager instance: return self._get_domain_by_name instance.name nova.compute.manager instance: file line in _get_domain_by_name nova.compute.manager instance: conn self.get_connection nova.compute.manager instance: file line in get_connection nova.compute.manager instance: raise exception.hypervisorunavailable host=conf.host nova.compute.manager instance: hypervisorunavailable: connection to the hypervisor is broken on host: cpt node nova.compute.manager instance: nova.compute.manager instance: terminating instance nova.virt.libvirt.driver connection event reason failed to connect to libvirt nova.virt.libvirt.host connection to libvirt failed: failed to connect socket to connection refused nova.virt.libvirt.host traceback most recent call last : nova.virt.libvirt.host file line in get_connection nova.virt.libvirt.host conn self._get_connection nova.virt.libvirt.host file line in _get_connection nova.virt.libvirt.host wrapped_conn self._get_new_connection nova.virt.libvirt.host file line in _get_new_connection nova.virt.libvirt.host wrapped_conn self._connect self._uri, self._read_only nova.virt.libvirt.host file line in _connect nova.virt.libvirt.host libvirt.openauth, uri, auth, flags nova.virt.libvirt.host file line in proxy_call nova.virt.libvirt.host rv execute f, nova.virt.libvirt.host file line in execute nova.virt.libvirt.host six.reraise c, e, tb nova.virt.libvirt.host file line in tworker nova.virt.libvirt.host rv meth nova.virt.libvirt.host file line in openauth nova.virt.libvirt.host if ret is none:raise libvirterror virconnectopenauth failed nova.virt.libvirt.host libvirterror: failed to connect socket to connection refused nova.virt.libvirt.host nova.compute.manager could not clean up failed build, not rescheduling. error: connection to the hypervisor is broken on host: cpt node nova.scheduler.client.report unable to refresh my resource provider record nova.compute.manager instance: build of instance aborted: connection to the hypervisor is broken on host: cpt node nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _do_build_and_run_instance nova.compute.manager instance: filter_properties nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: nova.compute.manager instance: file line in __exit__ nova.compute.manager instance: self.force_reraise nova.compute.manager instance: file line in force_reraise nova.compute.manager instance: six.reraise self.type_, self.value, self.tb nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: nova.compute.manager instance: file line in __exit__ nova.compute.manager instance: self.gen.throw type, value, traceback nova.compute.manager instance: file line in _build_resources nova.compute.manager instance: exc nova.compute.manager instance: buildabortexception: build of instance aborted: connection to the hypervisor is broken on host: cpt node nova.compute.manager instance: nova.compute.manager instance: took seconds to deallocate network for instance. nova.virt.libvirt.driver connection event reason nova.compute.resource_tracker auditing locally available compute resources for node cpt node nova.scheduler.client.report no authentication information found for placement api. placement is optional in newton, but required in ocata. please enable the placement service before upgrading. nova.scheduler.client.report unable to refresh my resource provider record nova.compute.resource_tracker total usable vcpus: total allocated vcpus: nova.compute.resource_tracker final resource view: cpt node phys_ram=8191mb used_ram=512mb phys_disk=191gb used_disk=0gb total_vcpus=4 used_vcpus=0 pci_stats= nova.scheduler.client.report unable to refresh my resource provider record nova.compute.resource_tracker compute_service record updated for cpt node node 1 pre"
    },
    {
        "url": "https://stackoverflow.com/questions/45407430",
        "text": "journalctl xe jul ha attrd : notice: removing all ha attributes for attrd_peer_change_cb jul ha attrd : notice: lost attribute writer ha jul ha attrd : notice: removing ha from the membership list jul ha attrd : notice: purged peers with and or from the membership cache jul ha xinetd : start: mysqlchk jul ha xinetd : exit: mysqlchk sec jul ha xinetd : exit: mysqlchk sec jul ha xinetd : exit: mysqlchk sec jul ha corosync : totem a new membership was formed. members jul ha corosync : quorum members : jul ha corosync : main completed service synchronization, ready to provide service. jul ha crmd : notice: state transition s_election s_integration jul ha crmd : warning: fsa: input i_election_dc from do_election_check received in state s_integration jul ha crmd : notice: notifications disabled jul ha corosync : totem a new membership was formed. members joined: jul ha pacemakerd : error: node ha appears to be online even though we think it is dead jul ha pacemakerd : notice: pcmk_cpg_membership: node ha state is now member was lost jul ha pacemakerd : error: node ha appears to be online even though we think it is dead jul ha pacemakerd : notice: pcmk_cpg_membership: node ha state is now member was lost jul ha attrd : notice: crm_update_peer_proc: node ha state is now member was null jul ha cib : notice: crm_update_peer_proc: node ha state is now member was null jul ha corosync : quorum this node is within the primary component and will provide service. jul ha corosync : quorum members : jul ha corosync : main completed service synchronization, ready to provide service. jul ha pacemakerd : notice: membership quorum acquired jul ha stonith ng : notice: crm_update_peer_proc: node ha state is now member was null jul ha cib : notice: crm_update_peer_proc: node ha state is now member was null jul ha attrd : notice: crm_update_peer_proc: node ha state is now member was null jul ha stonith ng : notice: crm_update_peer_proc: node ha state is now member was null jul ha crmd : notice: membership quorum acquired jul ha crmd : notice: pcmk_quorum_notification: node ha state is now member was lost jul ha crmd : notice: pcmk_quorum_notification: node ha state is now member was lost jul ha attrd : notice: recorded attribute writer: ha jul ha attrd : notice: processing sync response from ha"
    },
    {
        "url": "https://stackoverflow.com/questions/45421462",
        "text": "file line in module from neutron.agent.common import utils file line in module from neutron.common import utils as neutron_utils file line in module from neutron.db import api as db_api file line in module from neutron_lib.db import api importerror: cannot import name api lib neutron legacy:_configure_neutron_common:1 : exit_trap . stack.sh:exit_trap:494 : local . stack.sh:exit_trap:495 : jobs p . stack.sh:exit_trap:495 : . stack.sh:exit_trap:498 : n . stack.sh:exit_trap:504 : kill_spinner . stack.sh:kill_spinner:390 : z . stack.sh:exit_trap:506 : ne . stack.sh:exit_trap:507 : echo on exit on exit . stack.sh:exit_trap:508 : generate subunit fail . stack.sh:exit_trap:509 : z . stack.sh:exit_trap:512 : d . stack.sh:exit_trap:518 : exit virtual machine:~ devstack"
    },
    {
        "url": "https://stackoverflow.com/questions/45466370",
        "text": "... nova.virt.libvirt.driver cannot update service status on host due to an unexpected exception. nova.virt.libvirt.driver traceback most recent call last : nova.virt.libvirt.driver file line in _set_host_enabled nova.virt.libvirt.driver service objects.service.get_by_compute_host ctx, conf.host nova.virt.libvirt.driver file line in wrapper nova.virt.libvirt.driver args, kwargs nova.virt.libvirt.driver file line in object_class_action_versions nova.virt.libvirt.driver nova.virt.libvirt.driver file line in call nova.virt.libvirt.driver nova.virt.libvirt.driver file line in _send nova.virt.libvirt.driver nova.virt.libvirt.driver file line in send nova.virt.libvirt.driver nova.virt.libvirt.driver file line in _send nova.virt.libvirt.driver result self._waiter.wait msg_id, timeout nova.virt.libvirt.driver file line in wait nova.virt.libvirt.driver message self.waiters.get msg_id, nova.virt.libvirt.driver file line in get nova.virt.libvirt.driver to message id msg_id nova.virt.libvirt.driver messagingtimeout: timed out waiting for a reply to message id nova.virt.libvirt.driver nova.virt.libvirt.host libvirt host capabilities capabilities ..."
    },
    {
        "url": "https://stackoverflow.com/questions/45497102",
        "text": "neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling. neutron.db.agentschedulers_db exception encountered during router rescheduling."
    },
    {
        "url": "https://stackoverflow.com/questions/45497440",
        "text": "debug gnocchi.service: debug gnocchi.service: configuration options gathered from: debug gnocchi.service: command line args: debug gnocchi.service: config files: debug gnocchi.service: debug gnocchi.service: config_dir debug gnocchi.service: config_file debug gnocchi.service: debug true debug gnocchi.service: log_dir debug gnocchi.service: log_file none debug gnocchi.service: syslog_log_facility user debug gnocchi.service: use_journal false debug gnocchi.service: use_syslog false debug gnocchi.service: verbose true debug gnocchi.service: statsd.archive_policy_name low debug gnocchi.service: statsd.creator : debug gnocchi.service: statsd.flush_delay debug gnocchi.service: statsd.host debug gnocchi.service: statsd.port debug gnocchi.service: statsd.project_id none debug gnocchi.service: statsd.resource_id debug gnocchi.service: statsd.user_id none debug gnocchi.service: incoming.ceph_conffile debug gnocchi.service: incoming.ceph_keyring debug gnocchi.service: incoming.ceph_pool gnocchi debug gnocchi.service: incoming.ceph_secret debug gnocchi.service: incoming.ceph_timeout debug gnocchi.service: incoming.ceph_username debug gnocchi.service: incoming.driver file debug gnocchi.service: incoming.file_basepath debug gnocchi.service: incoming.redis_url redis: localhost:6379 debug gnocchi.service: incoming.s3_access_key_id debug gnocchi.service: incoming.s3_bucket_prefix gnocchi debug gnocchi.service: incoming.s3_check_consistency_timeout debug gnocchi.service: incoming.s3_endpoint_url debug gnocchi.service: incoming.s3_region_name debug gnocchi.service: incoming.s3_secret_access_key debug gnocchi.service: incoming.swift_auth_version debug gnocchi.service: incoming.swift_authurl debug gnocchi.service: incoming.swift_container_prefix gnocchi debug gnocchi.service: incoming.swift_endpoint_type publicurl debug gnocchi.service: incoming.swift_key debug gnocchi.service: incoming.swift_preauthtoken debug gnocchi.service: incoming.swift_preauthurl debug gnocchi.service: incoming.swift_project_domain_name debug gnocchi.service: incoming.swift_project_name debug gnocchi.service: incoming.swift_timeout debug gnocchi.service: incoming.swift_user admin:admin debug gnocchi.service: incoming.swift_user_domain_name debug gnocchi.service: metricd.metric_cleanup_delay debug gnocchi.service: metricd.metric_processing_delay debug gnocchi.service: metricd.metric_reporting_delay debug gnocchi.service: metricd.processing_replicas debug gnocchi.service: metricd.worker_sync_rate debug gnocchi.service: metricd.workers debug gnocchi.service: database.backend sqlalchemy debug gnocchi.service: database.connection debug gnocchi.service: database.connection_debug debug gnocchi.service: database.connection_trace false debug gnocchi.service: database.db_inc_retry_interval true debug gnocchi.service: database.db_max_retries debug gnocchi.service: database.db_max_retry_interval debug gnocchi.service: database.db_retry_interval debug gnocchi.service: database.idle_timeout debug gnocchi.service: database.max_overflow debug gnocchi.service: database.max_pool_size debug gnocchi.service: database.max_retries debug gnocchi.service: database.min_pool_size debug gnocchi.service: database.mysql_sql_mode traditional debug gnocchi.service: database.pool_timeout none debug gnocchi.service: database.retry_interval debug gnocchi.service: database.slave_connection debug gnocchi.service: database.sqlite_db oslo.sqlite debug gnocchi.service: database.sqlite_synchronous true debug gnocchi.service: database.use_db_reconnect false debug gnocchi.service: storage.aggregation_workers_number debug gnocchi.service: storage.ceph_conffile debug gnocchi.service: storage.ceph_keyring none debug gnocchi.service: storage.ceph_pool gnocchi debug gnocchi.service: storage.ceph_secret debug gnocchi.service: storage.ceph_timeout debug gnocchi.service: storage.ceph_username none debug gnocchi.service: storage.coordination_url debug gnocchi.service: storage.driver file debug gnocchi.service: storage.file_basepath debug gnocchi.service: storage.redis_url redis: localhost:6379 debug gnocchi.service: storage.s3_access_key_id none debug gnocchi.service: storage.s3_bucket_prefix gnocchi debug gnocchi.service: storage.s3_check_consistency_timeout debug gnocchi.service: storage.s3_endpoint_url none debug gnocchi.service: storage.s3_region_name none debug gnocchi.service: storage.s3_secret_access_key none debug gnocchi.service: storage.swift_auth_version debug gnocchi.service: storage.swift_authurl debug gnocchi.service: storage.swift_container_prefix gnocchi debug gnocchi.service: storage.swift_endpoint_type publicurl debug gnocchi.service: storage.swift_key debug gnocchi.service: storage.swift_preauthtoken debug gnocchi.service: storage.swift_preauthurl none debug gnocchi.service: storage.swift_project_domain_name debug gnocchi.service: storage.swift_project_name none debug gnocchi.service: storage.swift_timeout debug gnocchi.service: storage.swift_user admin:admin debug gnocchi.service: storage.swift_user_domain_name debug gnocchi.service: indexer.url debug gnocchi.service: api.auth_mode keystone debug gnocchi.service: api.max_limit debug gnocchi.service: api.paste_config debug gnocchi.service: api.refresh_timeout debug gnocchi.service: archive_policy.default_aggregation_methods debug gnocchi.service: oslo_policy.policy_default_rule debug gnocchi.service: oslo_policy.policy_dirs debug gnocchi.service: oslo_policy.policy_file debug gnocchi.service: gnocchi.rest.app: wsgi config used: keystonemiddleware.auth_token: authtoken middleware is set with keystone_authtoken.service_token_roles_required set to false. this is backwards compatible but deprecated behaviour. please set this to true. keystonemiddleware.auth_token: configuring auth_uri to point to the public identity endpoint is required; clients may not be able to authenticate against an admin endpoint critical root: traceback most recent call last : file line in module server wss.make_server args.port, build_wsgi_app file line in make_server server server_class host, port , handler_class file line in __init__ self.server_bind file line in server_bind httpserver.server_bind self file line in server_bind socketserver.tcpserver.server_bind self file line in server_bind self.socket.bind self.server_address file line in meth return getattr self._sock,name *args error: errno address already in use"
    },
    {
        "url": "https://stackoverflow.com/questions/45511739",
        "text": "nova.scheduler.utils failed to compute_task_build_instances: no valid host was found. there are not enough hosts available. traceback most recent call last : file line in inner return func file line in select_destinations dests self.driver.select_destinations ctxt, spec_obj file line in select_destinations raise exception.novalidhost reason=reason novalidhost: no valid host was found. there are not enough hosts available. nova.scheduler.utils instance: setting instance to state. oslo_config.cfg option from group is deprecated. use option from group"
    },
    {
        "url": "https://stackoverflow.com/questions/45511739",
        "text": "...... nova.compute.resource_tracker compute_service record updated for ha node1 nova.compute.manager instance: instance failed to spawn nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _build_resources nova.compute.manager instance: yield resources nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: block_device_info=block_device_info nova.compute.manager instance: file line in spawn nova.compute.manager instance: admin_pass=admin_password nova.compute.manager instance: file line in _create_image nova.compute.manager instance: fileutils.ensure_tree libvirt_utils.get_instance_path instance nova.compute.manager instance: file line in ensure_tree nova.compute.manager instance: os.makedirs path, mode nova.compute.manager instance: file line in makedirs nova.compute.manager instance: mkdir name, mode nova.compute.manager instance: oserror: errno permission denied: nova.compute.manager instance: nova.compute.manager instance: terminating instance ...."
    },
    {
        "url": "https://stackoverflow.com/questions/46018329",
        "text": "nova file line in _send nova msg.update self._get_reply_q nova file line in _get_reply_q nova conn self._get_connection rpc_common.purpose_listen nova file line in _get_connection nova nova file line in __init__ nova self.connection connection_pool.create purpose nova file line in create nova return self.connection_cls self.conf, self.url, purpose nova file line in __init__ nova self.ensure_connection nova file line in ensure_connection nova self.ensure method=lambda: self.connection.connection nova file line in ensure nova ret, channel autoretry_method nova file line in _ensured nova return fun nova file line in __call__ nova self.revive create_channel nova file line in channel nova chan self.transport.create_channel self.connection nova file line in connection nova self._connection self._establish_connection nova file line in _establish_connection nova conn self.transport.establish_connection nova file line in establish_connection nova conn self.connection **opts nova file line in __init__ nova , tune nova file line in wait nova self.channel_id, allowed_methods, timeout nova file line in _wait_method nova wait nova file line in wait nova return self.dispatch_method method_sig, args, content nova file line in dispatch_method nova return amqp_method self, args nova file line in _close nova class_id, method_id , connectionerror br nova accessrefused: : access_refused login was refused using authentication mechanism amqplain. for details see the broker logfile."
    },
    {
        "url": "https://stackoverflow.com/questions/46108682",
        "text": "failed to get unit file state for no such file or directory common:service_check:1538 for service in common:service_check:1540 sudo systemctl is enabled net.service enabled common:service_check:1544 sudo systemctl status net.service no pager net.service devstack net.service loaded: loaded enabled; vendor preset: enabled active: failed result: exit code since fri utc; ago main pid: failure sep sharandevstack systemd : started devstack net.service. sep sharandevstack nova network : config option oslo_config.cfg.optgroup object at .idle_timeout is deprecated. use option database.connection_recycle_time instead. sep sharandevstack nova network : nova.network nova network is deprecated and not supported except as required for deployments. sep sharandevstack systemd : net.service: main process exited, failure sep sharandevstack systemd : net.service: unit entered failed state. sep sharandevstack systemd : net.service: failed with result common:service_check:1 exit_trap stack.sh:exit_trap:493 local stack.sh:exit_trap:494 jobs p stack.sh:exit_trap:494 stack.sh:exit_trap:497 n stack.sh:exit_trap:503 f stack.sh:exit_trap:504 rm stack.sh:exit_trap:508 kill_spinner stack.sh:kill_spinner:407 z stack.sh:exit_trap:510 ne stack.sh:exit_trap:511 echo on exit on exit stack.sh:exit_trap:512 generate subunit fail stack.sh:exit_trap:513 z stack.sh:exit_trap:516 d stack.sh:exit_trap:522 exit"
    },
    {
        "url": "https://stackoverflow.com/questions/46108682",
        "text": "sep sharandevstack nova network : nova.network nova network is deprecated and not supported except as required for deployments."
    },
    {
        "url": "https://stackoverflow.com/questions/46313029",
        "text": "...... keystone.common.wsgi get keystone.common.wsgi object does not support item assignment keystone.common.wsgi traceback most recent call last : keystone.common.wsgi file line in __call__ keystone.common.wsgi result method req, keystone.common.wsgi file line in wrapper keystone.common.wsgi return f self, request, filters, keystone.common.wsgi file line in list_users keystone.common.wsgi return request.context_dict, refs, keystone.common.wsgi file line in wrap_collection keystone.common.wsgi cls.wrap_member context, ref keystone.common.wsgi file line in wrap_member keystone.common.wsgi cls._add_self_referential_link context, ref keystone.common.wsgi file line in _add_self_referential_link keystone.common.wsgi ref cls.base_url context ref keystone.common.wsgi typeerror: object does not support item assignment"
    },
    {
        "url": "https://stackoverflow.com/questions/46361526",
        "text": "fri sep :error warning:root: and in local_ settings is deprecated now and may be unsupported in some future release. the preferred way to specify the order of dashboards and the dashboard is the pluggable dashboard mechanism in fri sep :error warning:root: and in local_ settings is deprecated now and may be unsupported in some future release. the preferred way to specify the order of dashboards and the dashboard is the pluggable dashboard mechanism in fri sep core:error client end of script output before headers: django.wsgi fri sep core:error client end of script output before headers: django.wsgi fri sep :error warning:root: and in local_ settings is deprecated now and may be unsupported in some future release. the preferred way to specify the order of dashboards and the dashboard is the pluggable dashboard mechanism in fri sep core:error client end of script output before headers: django.wsgi fri sep :error warning:root: and in local_ settings is deprecated now and may be unsupported in some future release. the preferred way to specify the order of dashboards and the dashboard is the pluggable dashboard mechanism in fri sep core:error client end of script output before headers: django.wsg"
    },
    {
        "url": "https://stackoverflow.com/questions/46361526",
        "text": "fri sep mpm_prefork:notice caught sigwinch, shutting down gracefully fri sep suexec:notice suexec mechanism enabled wrapper: fri sep so:warn module access_compat_module is already loaded, skipping fri sep so:warn module actions_module is already loaded, skipping fri sep so:warn module alias_module is already loaded, skipping fri sep so:warn module auth_basic_module is already loaded, skipping fri sep so:warn module auth_digest_module is already loaded, skipping fri sep so:warn module authn_anon_module is already loaded, skipping fri sep so:warn module authn_core_module is already loaded, skipping fri sep so:warn module authn_dbm_module is already loaded, skipping fri sep so:warn module authn_file_module is already loaded, skipping fri sep so:warn module authz_core_module is already loaded, skipping fri sep so:warn module authz_dbm_module is already loaded, skipping fri sep so:warn module authz_groupfile_module is already loaded, skipping fri sep so:warn module authz_host_module is already loaded, skipping fri sep so:warn module authz_owner_module is already loaded, skipping fri sep so:warn module authz_user_module is already loaded, skipping fri sep so:warn module autoindex_module is already loaded, skipping fri sep so:warn module cache_module is already loaded, skipping fri sep so:warn module deflate_module is already loaded, skipping fri sep so:warn module dir_module is already loaded, skipping fri sep so:warn module env_module is already loaded, skipping fri sep so:warn module expires_module is already loaded, skipping fri sep so:warn module ext_filter_module is already loaded, skipping fri sep so:warn module filter_module is already loaded, skipping fri sep so:warn module include_module is already loaded, skipping fri sep so:warn module log_config_module is already loaded, skipping fri sep so:warn module logio_module is already loaded, skipping fri sep so:warn module mime_magic_module is already loaded, skipping fri sep so:warn module mime_module is already loaded, skipping fri sep so:warn module negotiation_module is already loaded, skipping fri sep so:warn module rewrite_module is already loaded, skipping fri sep so:warn module setenvif_module is already loaded, skipping fri sep so:warn module substitute_module is already loaded, skipping fri sep so:warn module suexec_module is already loaded, skipping fri sep so:warn module unixd_module is already loaded, skipping fri sep so:warn module version_module is already loaded, skipping fri sep so:warn module vhost_alias_module is already loaded, skipping fri sep so:warn module dav_module is already loaded, skipping fri sep so:warn module dav_fs_module is already loaded, skipping fri sep so:warn module mpm_prefork_module is already loaded, skipping fri sep so:warn module systemd_module is already loaded, skipping fri sep so:warn module cgi_module is already loaded, skipping fri sep so:warn module wsgi_module is already loaded, skipping fri sep alias:warn the alias directive in at line will probably never match because it overlaps an earlier alias. fri sep auth_digest:notice generating secret for digest authentication ... fri sep lbmethod_heartbeat:notice no slotmem from mod_heartmonitor fri sep mpm_prefork:notice apache centos mod_wsgi 3.4 python configured resuming normal operations fri sep core:notice command line: d foreground fri sep mpm_prefork:notice caught sigwinch, shutting down gracefully fri sep suexec:notice suexec mechanism enabled wrapper: fri sep so:warn module access_compat_module is already loaded, skipping fri sep so:warn module actions_module is already loaded, skipping fri sep so:warn module alias_module is already loaded, skipping fri sep so:warn module auth_basic_module is already loaded, skipping fri sep so:warn module auth_digest_module is already loaded, skipping fri sep so:warn module authn_anon_module is already loaded, skipping fri sep so:warn module authn_core_module is already loaded, skipping fri sep so:warn module authn_dbm_module is already loaded, skipping fri sep so:warn module authn_file_module is already loaded, skipping fri sep so:warn module authz_core_module is already loaded, skipping fri sep so:warn module authz_dbm_module is already loaded, skipping fri sep so:warn module authz_groupfile_module is already loaded, skipping fri sep so:warn module authz_host_module is already loaded, skipping fri sep so:warn module authz_owner_module is already loaded, skipping fri sep so:warn module authz_user_module is already loaded, skipping fri sep so:warn module autoindex_module is already loaded, skipping fri sep so:warn module cache_module is already loaded, skipping fri sep so:warn module deflate_module is already loaded, skipping fri sep so:warn module dir_module is already loaded, skipping fri sep so:warn module env_module is already loaded, skipping fri sep so:warn module expires_module is already loaded, skipping fri sep so:warn module ext_filter_module is already loaded, skipping fri sep so:warn module filter_module is already loaded, skipping fri sep so:warn module include_module is already loaded, skipping fri sep so:warn module log_config_module is already loaded, skipping fri sep so:warn module logio_module is already loaded, skipping fri sep so:warn module mime_magic_module is already loaded, skipping fri sep so:warn module mime_module is already loaded, skipping fri sep so:warn module negotiation_module is already loaded, skipping fri sep so:warn module rewrite_module is already loaded, skipping fri sep so:warn module setenvif_module is already loaded, skipping fri sep so:warn module substitute_module is already loaded, skipping fri sep so:warn module suexec_module is already loaded, skipping fri sep so:warn module unixd_module is already loaded, skipping fri sep so:warn module version_module is already loaded, skipping fri sep so:warn module vhost_alias_module is already loaded, skipping fri sep so:warn module dav_module is already loaded, skipping fri sep so:warn module dav_fs_module is already loaded, skipping fri sep so:warn module mpm_prefork_module is already loaded, skipping fri sep so:warn module systemd_module is already loaded, skipping fri sep so:warn module cgi_module is already loaded, skipping fri sep so:warn module wsgi_module is already loaded, skipping fri sep alias:warn the alias directive in at line will probably never match because it overlaps an earlier alias. fri sep auth_digest:notice generating secret for digest authentication ... fri sep lbmethod_heartbeat:notice no slotmem from mod_heartmonitor fri sep mpm_prefork:notice apache centos mod_wsgi 3.4 python configured resuming normal operations fri sep core:notice command line: d foreground httpd"
    },
    {
        "url": "https://stackoverflow.com/questions/46555856",
        "text": "keystone.common.wsgi get keystone.common.wsgi post keystone.auth.controllers could not find domain: keystone.common.wsgi authorization failed. the request you have made requires authentication. from"
    },
    {
        "url": "https://stackoverflow.com/questions/46587783",
        "text": "mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir mkdir build in constructor build note: variable tracking size limit exceeded with fvar tracking assignments, retrying without vizconstants::vizconstants in file included from controller from controller controller fatal error: grok.h: no such file or directory grok.h compilation terminated. scons: build ret_val=2 ne exit clean local echo exited with status exit"
    },
    {
        "url": "https://stackoverflow.com/questions/46670881",
        "text": "cinder.volume.manager traceback most recent call last : cinder.volume.manager file line in init_host cinder.volume.manager self.driver.check_for_setup_error cinder.volume.manager file line in check_for_setup_error cinder.volume.manager with radosclient self : cinder.volume.manager file line in __init__ cinder.volume.manager self.cluster, self.ioctx driver._connect_to_rados pool cinder.volume.manager file line in _connect_to_rados cinder.volume.manager return _do_conn pool, remote, timeout cinder.volume.manager file line in _wrapper cinder.volume.manager return r.call f, cinder.volume.manager file line in call cinder.volume.manager return attempt.get self._wrap_exception cinder.volume.manager file line in get cinder.volume.manager six.reraise self.value 0 , self.value 1 self.value 2 cinder.volume.manager file line in call cinder.volume.manager attempt attempt fn , attempt_number, false cinder.volume.manager file line in _do_conn cinder.volume.manager client.connect cinder.volume.manager file line in rados.rados.connect cinder.volume.manager file line in rados.make_ex cinder.volume.manager typeerror: invalidargumenterror does not take keyword arguments"
    },
    {
        "url": "https://stackoverflow.com/questions/46684804",
        "text": "nova.osapi_compute.wsgi.server get http status: len: time: nova.api.openstack.wsgi http exception thrown hosting type flavor: did not nova.osapi_compute.wsgi.server get http status: len: time: nova.osapi_compute.wsgi.server get http status: len: time: nova.osapi_compute.wsgi.server post http status: len: time: nova.api.openstack.wsgi http exception not find valid nova.osapi_compute.wsgi.server post http status: len: time:"
    },
    {
        "url": "https://stackoverflow.com/questions/46693104",
        "text": "openstack_volumes.go:320 failed to create a gb volume: invalid request due to incorrect syntax or missing required parameters. cinder_util.go:207 creating cinder volume: invalid request due to incorrect syntax or missing required parameters. pv_controller.go:1331 failed to provision volume for claim with storageclass invalid request due to incorrect syntax or missing required parameters."
    },
    {
        "url": "https://stackoverflow.com/questions/46727751",
        "text": "debug oslo_concurrency.lockutils acquired semaphore lock debug oslo_concurrency.lockutils releasing semaphore lock keystone.common.environment.eventlet_server could not bind to root failed to start the admin server root traceback most recent call last : root file line in serve root server.launch_with launcher root file line in launch_with root self.server.listen root file line in listen root root file line in listen root sock.listen backlog root file line in meth root return getattr self._sock,name *args root error: errno address already in use root critical keystone error: errno address already in use keystone traceback most recent call last : keystone file line in module keystone sys.exit main keystone file line in main keystone eventlet_server.run possible_topdir keystone file line in run keystone serve keystone file line in serve keystone server.launch_with launcher keystone file line in launch_with keystone self.server.listen keystone file line in listen keystone keystone file line in listen keystone sock.listen backlog keystone file line in meth keystone return getattr self._sock,name *args keystone error: errno address already in use keystone"
    },
    {
        "url": "https://stackoverflow.com/questions/46727751",
        "text": "fri oct ssl:warn ip server certificate is a ca certificate basicconstraints: ca true !? fri oct ssl:warn ip server certificate is a ca certificate basicconstraints: ca true !?"
    },
    {
        "url": "https://stackoverflow.com/questions/46727751",
        "text": "fri oct ssl:warn ip server certificate is a ca certificate basicconstraints: ca true !? fri oct ssl:warn ip server certificate is a ca certificate basicconstraints: ca true !?"
    },
    {
        "url": "https://stackoverflow.com/questions/47394880",
        "text": "ceilometer.neutron_client the resource could not be found. debug ceilometer.agent.manager skip pollster network.services.lb.pool, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster ip.floating, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster switch.table.active.entries, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster hardware.system_stats.io.incoming.blocks, no resources found this cycle poll_and_notify ceilometer.neutron_client the resource could not be found. debug ceilometer.agent.manager skip pollster network.services.lb.vip, no resources found this cycle poll_and_notify ceilometer.agent.manager polling pollster radosgw.containers.objects in the context of all_pollsters"
    },
    {
        "url": "https://stackoverflow.com/questions/47394880",
        "text": "debug ceilometer.agent.manager skip pollster network.incoming.bytes, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster disk.device.latency, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster network.outgoing.packets.error, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster disk.iops, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster disk.read.bytes, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster disk.device.capacity, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster disk.device.read.bytes.rate, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster cpu, no resources found this cycle poll_and_notify debug ceilometer.agent.manager skip pollster cpu_l3_cache, no resources found this cycle poll_and_notify"
    },
    {
        "url": "https://stackoverflow.com/questions/47394880",
        "text": "ceilometer.publisher.gnocchi public endpoint for metric service in regionone region not found ceilometer.publisher.gnocchi traceback most recent call last : ceilometer.publisher.gnocchi file line in publish_samples ceilometer.publisher.gnocchi self._update_resource, resource_extra ceilometer.publisher.gnocchi file line in _if_not_cached ceilometer.publisher.gnocchi method resource_type, resource, ceilometer.publisher.gnocchi file line in _update_resource ceilometer.publisher.gnocchi resource_extra ceilometer.publisher.gnocchi file line in update ceilometer.publisher.gnocchi resource .json ceilometer.publisher.gnocchi file line in _patch ceilometer.publisher.gnocchi return self.client.api.patch *args, ceilometer.publisher.gnocchi file line in patch ceilometer.publisher.gnocchi return self.request url, ceilometer.publisher.gnocchi file line in request ceilometer.publisher.gnocchi ceilometer.publisher.gnocchi file line in request ceilometer.publisher.gnocchi return self.session.request url, method, ceilometer.publisher.gnocchi file line in inner ceilometer.publisher.gnocchi return wrapped ceilometer.publisher.gnocchi file line in request ceilometer.publisher.gnocchi ceilometer.publisher.gnocchi file line in get_endpoint ceilometer.publisher.gnocchi return auth.get_endpoint self, ceilometer.publisher.gnocchi file line in get_endpoint ceilometer.publisher.gnocchi service_name=service_name ceilometer.publisher.gnocchi file line in inner ceilometer.publisher.gnocchi return wrapped ceilometer.publisher.gnocchi file line in url_for ceilometer.publisher.gnocchi raise exceptions.endpointnotfound msg ceilometer.publisher.gnocchi endpointnotfound: public endpoint for metric service in regionone region not found"
    },
    {
        "url": "https://stackoverflow.com/questions/47463610",
        "text": "dib run parts thu nov est check hadoop completed dib run parts thu nov est running dib run parts thu nov est check hive completed dib run parts thu nov est running dib run parts thu nov est check oozie completed dib run parts thu nov est running line dib_spark_version: unbound variable diskimage_builder.block_device.blockdevice state already cleaned no way to do anything here error: invocationerror: _______________________________________________________________________________________________________ summary ________________________________________________________________________________________________________ error: venv: commands failed"
    },
    {
        "url": "https://stackoverflow.com/questions/47504867",
        "text": "nova api os compute.log nova.api.openstack.extensions unexpected exception in api method: messagingtimeout: timed out waiting for a reply to message id nova conductor.log nova.conductor.manager novalidhost: no valid host was found. there are not enough hosts available. novalidhost: no valid host was found. there are not enough hosts available. nova.scheduler.utils instance: setting instance to state.: novalidhost_remote: no valid host was found. there are not enough hosts available."
    },
    {
        "url": "https://stackoverflow.com/questions/47520779",
        "text": "gnocchi.rest.app wsgi config used: keystonemiddleware._common.config the option"
    },
    {
        "url": "https://stackoverflow.com/questions/47547662",
        "text": ": openstack.compute.v2.server.serverdetail os az:availability_zone=, key_name=none, os extended volumes:volumes_attached= , os srv usg:launched_at=none, os ext sts:vm_state=error, , , , , , os dcf:diskconfig=manual, user_id=41bb48ee30e449d5868f7af9e6251156, os srv usg:terminated_at=none, config_drive=, os ext sts:power_state=0, , os ext sts:task_state=none, tenant_id=233cf23186bf4c52afc464ee008cdf7f openstack.compute.v2.server.serverdetail os az:availability_zone=, key_name=none, os extended volumes:volumes_attached= , os srv usg:launched_at=none, os ext sts:vm_state=error, , , , , ......."
    },
    {
        "url": "https://stackoverflow.com/questions/47698116",
        "text": "vim neutron.common.config logging enabled! neutron.common.config version neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent interface mappings: neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent bridge mappings: neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent interface ifcfg for physical network provider does not exist. agent terminated!"
    },
    {
        "url": "https://stackoverflow.com/questions/48246801",
        "text": "long poll interval user, system"
    },
    {
        "url": "https://stackoverflow.com/questions/48362217",
        "text": "eventlet.wsgi.server put http eventlet.wsgi.server get http"
    },
    {
        "url": "https://stackoverflow.com/questions/48491171",
        "text": "nova.compute.manager updating resources for node jupiter.: libvirterror: node device not found: no node device with matching name nova.compute.manager traceback most recent call last : nova.compute.manager file line in update_available_resource_for_node nova.compute.manager rt.update_available_resource context, nodename nova.compute.manager file line in update_available_resource nova.compute.manager resources self.driver.get_available_resource nodename nova.compute.manager file line in get_available_resource nova.compute.manager nova.compute.manager file line in _get_pci_passthrough_devices nova.compute.manager for name in dev_names: nova.compute.manager file line in _get_pcidev_info nova.compute.manager device device nova.compute.manager file line in _get_device_capabilities nova.compute.manager pcinet_info self._get_pcinet_info address nova.compute.manager file line in _get_pcinet_info nova.compute.manager virtdev self._host.device_lookup_by_name devname nova.compute.manager file line in device_lookup_by_name nova.compute.manager return self.get_connection .nodedevicelookupbyname name nova.compute.manager file line in doit nova.compute.manager result proxy_call self._autowrap, f, nova.compute.manager file line in proxy_call nova.compute.manager rv execute f, nova.compute.manager file line in execute nova.compute.manager six.reraise c, e, tb nova.compute.manager file line in tworker nova.compute.manager rv meth nova.compute.manager file line in nodedevicelookupbyname nova.compute.manager if ret is none:raise libvirterror virnodedevicelookupbyname failed , nova.compute.manager libvirterror: node device not found: no node device with matching name nova.compute.manager"
    },
    {
        "url": "https://stackoverflow.com/questions/49004192",
        "text": "cinder.volume.flows.manager.create_volume failed to copy image to volume: imagetoobig: image size exceeded available disk space: there is no space to convert image. requested: available: cinder.volume.flows.manager.create_volume traceback most recent call last : cinder.volume.flows.manager.create_volume file line in _copy_image_to_volume cinder.volume.flows.manager.create_volume context, volume, image_service, image_id cinder.volume.flows.manager.create_volume file line in copy_image_to_volume cinder.volume.flows.manager.create_volume cinder.volume.flows.manager.create_volume file line in fetch_to_raw cinder.volume.flows.manager.create_volume run_as_root=run_as_root cinder.volume.flows.manager.create_volume file line in fetch_to_volume_format cinder.volume.flows.manager.create_volume check_available_space dest, data.virtual_size, image_id cinder.volume.flows.manager.create_volume file line in check_available_space cinder.volume.flows.manager.create_volume raise exception.imagetoobig image_id=image_id, cinder.volume.flows.manager.create_volume imagetoobig: image size exceeded available disk space: there is no space to convert image. requested: available: cinder.volume.flows.manager.create_volume cinder.volume.flows.manager.create_volume volume volume : created successfully cinder.volume.manager created volume successfully. grep e image_conversion_dir ll total df h filesystem size used avail mounted on var"
    },
    {
        "url": "https://stackoverflow.com/questions/49401688",
        "text": "wed mar :error attempted scope to domain failed, will attemptto scope to another domain. wed mar :error login successful for user remote address wed mar authz_core:error client client denied by server configuration: referer:"
    },
    {
        "url": "https://stackoverflow.com/questions/49679219",
        "text": "task deploy kubeadm aio common : dumping logs for deploy kube action ok: local play all , task gathering facts , ok: task deploy kubeadm master : storing node hostname , ok: task deploy kubeadm master : setup directorys on host , ok: changed: task deploy kubeadm master : generating initial admin token , changed: task deploy kubeadm master : storing initial admin token , ok: task deploy kubeadm master : kubelet copying config to host , changed: task deploy kubeadm master : master deploy certs ca , changed: task deploy kubeadm master : master deploy certs apiserver , changed: task deploy kubeadm master : master deploy certs apiserver kubelet client , changed: task deploy kubeadm master : master deploy certs sa , changed: task deploy kubeadm master : master deploy certs front proxy ca , changed: task deploy kubeadm master : master deploy certs front proxy client , changed: task deploy kubeadm master : master deploy kubeconfig admin , changed: task deploy kubeadm master : master deploy kubeconfig kubelet , changed: task deploy kubeadm master : master deploy kubeconfig controller manager , changed: task deploy kubeadm master : master deploy kubeconfig scheduler , changed: task deploy kubeadm master : generating etcd static manifest , changed: task deploy kubeadm master : master deploy controlplane apiserver , changed: task deploy kubeadm master : master deploy controlplane controller manager , changed: task deploy kubeadm master : master deploy controlplane scheduler , changed: task deploy kubeadm master : wait for kube api , failed retrying: wait for kube api retries left . , changed: task deploy kubeadm master : wait for node to come online , failed retrying: wait for node to come online retries left . , failed retrying: wait for node to come online retries left . , ... failed retrying: wait for node to come online retries left . , failed retrying: wait for node to come online retries left . , fatal: failed! true, kubectl get node no headers gawk print grep q , non zero return , , from server notfound : nodes not , from server notfound : nodes not , , retry, use: limit play recap , : task deploy kubeadm aio common : exiting if deploy kube action failed fatal: local : failed! false, exit , errno no such file or directory ,"
    },
    {
        "url": "https://stackoverflow.com/questions/49714711",
        "text": "bosh e sdp bosh env upload stemcell fix using environment as client task task update stemcell: downloading remote stemcell task update stemcell: extracting stemcell archive task update stemcell: verifying stemcell manifest task update stemcell: checking if this stemcell already exists task update stemcell: uploading stemcell bosh openstack kvm ubuntu trusty go_agent 3541.12 to the cloud l error: cpi with message unable to connect to the openstack keystone api connection timed out connect for errno::etimedout in cpi method task error: cpi with message unable to connect to the openstack keystone api connection timed out connect for errno::etimedout in cpi method task started sat apr utc task finished sat apr utc task duration task uploading remote stemcell expected task to succeed but state is exit code"
    },
    {
        "url": "https://stackoverflow.com/questions/49895504",
        "text": "oslo.service.loopingcall programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select opendaylight_periodic_task.state as opendaylight_periodic_task_state, opendaylight_periodic_task.processing_operation as opendaylight_periodic_task_processing_operation, opendaylight_periodic_task.task as opendaylight_periodic_task_task, opendaylight_periodic_task.lock_updated as opendaylight_periodic_task_lock_updated opendaylight_periodic_task opendaylight_periodic_task.task task_1 and opendaylight_periodic_task.lock_updated lock_updated_1 s parameters: u u datetime.datetime 2018, oslo.service.loopingcall neutron.db.metering.metering_rpc unable to find agent oslo_config.cfg option from group is deprecated for removal replaced by transport_url . its value may be silently ignored in the future. oslo_config.cfg option from group is deprecated for removal replaced by transport_url . its value may be silently ignored in the future. oslo_config.cfg option from group is deprecated for removal replaced by transport_url . its value may be silently ignored in the future. oslo_config.cfg option from group is deprecated for removal replaced by transport_url . its value may be silently ignored in the future. oslo_config.cfg option from group is deprecated. use option from group neutron.quota loaded quota_driver: neutron.db.quota.driver.dbquotadriver object at . neutron.wsgi post http networking_odl.journal.periodic_task starting hostconfig periodic task. oslo.service.loopingcall fixed interval looping call failed: programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select opendaylight_periodic_task.state as opendaylight_periodic_task_state, opendaylight_periodic_task.processing_operation as opendaylight_periodic_task_processing_operation, opendaylight_periodic_task.task as opendaylight_periodic_task_task, opendaylight_periodic_task.lock_updated as opendaylight_periodic_task_lock_updated opendaylight_periodic_task opendaylight_periodic_task.task task_1 and opendaylight_periodic_task.lock_updated lock_updated_1 s parameters: u u datetime.datetime 2018, oslo.service.loopingcall traceback most recent call last : oslo.service.loopingcall file line in func oslo.service.loopingcall return f oslo.service.loopingcall file line in execute_ops oslo.service.loopingcall if not forced and self.task_already_executed_recently context : oslo.service.loopingcall file line in task_already_executed_recently oslo.service.loopingcall context.session, self.task, self.interval oslo.service.loopingcall file line in was_periodic_task_executed_recently oslo.service.loopingcall now delta models.opendaylightperiodictask.lock_updated oslo.service.loopingcall file line in one_or_none oslo.service.loopingcall ret list self oslo.service.loopingcall file line in __iter__ oslo.service.loopingcall return self._execute_and_instances context oslo.service.loopingcall file line in _execute_and_instances oslo.service.loopingcall result conn.execute querycontext.statement, self._params oslo.service.loopingcall file line in execute oslo.service.loopingcall return meth self, multiparams, params oslo.service.loopingcall file line in _execute_on_connection oslo.service.loopingcall return connection._execute_clauseelement self, multiparams, params oslo.service.loopingcall file line in _execute_clauseelement oslo.service.loopingcall compiled_sql, distilled_params oslo.service.loopingcall file line in _execute_context oslo.service.loopingcall context oslo.service.loopingcall file line in _handle_dbapi_exception oslo.service.loopingcall util.raise_from_cause newraise, exc_info oslo.service.loopingcall file line in raise_from_cause oslo.service.loopingcall reraise type exception , exception, oslo.service.loopingcall file line in _execute_context oslo.service.loopingcall context oslo.service.loopingcall file line in do_execute oslo.service.loopingcall cursor.execute statement, parameters oslo.service.loopingcall file line in execute oslo.service.loopingcall result self._query query oslo.service.loopingcall file line in _query oslo.service.loopingcall conn.query q oslo.service.loopingcall file line in query oslo.service.loopingcall self._affected_rows self._read_query_result unbuffered=unbuffered oslo.service.loopingcall file line in _read_query_result oslo.service.loopingcall result.read oslo.service.loopingcall file line in read oslo.service.loopingcall first_packet self.connection._read_packet oslo.service.loopingcall file line in _read_packet oslo.service.loopingcall packet.check_error oslo.service.loopingcall file line in check_error oslo.service.loopingcall err.raise_mysql_exception self._data oslo.service.loopingcall file line in raise_mysql_exception oslo.service.loopingcall raise errorclass errno, errval oslo.service.loopingcall programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select opendaylight_periodic_task.state as opendaylight_periodic_task_state, opendaylight_periodic_task.processing_operation as opendaylight_periodic_task_processing_operation, opendaylight_periodic_task.task as opendaylight_periodic_task_task, opendaylight_periodic_task.lock_updated as opendaylight_periodic_task_lock_updated opendaylight_periodic_task opendaylight_periodic_task.task task_1 and opendaylight_periodic_task.lock_updated lock_updated_1 s parameters: u u datetime.datetime 2018, oslo.service.loopingcall neutron.db.metering.metering_rpc unable to find agent networking_odl.journal.periodic_task starting hostconfig periodic task. oslo.service.loopingcall fixed interval looping call failed: programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select opendaylight_periodic_task.state as opendaylight_periodic_task_state, opendaylight_periodic_task.processing_operation as opendaylight_periodic_task_processing_operation, opendaylight_periodic_task.task as opendaylight_periodic_task_task, opendaylight_periodic_task.lock_updated as opendaylight_periodic_task_lock_updated opendaylight_periodic_task opendaylight_periodic_task.task task_1 and opendaylight_periodic_task.lock_updated lock_updated_1 s parameters: u u datetime.datetime 2018, oslo.service.loopingcall traceback most recent call last : oslo.service.loopingcall file line in func oslo.service.loopingcall return f oslo.service.loopingcall file line in execute_ops oslo.service.loopingcall if not forced and self.task_already_executed_recently context : oslo.service.loopingcall file line in task_already_executed_recently oslo.service.loopingcall context.session, self.task, self.interval oslo.service.loopingcall file line in was_periodic_task_executed_recently oslo.service.loopingcall now delta models.opendaylightperiodictask.lock_updated oslo.service.loopingcall file line in one_or_none oslo.service.loopingcall ret list self oslo.service.loopingcall file line in __iter__ oslo.service.loopingcall return self._execute_and_instances context oslo.service.loopingcall file line in _execute_and_instances oslo.service.loopingcall result conn.execute querycontext.statement, self._params oslo.service.loopingcall file line in execute oslo.service.loopingcall return meth self, multiparams, params oslo.service.loopingcall file line in _execute_on_connection oslo.service.loopingcall return connection._execute_clauseelement self, multiparams, params oslo.service.loopingcall file line in _execute_clauseelement oslo.service.loopingcall compiled_sql, distilled_params oslo.service.loopingcall file line in _execute_context oslo.service.loopingcall context oslo.service.loopingcall file line in _handle_dbapi_exception oslo.service.loopingcall util.raise_from_cause newraise, exc_info oslo.service.loopingcall file line in raise_from_cause oslo.service.loopingcall reraise type exception , exception, oslo.service.loopingcall file line in _execute_context oslo.service.loopingcall context oslo.service.loopingcall file line in do_execute oslo.service.loopingcall cursor.execute statement, parameters oslo.service.loopingcall file line in execute oslo.service.loopingcall result self._query query oslo.service.loopingcall file line in _query oslo.service.loopingcall conn.query q oslo.service.loopingcall file line in query oslo.service.loopingcall self._affected_rows self._read_query_result unbuffered=unbuffered oslo.service.loopingcall file line in _read_query_result oslo.service.loopingcall result.read oslo.service.loopingcall file line in read oslo.service.loopingcall first_packet self.connection._read_packet oslo.service.loopingcall file line in _read_packet oslo.service.loopingcall packet.check_error oslo.service.loopingcall file line in check_error oslo.service.loopingcall err.raise_mysql_exception self._data oslo.service.loopingcall file line in raise_mysql_exception oslo.service.loopingcall raise errorclass errno, errval oslo.service.loopingcall programmingerror: pymysql.err.programmingerror u table doesn t exist sql: u select opendaylight_periodic_task.state as opendaylight_periodic_task_state, opendaylight_periodic_task.processing_operation as opendaylight_periodic_task_processing_operation, opendaylight_periodic_task.task as opendaylight_periodic_task_task, opendaylight_periodic_task.lock_updated as opendaylight_periodic_task_lock_updated opendaylight_periodic_task opendaylight_periodic_task.task task_1 and opendaylight_periodic_task.lock_updated lock_updated_1 s parameters: u u datetime.datetime 2018, oslo.service.loopingcall keystone_admin #"
    },
    {
        "url": "https://stackoverflow.com/questions/50179178",
        "text": "rsyncd version starting, listening on port bind failed: cannot assign requested address address family unable to bind any inbound sockets on port rsync error: in socket io code at socket.c 555"
    },
    {
        "url": "https://stackoverflow.com/questions/50618986",
        "text": "nova.service starting scheduler node version debug nova.service creating rpc server for service scheduler start debug oslo.messaging._drivers.pool pool creating new connection create debug oslo.messaging._drivers.impl_rabbit connecting to amqp server on localhost:5672 __init__ debug nova.scheduler.host_manager found cells: _load_cells debug nova.scheduler.host_manager start:_async_init_instance_info _async_init_instance_info debug oslo_concurrency.lockutils lock acquired by :: waited inner debug oslo_concurrency.lockutils lock released by :: held inner debug oslo.messaging._drivers.impl_rabbit received recoverable from kombu: on_error oslo.messaging._drivers.impl_rabbit traceback most recent call last : oslo.messaging._drivers.impl_rabbit file line in _ensured oslo.messaging._drivers.impl_rabbit return fun oslo.messaging._drivers.impl_rabbit file line in __call__ oslo.messaging._drivers.impl_rabbit self.revive self.connection.default_channel oslo.messaging._drivers.impl_rabbit file line in default_channel oslo.messaging._drivers.impl_rabbit self.connection oslo.messaging._drivers.impl_rabbit file line in connection oslo.messaging._drivers.impl_rabbit self._connection self._establish_connection oslo.messaging._drivers.impl_rabbit file line in _establish_connection oslo.messaging._drivers.impl_rabbit conn self.transport.establish_connection oslo.messaging._drivers.impl_rabbit file line in establish_connection oslo.messaging._drivers.impl_rabbit conn.connect oslo.messaging._drivers.impl_rabbit file line in connect oslo.messaging._drivers.impl_rabbit self.drain_events timeout=self.connect_timeout oslo.messaging._drivers.impl_rabbit file line in drain_events oslo.messaging._drivers.impl_rabbit return self.blocking_read timeout oslo.messaging._drivers.impl_rabbit file line in blocking_read oslo.messaging._drivers.impl_rabbit frame self.transport.read_frame oslo.messaging._drivers.impl_rabbit file line in read_frame oslo.messaging._drivers.impl_rabbit frame_header read true oslo.messaging._drivers.impl_rabbit file line in _read oslo.messaging._drivers.impl_rabbit s recv n len rbuf oslo.messaging._drivers.impl_rabbit file line in recv oslo.messaging._drivers.impl_rabbit return self._recv_loop self.fd.recv, b bufsize, flags oslo.messaging._drivers.impl_rabbit file line in _recv_loop oslo.messaging._drivers.impl_rabbit self._read_trampoline oslo.messaging._drivers.impl_rabbit file line in _read_trampoline oslo.messaging._drivers.impl_rabbit timeout_exc=socket.timeout out oslo.messaging._drivers.impl_rabbit file line in _trampoline oslo.messaging._drivers.impl_rabbit mark_as_closed=self._mark_as_closed oslo.messaging._drivers.impl_rabbit file line in trampoline oslo.messaging._drivers.impl_rabbit return hub.switch oslo.messaging._drivers.impl_rabbit file line in switch oslo.messaging._drivers.impl_rabbit return self.greenlet.switch oslo.messaging._drivers.impl_rabbit timeout: timed out oslo.messaging._drivers.impl_rabbit oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: timed out. trying again in seconds. client port: none: timeout: timed out"
    },
    {
        "url": "https://stackoverflow.com/questions/50706640",
        "text": "mod_wsgi : target wsgi script cannot be loaded as python module. mod_wsgi : exception occurred processing wsgi script traceback most recent call last : file line in module application initialize_admin_application file line in initialize_admin_application config_files=_get_config_files file line in initialize_application common.configure config_files=config_files file line in configure keystone.conf.configure file line in configure do not monkey patch threading system modules. file line in __inner result f self, file line in register_cli_opt raise argsalreadyparsederror cannot register cli option argsalreadyparsederror: arguments already parsed: cannot register cli option"
    },
    {
        "url": "https://stackoverflow.com/questions/50706640",
        "text": "tue jun mpm_event:notice caught sigterm, shutting down tue jun wsgi:warn mod_wsgi: compiled for python tue jun wsgi:warn mod_wsgi: runtime using python tue jun mpm_event:notice apache ubuntu mod_wsgi 4.3.0 python configured resuming normal operations tue jun core:notice command line:"
    },
    {
        "url": "https://stackoverflow.com/questions/50720958",
        "text": "neutron.common.config logging enabled! neutron.common.config version neutron.common.config logging enabled! neutron.common.config version keyring.backend loading windows alt keyring.backend loading pyfs keyring.backend loading multi keyring.backend loading google keyring.backend loading gnome keyring.backend loading keyczar keyring.backend loading file neutron.manager loading core plugin: neutron.plugins.ml2.managers configured type driver names: neutron.plugins.ml2.managers loaded type driver names: neutron.plugins.ml2.managers registered types: neutron.plugins.ml2.managers no type driver for tenant network_type: local. service terminated!"
    },
    {
        "url": "https://stackoverflow.com/questions/50720958",
        "text": "neutron.plugins.ml2.drivers.agent._common_agent failed reporting state!: messagingtimeout: timed neutron.plugins.ml2.drivers.agent._common_agent traceback most recent call last : neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent true neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent return method context, neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent result self._waiter.wait msg_id, timeout neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent message self.waiters.get msg_id, neutron.plugins.ml2.drivers.agent._common_agent file neutron.plugins.ml2.drivers.agent._common_agent to message id msg_id neutron.plugins.ml2.drivers.agent._common_agent messagingtimeout: timed out waiting for a reply to neutron.plugins.ml2.drivers.agent._common_agent oslo.service.loopingcall function neutron.plugins.ml2.drivers.agent._common_agent.commonagentloop$"
    },
    {
        "url": "https://stackoverflow.com/questions/50720958",
        "text": "neutron.agent.metadata.agent failed reporting state!: messagingtimeout: timed out waiting for a neutron.agent.metadata.agent traceback most recent call last : neutron.agent.metadata.agent file neutron.agent.metadata.agent use_call=self.agent_state.get neutron.agent.metadata.agent file line neutron.agent.metadata.agent return method context, neutron.agent.metadata.agent file neutron.agent.metadata.agent neutron.agent.metadata.agent file neutron.agent.metadata.agent neutron.agent.metadata.agent file neutron.agent.metadata.agent neutron.agent.metadata.agent file neutron.agent.metadata.agent result self._waiter.wait msg_id, timeout neutron.agent.metadata.agent file neutron.agent.metadata.agent message self.waiters.get msg_id, neutron.agent.metadata.agent file neutron.agent.metadata.agent to message id msg_id neutron.agent.metadata.agent messagingtimeout: timed out waiting for a reply to message id neutron.agent.metadata.agent oslo.service.loopingcall function neutron.agent.metadata.agent.unixdomainmetadataproxy._report_st$"
    },
    {
        "url": "https://stackoverflow.com/questions/50971449",
        "text": "jun krishna lenovo cinder volume : oslo_service.service httpconnectionpool : max retries exceeded with url: caused by newconnectionerror object at : failed to establish a new connection: errno enetunreach , jun krishna lenovo cinder volume : oslo_service.service jun krishna lenovo cinder volume : oslo_concurrency.lockutils semaphore lock jun krishna lenovo cinder volume : oslo_concurrency.lockutils semaphore lock jun krishna lenovo cinder volume : oslo_service.service exited with status jun krishna lenovo cinder volume : oslo_service.service child _start_child jun krishna lenovo cinder volume : cinder.service cinder volume node version jun krishna lenovo cinder volume : oslo_service.service starting thread.#033 connectionfailederror: httpconnectionpool : max retries exceeded with url: caused by newconnectionerror object at : failed to establish a new connection: errno enetunreach , jun krishna lenovo cinder volume : oslo_service.service most recent call last : jun krishna lenovo cinder volume : oslo_service.service file line in run_service jun krishna lenovo cinder volume : oslo_service.service service.start jun krishna lenovo cinder volume : oslo_service.service file line in start jun krishna lenovo cinder volume : oslo_service.service coordination.coordinator.start jun krishna lenovo cinder volume : oslo_service.service file line in start jun krishna lenovo cinder volume : oslo_service.service self.coordinator.start start_heart=true jun krishna lenovo cinder volume : oslo_service.service file line in start jun krishna lenovo cinder volume : oslo_service.service super coordinationdriverwithexecutor, self .start start_heart jun krishna lenovo cinder volume : oslo_service.service file line in start jun krishna lenovo cinder volume : oslo_service.service self._start jun krishna lenovo cinder volume : oslo_service.service file line in _start jun krishna lenovo cinder volume : oslo_service.service self._membership_lease self.client.lease self.membership_timeout jun krishna lenovo cinder volume : oslo_service.service file line in lease jun krishna lenovo cinder volume : oslo_service.service ttl, jun krishna lenovo cinder volume : oslo_service.service file line in post jun krishna lenovo cinder volume : oslo_service.service raise exceptions.connectionfailederror six.text_type ex"
    },
    {
        "url": "https://stackoverflow.com/questions/51081854",
        "text": "neutron.agent.ovsdb.native.commands neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent while processing vif ports neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent traceback most recent call last : neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in rpc_loop neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent failed_devices, failed_ancillary_devices neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in wrapper neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent return f neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in process_port_info neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent updated_ports_copy neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in wrapper neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent return f neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in scan_ports neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent updated_ports.update self.check_changed_vlans neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in wrapper neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent return f neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in check_changed_vlans neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent port_tags self.int_br.get_port_tag_dict neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in get_port_tag_dict neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent if_exists=true neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in get_ports_attributes neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent execute check_error=check_error, log_errors=log_errors neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in execute neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent ctx.reraise false neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in __exit__ neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent self.force_reraise neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in force_reraise neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent six.reraise self.type_, self.value, self.tb neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in execute neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent txn.add self neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in __exit__ neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent self.result self.commit neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent file line in commit neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent self.timeout neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent timeoutexception: commands dblistcommand if_exists=true, u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u .......... exceeded timeout seconds neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent"
    },
    {
        "url": "https://stackoverflow.com/questions/51136850",
        "text": "neutron.common.config logging enabled! neutron.common.config version neutron.agent.l3.agent an interface driver must be specified"
    },
    {
        "url": "https://stackoverflow.com/questions/51136850",
        "text": "neutron.agent.securitygroups_rpc security group member updated u neutron.plugins.ml2.drivers.agent._common_agent occurred while removing port remoteerror: remote error: agentnotfoundbytypehost agent with agent_type=l3 agent and could not be found u traceback most recent call last , u file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , u file line in return self._do_dispatch endpoint, method, ctxt, args , u file line in _do_dispatch\\n result func ctxt, \\n u file line in update_device_down\\n n_const.port_status_down, host , u file line in notify_l2pop_port_wiring\\n port_context \\n , u file line in update_port_down\\n admin_context, agent_host, port u file line in list_router_ids_on_host\\n context, constants.agent_type_l3, host , u file line in _get_agent_by_type_and_host\\n , u agentnotfoundbytypehost: agent with agent_type=l3 agent and could not be . neutron.plugins.ml2.drivers.agent._common_agent traceback most recent call last : neutron.plugins.ml2.drivers.agent._common_agent file line in treat_devices_removed neutron.plugins.ml2.drivers.agent._common_agent cfg.conf.host neutron.plugins.ml2.drivers.agent._common_agent file line in update_device_down neutron.plugins.ml2.drivers.agent._common_agent agent_id=agent_id, neutron.plugins.ml2.drivers.agent._common_agent file line in call neutron.plugins.ml2.drivers.agent._common_agent return self._original_context.call ctxt, method, neutron.plugins.ml2.drivers.agent._common_agent file line in call neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in _send neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in send neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in _send neutron.plugins.ml2.drivers.agent._common_agent raise result neutron.plugins.ml2.drivers.agent._common_agent remoteerror: remote error: agentnotfoundbytypehost agent with agent_type=l3 agent and could not be found neutron.plugins.ml2.drivers.agent._common_agent u traceback most recent call last , u file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , u file line in return self._do_dispatch endpoint, method, ctxt, args , u file line in _do_dispatch\\n result func ctxt, \\n u file line in update_device_down\\n n_const.port_status_down, host , u file line in notify_l2pop_port_wiring\\n port_context \\n , u file line in update_port_down\\n admin_context, agent_host, port u file line in list_router_ids_on_host\\n context, constants.agent_type_l3, host , u file line in _get_agent_by_type_and_host\\n , u agentnotfoundbytypehost: agent with agent_type=l3 agent and could not be . neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent attachment removed neutron.agent.securitygroups_rpc security group member updated u neutron.plugins.ml2.drivers.agent._common_agent occurred while removing port remoteerror: remote error: agentnotfoundbytypehost agent with agent_type=l3 agent and could not be found u traceback most recent call last , u file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , u file line in return self._do_dispatch endpoint, method, ctxt, args , u file line in _do_dispatch\\n result func ctxt, \\n u file line in update_device_down\\n n_const.port_status_down, host , u file line in notify_l2pop_port_wiring\\n port_context \\n , u file line in update_port_down\\n admin_context, agent_host, port u file line in list_router_ids_on_host\\n context, constants.agent_type_l3, host , u file line in _get_agent_by_type_and_host\\n , u agentnotfoundbytypehost: agent with agent_type=l3 agent and could not be . neutron.plugins.ml2.drivers.agent._common_agent traceback most recent call last : neutron.plugins.ml2.drivers.agent._common_agent file line in treat_devices_removed neutron.plugins.ml2.drivers.agent._common_agent cfg.conf.host neutron.plugins.ml2.drivers.agent._common_agent file line in update_device_down neutron.plugins.ml2.drivers.agent._common_agent agent_id=agent_id, neutron.plugins.ml2.drivers.agent._common_agent file line in call neutron.plugins.ml2.drivers.agent._common_agent return self._original_context.call ctxt, method, neutron.plugins.ml2.drivers.agent._common_agent file line in call neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in _send neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in send neutron.plugins.ml2.drivers.agent._common_agent neutron.plugins.ml2.drivers.agent._common_agent file line in _send neutron.plugins.ml2.drivers.agent._common_agent raise result neutron.plugins.ml2.drivers.agent._common_agent remoteerror: remote error: agentnotfoundbytypehost agent with agent_type=l3 agent and could not be found neutron.plugins.ml2.drivers.agent._common_agent u traceback most recent call last , u file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , u file line in return self._do_dispatch endpoint, method, ctxt, args , u file line in _do_dispatch\\n result func ctxt, \\n u file line in update_device_down\\n n_const.port_status_down, host , u file line in notify_l2pop_port_wiring\\n port_context \\n , u file line in update_port_down\\n admin_context, agent_host, port u file line in list_router_ids_on_host\\n context, constants.agent_type_l3, host , u file line in _get_agent_by_type_and_host\\n , u agentnotfoundbytypehost: agent with agent_type=l3 agent and could not be . neutron.plugins.ml2.drivers.agent._common_agent"
    },
    {
        "url": "https://stackoverflow.com/questions/51170886",
        "text": "heat.engine.resource admin create: cindervolume stack heat.engine.resource traceback most recent call last : heat.engine.resource file line in _action_recorder heat.engine.resource yield heat.engine.resource file line in _do_action heat.engine.resource yield self.action_handler_task action, heat.engine.resource file line in wrapper heat.engine.resource step next subtask heat.engine.resource file line in action_handler_task heat.engine.resource done check handler_data heat.engine.resource file line in check_create_complete heat.engine.resource complete super cindervolume, self .check_create_complete vol_id heat.engine.resource file line in check_create_complete heat.engine.resource resource_status=vol.status heat.engine.resource resourceinerror: went to status due to heat.engine.resource"
    },
    {
        "url": "https://stackoverflow.com/questions/51336502",
        "text": "mod_wsgi : target wsgi script cannot be loaded as python module. mod_wsgi : exception occurred processing wsgi script traceback most recent call last : file line in module application initialize_admin_application file line in initialize_admin_application config_files=_get_config_files file line in initialize_application common.configure config_files=config_files file line in configure keystone.conf.configure file line in configure do not monkey patch threading system modules. file line in __inner result f self, file line in register_cli_opt raise argsalreadyparsederror cannot register cli option argsalreadyparsederror: arguments already parsed: cannot register cli option mod_wsgi : target wsgi script cannot be loaded as python module. mod_wsgi : exception occurred processing wsgi script traceback most recent call last : file line in module application initialize_admin_application file line in initialize_admin_application config_files=_get_config_files file line in initialize_application common.configure config_files=config_files file line in configure keystone.conf.configure file line in configure do not monkey patch threading system modules. file line in __inner result f self, file line in register_cli_opt raise argsalreadyparsederror cannot register cli option argsalreadyparsederror: arguments already parsed: cannot register cli option"
    },
    {
        "url": "https://stackoverflow.com/questions/51550775",
        "text": "... packer: waiting for image creation status: saving packer: waiting for image creation status: saving packer: waiting for image creation status: saving packer: waiting for image creation status: saving packer: waiting for image creation status: saving packer: waiting for image creation status: saving ui error: openstack: waiting for image: resource not found openstack: waiting for image: resource not found ui: openstack: terminating the source server: ... openstack: terminating the source server: ... packer: waiting for state to become: deleted packer: waiting for state to become: deleted currently shutoff packer: on serverstaterefresh, returning deleted telemetry ending openstack telemetry found error: waiting for image: resource not found ui error: build errored: waiting for image: resource not found builds completed. waiting on interrupt barrier... machine readable: count string ui error: some builds didn t complete successfully and had errors: machine readable: openstack,error string waiting for image: resource not found ui error: openstack: waiting for image: resource not found ui: builds finished but no artifacts were created. telemetry finalizing. build errored: waiting for image: resource not found some builds didn t complete successfully and had errors: openstack: waiting for image: resource not found builds finished but no artifacts were created. waiting for all plugin processes to complete... plugin process exited plugin process exited"
    },
    {
        "url": "https://stackoverflow.com/questions/51670881",
        "text": "manila.share.drivers.service_instance no key path is available. may be non existent key path is provided. check path_to_private_key current value none and path_to_public_key current value in manila configuration file.: shareservernotfoundbyfilters: share server could not be found by filters: share_network_id is host is and status in or manila.network.linux.interface device aa already exists.: networkexception: bad router request: router already has a port on subnet bbda neutron server returns request_ids: manila.share.manager updating share status oslo_db.sqlalchemy.exc_filters dbapierror exception wrapped from pymysql.err.integrityerror u column cannot be null sql: u insert into share_server_backend_details created_at, updated_at, deleted_at, deleted, value, share_server_id values created_at s, updated_at s, deleted_at s, deleted s, key s, value s, share_server_id s parameters: datetime.datetime 2018, , none, none, none : integrityerror: u column cannot be null oslo_db.sqlalchemy.exc_filters traceback most recent call last : oslo_db.sqlalchemy.exc_filters file line in _execute_context oslo_db.sqlalchemy.exc_filters context oslo_db.sqlalchemy.exc_filters file line in do_execute oslo_db.sqlalchemy.exc_filters cursor.execute statement, parameters oslo_db.sqlalchemy.exc_filters file line in execute oslo_db.sqlalchemy.exc_filters result self._query query oslo_db.sqlalchemy.exc_filters file line in _query oslo_db.sqlalchemy.exc_filters conn.query q oslo_db.sqlalchemy.exc_filters file line in query oslo_db.sqlalchemy.exc_filters self._affected_rows self._read_query_result unbuffered=unbuffered oslo_db.sqlalchemy.exc_filters file line in _read_query_result oslo_db.sqlalchemy.exc_filters result.read oslo_db.sqlalchemy.exc_filters file line in read oslo_db.sqlalchemy.exc_filters first_packet self.connection._read_packet oslo_db.sqlalchemy.exc_filters file line in _read_packet oslo_db.sqlalchemy.exc_filters packet.check_error oslo_db.sqlalchemy.exc_filters file line in check_error oslo_db.sqlalchemy.exc_filters err.raise_mysql_exception self._data oslo_db.sqlalchemy.exc_filters file line in raise_mysql_exception oslo_db.sqlalchemy.exc_filters raise errorclass errno, errval oslo_db.sqlalchemy.exc_filters integrityerror: u column cannot be null oslo_db.sqlalchemy.exc_filters oslo_db.sqlalchemy.exc_filters dbapierror exception wrapped from pymysql.err.integrityerror u column cannot be null sql: u insert into share_server_backend_details created_at, updated_at, deleted_at, deleted, value, share_server_id values created_at s, updated_at s, deleted_at s, deleted s, key s, value s, share_server_id s parameters: datetime.datetime 2018, , none, none, none : integrityerror: u column cannot be null oslo_db.sqlalchemy.exc_filters traceback most recent call last : oslo_db.sqlalchemy.exc_filters file line in _execute_context oslo_db.sqlalchemy.exc_filters context oslo_db.sqlalchemy.exc_filters file line in do_execute oslo_db.sqlalchemy.exc_filters cursor.execute statement, parameters oslo_db.sqlalchemy.exc_filters file line in execute oslo_db.sqlalchemy.exc_filters result self._query query oslo_db.sqlalchemy.exc_filters file line in _query oslo_db.sqlalchemy.exc_filters conn.query q oslo_db.sqlalchemy.exc_filters file line in query oslo_db.sqlalchemy.exc_filters self._affected_rows self._read_query_result unbuffered=unbuffered oslo_db.sqlalchemy.exc_filters file line in _read_query_result oslo_db.sqlalchemy.exc_filters result.read oslo_db.sqlalchemy.exc_filters file line in read oslo_db.sqlalchemy.exc_filters first_packet self.connection._read_packet oslo_db.sqlalchemy.exc_filters file line in _read_packet oslo_db.sqlalchemy.exc_filters packet.check_error oslo_db.sqlalchemy.exc_filters file line in check_error oslo_db.sqlalchemy.exc_filters err.raise_mysql_exception self._data oslo_db.sqlalchemy.exc_filters file line in raise_mysql_exception oslo_db.sqlalchemy.exc_filters raise errorclass errno, errval oslo_db.sqlalchemy.exc_filters integrityerror: u column cannot be null oslo_db.sqlalchemy.exc_filters manila.share.manager creation of share instance failed: failed to get share server.: networkexception: unable to find in request body neutron server returns request_ids: manila.message.api creating message record for request_id oslo_messaging.rpc.server exception during message handling: networkexception: unable to find in request body neutron server returns request_ids: oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in _process_incoming oslo_messaging.rpc.server res self.dispatcher.dispatch message oslo_messaging.rpc.server file line in dispatch oslo_messaging.rpc.server return self._do_dispatch endpoint, method, ctxt, args oslo_messaging.rpc.server file line in _do_dispatch oslo_messaging.rpc.server result func ctxt, oslo_messaging.rpc.server file packages line in wrapped oslo_messaging.rpc.server return f self, oslo_messaging.rpc.server file line in wrapper oslo_messaging.rpc.server return func self, oslo_messaging.rpc.server file line in create_share_instance oslo_messaging.rpc.server oslo_messaging.rpc.server file line in __exit__ oslo_messaging.rpc.server self.force_reraise oslo_messaging.rpc.server file line in force_reraise oslo_messaging.rpc.server six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.server file line in create_share_instance oslo_messaging.rpc.server share_group=share_group_ref, oslo_messaging.rpc.server file line in _provide_share_server_for_share oslo_messaging.rpc.server return _wrapped_provide_share_server_for_share oslo_messaging.rpc.server file line in inner oslo_messaging.rpc.server return f oslo_messaging.rpc.server file line in _wrapped_provide_share_server_for_share oslo_messaging.rpc.server oslo_messaging.rpc.server file line in _create_share_server_in_backend oslo_messaging.rpc.server oslo_messaging.rpc.server file line in _setup_server oslo_messaging.rpc.server self.driver.deallocate_network context, share_server oslo_messaging.rpc.server file line in __exit__ oslo_messaging.rpc.server self.force_reraise oslo_messaging.rpc.server file line in force_reraise oslo_messaging.rpc.server six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.server file line in _setup_server oslo_messaging.rpc.server network_info, oslo_messaging.rpc.server file line in setup_server oslo_messaging.rpc.server return self._setup_server *args, oslo_messaging.rpc.server file line in _setup_server oslo_messaging.rpc.server self.admin_context, network_info oslo_messaging.rpc.server file line in set_up_service_instance oslo_messaging.rpc.server context, instance_name, network_info oslo_messaging.rpc.server file line in _create_service_instance oslo_messaging.rpc.server security_group self._get_or_create_security_group context oslo_messaging.rpc.server file line in inner oslo_messaging.rpc.server return f oslo_messaging.rpc.server file line in _get_or_create_security_group oslo_messaging.rpc.server name, description oslo_messaging.rpc.server file line in security_group_create oslo_messaging.rpc.server oslo_messaging.rpc.server networkexception: unable to find in request body oslo_messaging.rpc.server neutron server returns request_ids: oslo_messaging.rpc.server manila.share.manager updating share status manila.share.manager updating share status"
    },
    {
        "url": "https://stackoverflow.com/questions/51856695",
        "text": "magnum.conductor.handlers.cluster_conductor cluster has been deleted, stack_id: oslo.service.loopingcall fixed interval looping call failed: valueerror: field 0 cannot be none oslo.service.loopingcall traceback most recent call last : oslo.service.loopingcall file line in _run_loop oslo.service.loopingcall result func oslo.service.loopingcall file line in poll_and_check oslo.service.loopingcall self._sync_cluster_and_template_status stack oslo.service.loopingcall file line in _sync_cluster_and_template_status oslo.service.loopingcall self.cluster oslo.service.loopingcall file line in update_outputs oslo.service.loopingcall output.set_output stack, cluster_template, cluster oslo.service.loopingcall file line in set_output oslo.service.loopingcall self .set_output stack, cluster_template, cluster oslo.service.loopingcall file line in set_output oslo.service.loopingcall setattr cluster, self.cluster_attr, output_value oslo.service.loopingcall file line in setter oslo.service.loopingcall field_value field.coerce self, name, value oslo.service.loopingcall file line in coerce oslo.service.loopingcall return self._type.coerce obj, attr, value oslo.service.loopingcall file line in coerce oslo.service.loopingcall coerced_list.extend value oslo.service.loopingcall file line in extend oslo.service ..."
    },
    {
        "url": "https://stackoverflow.com/questions/51897825",
        "text": "oslo_db.sqlalchemy.engines mysql server has gone away e, oslo_db.sqlalchemy.engines dbconnectionerror: pymysql.err.operationalerror mysql server has gone away connection reset by peer sql: u select background on this at: oslo_db.sqlalchemy.engines database connection was found disconnected; reconnecting: dbconnectionerror: pymysql.err.operationalerror mysql server has gone away connection reset by peer sql: u select background on this at: oslo_db.sqlalchemy.engines mysql server has gone away e, oslo_db.sqlalchemy.engines dbconnectionerror: pymysql.err.operationalerror mysql server has gone away connection reset by peer sql: u select background on this at: oslo_db.sqlalchemy.engines database connection was found disconnected; reconnecting: dbconnectionerror: pymysql.err.operationalerror mysql server has gone away connection reset by peer sql: u select background on this at:"
    },
    {
        "url": "https://stackoverflow.com/questions/52001614",
        "text": "aug node glance registry: glance.registry.api.v1.images updating metadata for image aug node glance registry: eventlet.wsgi.server put http aug node glance registry: glance.registry.api.v1.images successfully deleted image aug node glance registry: eventlet.wsgi.server delete http"
    },
    {
        "url": "https://stackoverflow.com/questions/52001614",
        "text": "nova.compute.manager aada instance: instance snapshotting nova.compute.resource_tracker compute_service record updated for node 2.domain.tld nova.compute.manager instance: vm paused lifecycle event nova.compute.manager instance: during sync_power_state the instance has a pending task image_snapshot . skip. nova.virt.libvirt.driver aada instance: beginning cold snapshot process nova.compute.manager instance: vm stopped lifecycle event nova.compute.manager instance: during sync_power_state the instance has a pending task image_pending_upload . skip. nova.compute.resource_tracker auditing locally available compute resources for node node nova.compute.resource_tracker total usable vcpus: total allocated vcpus: nova.compute.resource_tracker final resource view: phys_ram=257584mb used_ram=135680mb phys_disk=1618gb used_disk=880gb total_vcpus=72 used_vcpus=44 pci_stats= count=53,numa_node=0,product_id= pcidevicepool pcidevicepool nova.compute.resource_tracker compute_service record updated for node 2.domain.tld nova.compute.manager instance: vm started lifecycle event nova.compute.manager instance: during sync_power_state the instance has a pending task image_pending_upload . skip. nova.compute.manager instance: vm resumed lifecycle event nova.compute.resource_tracker auditing locally available compute resources for node node nova.compute.manager instance: during sync_power_state the instance has a pending task image_pending_upload . skip. nova.compute.manager aada instance: successfully reverted task state from image_pending_upload on failure for instance. oslo_messaging.rpc.dispatcher aada exception during message handling: internal error: unable to execute qemu command device initialization failed oslo_messaging.rpc.dispatcher traceback most recent call last : oslo_messaging.rpc.dispatcher file line in _dispatch_and_reply oslo_messaging.rpc.dispatcher incoming.message oslo_messaging.rpc.dispatcher file line in _dispatch oslo_messaging.rpc.dispatcher return self._do_dispatch endpoint, method, ctxt, args oslo_messaging.rpc.dispatcher file line in _do_dispatch oslo_messaging.rpc.dispatcher result func ctxt, oslo_messaging.rpc.dispatcher file line in wrapped oslo_messaging.rpc.dispatcher payload oslo_messaging.rpc.dispatcher file line in __exit__ oslo_messaging.rpc.dispatcher self.force_reraise oslo_messaging.rpc.dispatcher file line in force_reraise oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher file line in wrapped oslo_messaging.rpc.dispatcher return f self, context, oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher log.warning msg, e, oslo_messaging.rpc.dispatcher file line in __exit__ oslo_messaging.rpc.dispatcher self.force_reraise oslo_messaging.rpc.dispatcher file line in force_reraise oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher return function self, context, oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher kwargs e, sys.exc_info oslo_messaging.rpc.dispatcher file line in __exit__ oslo_messaging.rpc.dispatcher self.force_reraise oslo_messaging.rpc.dispatcher file line in force_reraise oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher return function self, context, oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher kwargs e, sys.exc_info oslo_messaging.rpc.dispatcher file line in __exit__ oslo_messaging.rpc.dispatcher self.force_reraise oslo_messaging.rpc.dispatcher file line in force_reraise oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher return function self, context, oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher oslo_messaging.rpc.dispatcher file line in __exit__ oslo_messaging.rpc.dispatcher self.force_reraise oslo_messaging.rpc.dispatcher file line in force_reraise oslo_messaging.rpc.dispatcher six.reraise self.type_, self.value, self.tb oslo_messaging.rpc.dispatcher file line in decorated_function oslo_messaging.rpc.dispatcher oslo_messaging.rpc.dispatcher file line in snapshot_instance oslo_messaging.rpc.dispatcher task_states.image_snapshot oslo_messaging.rpc.dispatcher file line in _snapshot_instance oslo_messaging.rpc.dispatcher update_task_state oslo_messaging.rpc.dispatcher file line in snapshot oslo_messaging.rpc.dispatcher state, instance oslo_messaging.rpc.dispatcher file line in _snapshot_domain oslo_messaging.rpc.dispatcher self._attach_sriov_ports context, instance, guest oslo_messaging.rpc.dispatcher file line in _attach_sriov_ports oslo_messaging.rpc.dispatcher guest.attach_device cfg oslo_messaging.rpc.dispatcher file line in attach_device oslo_messaging.rpc.dispatcher self._domain.attachdeviceflags conf.to_xml , oslo_messaging.rpc.dispatcher file line in doit oslo_messaging.rpc.dispatcher result proxy_call self._autowrap, f, oslo_messaging.rpc.dispatcher file line in proxy_call oslo_messaging.rpc.dispatcher rv execute f, oslo_messaging.rpc.dispatcher file line in execute oslo_messaging.rpc.dispatcher six.reraise c, e, tb oslo_messaging.rpc.dispatcher file line in tworker oslo_messaging.rpc.dispatcher rv meth oslo_messaging.rpc.dispatcher file line in attachdeviceflags oslo_messaging.rpc.dispatcher if ret raise libvirterror virdomainattachdeviceflags failed , oslo_messaging.rpc.dispatcher libvirterror: internal error: unable to execute qemu command device initialization failed oslo_messaging.rpc.dispatcher nova.compute.resource_tracker total usable vcpus: total allocated vcpus: nova.compute.resource_tracker final resource view: phys_ram=257584mb used_ram=135680mb phys_disk=1618gb used_disk=880gb total_vcpus=72 used_vcpus=44 pci_stats= count=53,numa_node=0,product_id= pcidevicepool pcidevicepool"
    },
    {
        "url": "https://stackoverflow.com/questions/52108065",
        "text": "eption during message handling: attributeerror: object has no attribute oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in _process_incoming oslo_messaging.rpc.server res self.dispatcher.dispatch message oslo_messaging.rpc.server file line in dispatch oslo_messaging.rpc.server return self._do_dispatch endpoint, method, ctxt, args oslo_messaging.rpc.server file line in _do_dispatch oslo_messaging.rpc.server return self.serializer.serialize_entity ctxt, result oslo_messaging.rpc.server file line in serialize_entity oslo_messaging.rpc.server return self._base.serialize_entity context, entity oslo_messaging.rpc.server file line in serialize_entity oslo_messaging.rpc.server entity oslo_messaging.rpc.server file line in _process_iterable oslo_messaging.rpc.server for k, v in values.items oslo_messaging.rpc.server file line in dictcomp oslo_messaging.rpc.server for k, v in values.items oslo_messaging.rpc.server file line in serialize_entity oslo_messaging.rpc.server entity entity.obj_to_primitive oslo_messaging.rpc.server file line in __run_method oslo_messaging.rpc.server return getattr self.instance, __name oslo_messaging.rpc.server attributeerror: object has no attribute oslo_messaging.rpc.server"
    },
    {
        "url": "https://stackoverflow.com/questions/52192400",
        "text": "nova.osapi_compute.wsgi.server get http status: len: time:"
    },
    {
        "url": "https://stackoverflow.com/questions/52192400",
        "text": "nova.api.openstack.placement.requestlog delete status: len: microversion: authorization result of require all granted: granted authorization result of requireany : granted authorization result of require all granted: granted authorization result of requireany : granted nova.api.openstack.placement.requestlog get allocation_candidates?limit=1000 status: len: microversion:"
    },
    {
        "url": "https://stackoverflow.com/questions/52393580",
        "text": "sep vantiq dell api.service 32488 cinder.api.openstack.wsgi sep vantiq dell api.service 32488 cinder.api.openstack.wsgi sep vantiq dell api.service 32488 cinder.api.openstack.wsgi calling method: create, body: _process_stack sep vantiq dell api.service 32488 cinder.api.openstack.wsgi calling method: create, body: _process_stack sep vantiq dell api.service 32488 cinder.api.openstack.wsgi returned with http sep vantiq dell api.service 32488 pid: vars in bytes thu sep post generated bytes in msecs http headers in bytes switches on core sep vantiq dell api.service 32488 cinder.api.openstack.wsgi returned with http sep vantiq dell api.service 32488 pid: vars in bytes thu sep post generated bytes in msecs http headers in bytes switches on core"
    },
    {
        "url": "https://stackoverflow.com/questions/52466203",
        "text": "sep keystone wsgi admin : keystone.common.wsgi sep magnum api : wsme.api server si sep magnum api : traceback most recent call last : sep magnum api : file line in callfunction sep magnum api : result f self, sep magnum api : file decorator gen , line in get_all sep magnum api : file line in wrappe sep magnum api : sep magnum api : file line in enforce sep magnum api : add_policy_attributes target sep magnum api : file line in add_po sep magnum api : trustee_domain_id admin_osc.keystone .trustee_domain_id sep magnum api : file line in trus sep magnum api : access self.domain_admin_auth.get_access sep magnum api : file line in doma sep magnum api : auth_url=self.auth_url, sep magnum api : file line in auth_ sep magnum api : return conf ksconf.cfg_legacy_group .auth_uri.replace sep magnum api : attributeerror: object has no attribute"
    },
    {
        "url": "https://stackoverflow.com/questions/52670529",
        "text": "kubelet : desired_state_of_world_populator.go:273 processing volume for pod processing pvc pvc openstack mysql data mariadb server has non bound phase or empty pvc.spec.volumename"
    },
    {
        "url": "https://stackoverflow.com/questions/52757658",
        "text": "cinder.scheduler.host_manager abcd volume service is down or disabled. host: seldon cinder.scheduler.host_manager abcd volume service is down or disabled. host: scsi cinder.volume.flows.create_volume abcd failed to schedule_create_volume: no valid host was found."
    },
    {
        "url": "https://stackoverflow.com/questions/52866137",
        "text": "diskimage_builder.block_device.blockdevice create failed; rollback initiated traceback most recent call last : file packages line in cmd_create node.create"
    },
    {
        "url": "https://stackoverflow.com/questions/52884592",
        "text": "octavia.controller.queue.consumer starting consumer... cotyledon._utils unhandled exception: importerror: cannot import name opentype cotyledon._utils traceback most recent call last : cotyledon._utils file line in exit_on_exception cotyledon._utils yield cotyledon._utils file line in _run cotyledon._utils self.run cotyledon._utils file packages line in run cotyledon._utils self.endpoints endpoint.endpoint cotyledon._utils file line in __init__ cotyledon._utils invoke_on_load=true"
    },
    {
        "url": "https://stackoverflow.com/questions/53194657",
        "text": "debug neutron.wsgi accepted server debug neutron.pecan_wsgi.hooks.policy_enforcement attributes excluded by policy engine: u u _exclude_attributes_by_policy neutron.wsgi get http status: len: time: debug neutron.wsgi accepted server neutron.wsgi get http status: len: time: neutron.wsgi get http status: len: time: neutron.wsgi get http status: len: time: debug neutron.pecan_wsgi.hooks.policy_enforcement attributes excluded by policy engine: u u _exclude_attributes_by_policy neutron.wsgi get http status: len: time: neutron.wsgi get http status: len: time: neutron.wsgi get tenant_id=85ee25734f664d6d822379674c93da44 http status: len: time: debug neutron.wsgi returned with http __call__ neutron.wsgi get http status: len: time: neutron.wsgi get http status: len: time: neutron.wsgi get http status: len: time: debug neutron.db.agents_db agent healthcheck: found active agents agent_health_check debug neutron.db.agents_db agent healthcheck: found active agents agent_health_check"
    },
    {
        "url": "https://stackoverflow.com/questions/53194657",
        "text": "http status: len: time: debug neutron.wsgi accepted server neutron.pecan_wsgi.controllers.root no controller found for: floatingips returning response code pecannotfound neutron.pecan_wsgi.hooks.translation post failed client : the resource could not be found. debug neutron.pecan_wsgi.hooks.notifier skipping notifierhook processing as there was no resource associated with the request after neutron.wsgi post http status:"
    },
    {
        "url": "https://stackoverflow.com/questions/53206462",
        "text": "cat ....... cinder.volume.manager task transitioned into state from state predecessors most recent first : atom cinder.volume.flows.manager.create_volume.notifyvolumeactiontask;volume:create, create.start volume _name_id=none,admin_metadata= ? ,cluster_name=none,consistencygroup= ? ,consistencygroup_id=none,created_at=t,deleted=false,deleted_at=none,display_description=none,display_name= ? ? ,group_id=none,host= ? ,source_volid=none,status= ? ,volume_type=volumetype 4af0 4626 cinder.context.requestcontext object at , none volume _name_id=none,admin_metadata= ? ,cluster_name=none,consistencygroup= ? ,consistencygroup_id=none,created_at=t,deleted=false,deleted_at=none,display_description=none,display_name= ? ? ,group_id=none,host= ? ,source_volid=none,status= ? ,volume_type=volumetype 4af0 4626 requestspec cg_backend= ? ,backup_id=none,cgsnapshot_id=none,consistencygroup_id=none,group_backend= ? ,group_id=none,image_id=none,resource_backend= ? ,snapshot_id=none,source_replicaid= ? ,source_volid=none,volume=volume 4c5b b377 , cinder.context.requestcontext object at , u u u volume _name_id=none,admin_metadata= ? ,cluster_name=none,consistencygroup= ? ,consistencygroup_id=none,created_at=t,deleted=false,deleted_at=none,display_description=none,display_name= ? ? ,group_id=none,host= ? ,source_volid=none,status= ? ,volume_type=volumetype 4af0 4626 u , u u none, u u u u u u u u u u u u u u u u u false, u u u none, u none, u , u none, u none, u none, u none, u u u none, u none, u , u none, u none, u u none, u none, u u u none, u none, u none, u none, u none, u none, u none, u none, u u u u u u none, u none, u u u , u u u none, u none, u u u u u false, u false, u none, u none, u , u , u none, u , u none, u none, u none, u none, u none, u u u u u false, u true, u u u , u none, u none, u none, u , u u u none, u none, u none, u u u u none, u false, u u u none, u u u is true , u u u true, u none, u u u , u none , u u u u u u u u u u u u u u u u u u u false, u u u none, u none, u , u none, u none, u none, u none, u u u none, u none, u , u none , u u u u u volumetype created_at=t,deleted=false,deleted_at=none,description=none,extra_specs= is true ? ,qos_specs_id=none,updated_at=none , u none, u u u u u u u u u u u , u volumetype created_at=t,deleted=false,deleted_at=none,description=none,extra_specs= is true ? ,qos_specs_id=none,updated_at=none , u , cinder.context.requestcontext object at , requestspec cg_backend= ? ,backup_id=none,cgsnapshot_id=none,consistencygroup_id=none,group_backend= ? ,group_id=none,image_id=none,resource_backend= ? ,snapshot_id=none,source_replicaid= ? ,source_volid=none,volume=volume 4c5b b377 , none volume _name_id=none,admin_metadata= ? ,cluster_name=none,consistencygroup= ? ,consistencygroup_id=none,created_at=t,deleted=false,deleted_at=none,display_description=none,display_name= ? ? ,group_id=none,host= ? ,source_volid=none,status= ? ,volume_type=volumetype 4af0 4626 cinder.context.requestcontext object at , volume _name_id=none,admin_metadata= ? ,cluster_name=none,consistencygroup= ? ,consistencygroup_id=none,created_at=t,deleted=false,deleted_at=none,display_description=none,display_name= ? ? ,group_id=none,host= ? ,source_volid=none,status= ? ,volume_type=volumetype 4af0 4626 replicationerror: failed to enable image replication cinder.volume.manager traceback most recent call last : cinder.volume.manager file line in _execute_task cinder.volume.manager result task.execute **arguments cinder.volume.manager file line in execute cinder.volume.manager model_update self._create_raw_volume volume, cinder.volume.manager file line in _create_raw_volume cinder.volume.manager ret self.driver.create_volume volume cinder.volume.manager file line in create_volume cinder.volume.manager volume_id=volume.id cinder.volume.manager replicationerror: failed to enable image replication cinder.volume.manager cinder.volume.manager task transitioned into state from state cinder.volume.manager task cinder.volume.flows.manager.create_volume.notifyvolumeactiontask;volume:create, create.start transitioned into state from state cinder.volume.manager task transitioned into state from state cinder.volume.drivers.rbd volume volume no longer exists in backend cinder.volume.manager task transitioned into state from state cinder.volume.manager task transitioned into state from state cinder.volume.manager flow transitioned into state from state oslo_messaging.rpc.server exception during message handling: replicationerror: failed to enable image replication oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in _process_incoming oslo_messaging.rpc.server res self.dispatcher.dispatch message oslo_messaging.rpc.server file line in dispatch oslo_messaging.rpc.server return self._do_dispatch endpoint, method, ctxt, args oslo_messaging.rpc.server file line in _do_dispatch oslo_messaging.rpc.server result func ctxt, oslo_messaging.rpc.server file string , line in create_volume oslo_messaging.rpc.server file line in wrapper oslo_messaging.rpc.server result f oslo_messaging.rpc.server file line in create_volume oslo_messaging.rpc.server _run_flow oslo_messaging.rpc.server file line in _run_flow oslo_messaging.rpc.server flow_engine.run oslo_messaging.rpc.server file line in run oslo_messaging.rpc.server for _state in self.run_iter timeout=timeout : oslo_messaging.rpc.server file line in run_iter oslo_messaging.rpc.server failure.failure.reraise_if_any er_failures oslo_messaging.rpc.server file line in reraise_if_any oslo_messaging.rpc.server failures .reraise oslo_messaging.rpc.server file line in reraise oslo_messaging.rpc.server six.reraise *self._exc_info oslo_messaging.rpc.server file line in _execute_task oslo_messaging.rpc.server result task.execute **arguments oslo_messaging.rpc.server file line in execute oslo_messaging.rpc.server model_update self._create_raw_volume volume, oslo_messaging.rpc.server file line in _create_raw_volume oslo_messaging.rpc.server ret self.driver.create_volume volume oslo_messaging.rpc.server file line in create_volume oslo_messaging.rpc.server volume_id=volume.id oslo_messaging.rpc.server replicationerror: failed to enable image replication oslo_messaging.rpc.server"
    },
    {
        "url": "https://stackoverflow.com/questions/53206462",
        "text": "cat scheduler.log cinder.scheduler.filter_scheduler scheduling none from last vol service: server.loc@ceph#ceph : u traceback most recent call last , u file line in _execute_task\\n result task.execute **arguments u file line in model_update self._create_raw_volume volume, \\n u file line in _create_raw_volume\\n ret self.driver.create_volume volume \\n , u file line in create_volume\\n volume_id=volume.id , u replicationerror: failed to enable image cinder.scheduler.host_manager volume service is down. host: server.loc@lvm cinder.scheduler.host_manager volume service is down. host: server.loc@lvm2 cinder.scheduler.host_manager volume service is down. host: server.loc@ceph_2 cinder.scheduler.filter_scheduler scheduling none from last vol service: server.loc@ceph#ceph : u traceback most recent call last , u file line in _execute_task\\n result task.execute **arguments u file line in model_update self._create_raw_volume volume, \\n u file line in _create_raw_volume\\n ret self.driver.create_volume volume \\n , u file line in create_volume\\n volume_id=volume.id , u replicationerror: failed to enable image cinder.scheduler.host_manager volume service is down. host: server.loc@lvm cinder.scheduler.host_manager volume service is down. host: server.loc@lvm2 cinder.scheduler.host_manager volume service is down. host: server.loc@ceph_2 cinder.scheduler.filter_scheduler scheduling none from last vol service: server.loc@ceph#ceph : u traceback most recent call last , u file line in _execute_task\\n result task.execute **arguments u file line in model_update self._create_raw_volume volume, \\n u file line in _create_raw_volume\\n ret self.driver.create_volume volume \\n , u file line in create_volume\\n volume_id=volume.id , u replicationerror: failed to enable image cinder.message.api creating message record for request_id cinder.scheduler.flows.create_volume failed to run task cinder.scheduler.flows.create_volume.schedulecreatevolumetask;volume:create: no valid backend was found. exceeded maximum number of scheduling attempts for volume none: novalidbackend: no valid backend was found."
    },
    {
        "url": "https://stackoverflow.com/questions/53241374",
        "text": "d, t debug : rabbitmq.rb:66:in publishing heartbeat to stomp: send_fire, :last_sleep= d, t debug : rabbitmq.rb:64:in received heartbeat from stomp: receive_fire, d, t debug : rabbitmq.rb:66:in publishing heartbeat to stomp: send_fire, :last_sleep= d, t debug : rabbitmq.rb:64:in received heartbeat from stomp: receive_fire, t debug : pluginmanager.rb:167:in loading mcollective::facts::yaml_facts from mcollective d, t debug : pluginmanager.rb:44:in registering plugin facts_plugin with class mcollective::facts::yaml_facts single_instance: true d, t debug : pluginmanager.rb:167:in loading mcollective::connector::rabbitmq from mcollective d, t debug : pluginmanager.rb:44:in registering plugin connector_plugin with class mcollective::connector::rabbitmq single_instance: true d, t debug : pluginmanager.rb:167:in loading mcollective::security::psk from mcollective d, t debug : pluginmanager.rb:44:in registering plugin security_plugin with class mcollective::security::psk single_instance: true d, t debug : pluginmanager.rb:167:in"
    },
    {
        "url": "https://stackoverflow.com/questions/53419398",
        "text": "kubectl n kube system logs coredns reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout sigterm: shutting down servers then terminating reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout reflector.go:205 github.com failed to list get dial tcp i o timeout"
    },
    {
        "url": "https://stackoverflow.com/questions/53654545",
        "text": "trace heat.engine.resource file line in url_for trace heat.engine.resource raise exceptions.emptycatalog _ the service catalog is empty. trace heat.engine.resource emptycatalog: the service catalog is empty."
    },
    {
        "url": "https://stackoverflow.com/questions/54224530",
        "text": "keystone.auth.core could not find domain: default.: domainnotfound: could not find domain: default."
    },
    {
        "url": "https://stackoverflow.com/questions/54463543",
        "text": "cinder.api.middleware.fault caught error: class timed out waiting for a reply to message id cinder.api.middleware.fault traceback most recent call last : cinder.api.middleware.fault file line in __call__ cinder.api.middleware.fault return req.get_response self.application cinder.api.middleware.fault file line in send cinder.api.middleware.fault returned with http eventlet.wsgi.server post http status: len: time: cinder.api.openstack.wsgi post cinder.volume.api volume retrieved successfully."
    },
    {
        "url": "https://stackoverflow.com/questions/54463543",
        "text": "nova.compute.manager instance: instance failed block device setup nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _prep_block_device nova.compute.manager instance: wait_func=self._await_block_device_map_created nova.compute.manager instance: file line in attach_block_devices nova.compute.manager instance: _log_and_attach device nova.compute.manager instance: file line in _log_and_attach nova.compute.manager instance: bdm.attach *attach_args, nova.compute.manager instance: file line in attach nova.compute.manager instance: do_check_attach=do_check_attach nova.compute.manager instance: file line in wrapped nova.compute.manager instance: ret_val method obj, context, nova.compute.manager instance: file line in attach nova.compute.manager instance: connector nova.compute.manager instance: file line in wrapper nova.compute.manager instance: res method self, ctx, nova.compute.manager instance: file line in wrapper nova.compute.manager instance: res method self, ctx, volume_id, nova.compute.manager instance: file line in initialize_connection nova.compute.manager instance: exc.code if hasattr exc, else none nova.compute.manager instance: file line in __exit__ nova.compute.manager instance: self.force_reraise nova.compute.manager instance: file line in force_reraise nova.compute.manager instance: six.reraise self.type_, self.value, self.tb nova.compute.manager instance: file line in initialize_connection nova.compute.manager instance: context .volumes.initialize_connection volume_id, connector nova.compute.manager instance: file line in initialize_connection nova.compute.manager instance: connector nova.compute.manager instance: file line in _action nova.compute.manager instance: resp, body self.api.client.post url, nova.compute.manager instance: file line in post nova.compute.manager instance: return self._cs_request url, nova.compute.manager instance: file line in _cs_request nova.compute.manager instance: return self.request url, method, nova.compute.manager instance: file line in request nova.compute.manager instance: raise exceptions.from_response resp, body nova.compute.manager instance: clientexception: the server has either erred or is incapable of performing the requested operation. http request id: nova.compute.manager instance: debug nova.compute.claims instance: aborting claim: claim: mb memory, gb disk abort debug oslo_concurrency.lockutils lock acquired by :: waited inner nova.scheduler.client.report deleted allocation for instance"
    },
    {
        "url": "https://stackoverflow.com/questions/54528652",
        "text": "masakari processmonitor: command: systemctl restart openstack nova compute.service masakari processmonitor: exit code: masakari processmonitor: stdout: u masakari processmonitor: stderr: u job for openstack nova compute.service failed because the control process exited with code. see systemctl status openstack nova compute.service and journalctl xe for details.\\n processexecutionerror: unexpected while running command. masakari processmonitor: masakarimonitors.ha.masakari a my_domain_name , datetime.datetime 2019, masakari processmonitor: masakarimonitors.processmonitor.process exception caught: object has no attribute attributeerror: object has no attribute masakari processmonitor: masakarimonitors.processmonitor.process traceback most recent call last : masakari processmonitor: masakarimonitors.processmonitor.process file line in main masakari processmonitor: masakarimonitors.processmonitor.process self.process_handler.restart_processes down_process_list masakari processmonitor: masakarimonitors.processmonitor.process file line in restart_processes masakari processmonitor: masakarimonitors.processmonitor.process event masakari processmonitor: masakarimonitors.processmonitor.process file line in send_notification masakari processmonitor: masakarimonitors.processmonitor.process client self._make_client masakari processmonitor: masakarimonitors.processmonitor.process file line in _make_client masakari processmonitor: masakarimonitors.processmonitor.process return conn.instance_ha masakari processmonitor: masakarimonitors.processmonitor.process attributeerror: object has no attribute masakari processmonitor: masakarimonitors.processmonitor.process"
    },
    {
        "url": "https://stackoverflow.com/questions/55492698",
        "text": "openstackmagnum atomic install storage ostree system system package no set requests_ca_bundle= name heat container agent docker.io systemctl start heat container agent failed to start heat container agent.service: unit heat container agent.service not found. util.py : failed running"
    },
    {
        "url": "https://stackoverflow.com/questions/56184562",
        "text": "critical kuryr unauthorized: the request you have made requires authentication. http request id: kuryr traceback most recent call last : kuryr file line in module kuryr configure_app kuryr file line in configure_app kuryr controllers.check_for_neutron_ext_support kuryr file line in check_for_neutron_ext_support kuryr app.neutron.show_extension mandatory_neutron_extension kuryr file line in show_extension kuryr return self.get self.extension_path ext_alias, kuryr file line in get kuryr kuryr file line in retry_request kuryr kuryr file line in do_request kuryr resp, replybody self.httpclient.do_request action, method, kuryr file line in do_request kuryr self._check_uri_length url kuryr file line in _check_uri_length kuryr uri_len len self.endpoint_url len url kuryr file line in endpoint_url kuryr return self.get_endpoint kuryr file line in get_endpoint kuryr return self.session.get_endpoint auth or self.auth, kuryr file line in get_endpoint kuryr return auth.get_endpoint self, kuryr file line in get_endpoint kuryr service_catalog self.get_access session .service_catalog kuryr file line in get_access kuryr self.auth_ref self.get_auth_ref session kuryr file line in get_auth_ref kuryr return self._plugin.get_auth_ref session, kuryr file line in get_auth_ref kuryr kuryr file line in post kuryr return self.request url, kuryr file line in inner kuryr return wrapped kuryr file line in request kuryr raise exceptions.from_response resp, method, url kuryr unauthorized: the request you have made requires authentication. http request id: kuryr unable to load app callable not found or import no app loaded. going in full dynamic mode uwsgi is running in multiple interpreter mode"
    },
    {
        "url": "https://stackoverflow.com/questions/56871759",
        "text": "if int version current_ver: keystone valueerror: invalid literal for int with base"
    },
    {
        "url": "https://stackoverflow.com/questions/57568505",
        "text": "in the logs there is this: nova.compute.claims instance: attempting claim on node compute.test.local: memory mb, disk gb, vcpus cpu nova.compute.claims instance: total memory: mb, used: mb nova.compute.claims instance: memory limit not specified, defaulting to unlimited nova.compute.claims instance: total disk: gb, used: gb nova.compute.claims instance: disk limit not specified, defaulting to unlimited nova.compute.claims instance: total vcpu: vcpu, used: vcpu nova.compute.claims instance: vcpu limit not specified, defaulting to unlimited nova.compute.claims instance: claim successful on node compute.test.local nova.compute.manager instance failed network setup after attempt s : portnotusable: port not usable for instance nova.compute.manager traceback most recent call last : nova.compute.manager file line in _allocate_network_async nova.compute.manager resource_provider_mapping=resource_provider_mapping nova.compute.manager file line in allocate_for_instance nova.compute.manager context, instance, neutron, requested_networks, nova.compute.manager file line in _validate_requested_port_ids nova.compute.manager nova.compute.manager portnotusable: port not usable for instance nova.compute.manager nova.virt.libvirt.driver instance: creating image nova.compute.manager instance: instance failed to spawn: portnotusable: port not usable for instance nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _build_resources nova.compute.manager instance: yield resources nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: block_device_info=block_device_info nova.compute.manager instance: file line in spawn nova.compute.manager instance: nova.compute.manager instance: file line in _get_guest_xml nova.compute.manager instance: network_info_str str network_info nova.compute.manager instance: file line in __str__ nova.compute.manager instance: return self._sync_wrapper fn, nova.compute.manager instance: file line in _sync_wrapper nova.compute.manager instance: self.wait nova.compute.manager instance: file line in wait nova.compute.manager instance: self : self._gt.wait nova.compute.manager instance: file line in wait nova.compute.manager instance: return self._exit_event.wait nova.compute.manager instance: file line in wait nova.compute.manager instance: current.throw *self._exc nova.compute.manager instance: file line in main nova.compute.manager instance: result function nova.compute.manager instance: file line in context_wrapper nova.compute.manager instance: return func nova.compute.manager instance: file line in _allocate_network_async nova.compute.manager instance: six.reraise *exc_info nova.compute.manager instance: file line in _allocate_network_async nova.compute.manager instance: resource_provider_mapping=resource_provider_mapping nova.compute.manager instance: file line in allocate_for_instance nova.compute.manager instance: context, instance, neutron, requested_networks, nova.compute.manager instance: file line in _validate_requested_port_ids nova.compute.manager instance: nova.compute.manager instance: portnotusable: port not usable for instance nova.compute.manager instance: nova.compute.manager instance: terminating instance nova.virt.libvirt.driver instance: instance destroyed successfully. nova.virt.libvirt.driver instance: deleting instance files nova.virt.libvirt.driver instance: deletion of complete nova.compute.manager instance: took seconds to destroy the instance on the hypervisor. nova.network.neutronv2.api unable to clear device id for port badrequest: expecting to find domain in user. the server could not comply with the request since it is either malformed or otherwise incorrect. the client is assumed to be in error. http request id: nova.network.neutronv2.api traceback most recent call last : nova.network.neutronv2.api file line in _unbind_ports nova.network.neutronv2.api port_client.update_port port_id, port_req_body nova.network.neutronv2.api file line in wrapper nova.network.neutronv2.api ret obj nova.network.neutronv2.api file line in update_port nova.network.neutronv2.api revision_number=revision_number nova.network.neutronv2.api file line in wrapper nova.network.neutronv2.api ret obj nova.network.neutronv2.api file line in _update_resource nova.network.neutronv2.api return self.put path, nova.network.neutronv2.api file line in wrapper nova.network.neutronv2.api ret obj nova.network.neutronv2.api file line in put nova.network.neutronv2.api nova.network.neutronv2.api file line in wrapper nova.network.neutronv2.api ret obj nova.network.neutronv2.api file line in retry_request nova.network.neutronv2.api nova.network.neutronv2.api file line in wrapper nova.network.neutronv2.api ret obj nova.network.neutronv2.api file line in do_request nova.network.neutronv2.api nova.network.neutronv2.api file line in do_request nova.network.neutronv2.api return self.request url, method, nova.network.neutronv2.api file line in request nova.network.neutronv2.api resp super sessionclient, self .request nova.network.neutronv2.api file line in request nova.network.neutronv2.api return self.session.request url, method, nova.network.neutronv2.api file line in request nova.network.neutronv2.api auth_headers self.get_auth_headers auth nova.network.neutronv2.api file line in get_auth_headers nova.network.neutronv2.api return auth.get_headers self, nova.network.neutronv2.api file line in get_headers nova.network.neutronv2.api token self.get_token session nova.network.neutronv2.api file line in get_token nova.network.neutronv2.api return self.get_access session .auth_token nova.network.neutronv2.api file line in get_access nova.network.neutronv2.api self.auth_ref self.get_auth_ref session nova.network.neutronv2.api file line in get_auth_ref nova.network.neutronv2.api return self._plugin.get_auth_ref session, nova.network.neutronv2.api file line in get_auth_ref nova.network.neutronv2.api nova.network.neutronv2.api file line in post nova.network.neutronv2.api return self.request url, nova.network.neutronv2.api file line in request nova.network.neutronv2.api raise exceptions.from_response resp, method, url nova.network.neutronv2.api badrequest: expecting to find domain in user. the server could not comply with the request since it is either malformed or otherwise incorrect. the client is assumed to be in error. http request id: nova.network.neutronv2.api"
    },
    {
        "url": "https://stackoverflow.com/questions/57756313",
        "text": "novalidhost: no valid host was found. there are not enough hosts available. : novalidhost_remote: no valid host was found. there are not enough hosts available. nova.scheduler.utils instance: setting instance to state.: novalidhost_remote: no valid host was found. there are not enough hosts available."
    },
    {
        "url": "https://stackoverflow.com/questions/57967958",
        "text": "nova.virt.libvirt.driver instance: live migration failure: unsafe migration: migration without shared storage is unsafe: libvirterror: unsafe migration: migration without shared storage is unsafe nova.virt.libvirt.driver instance: migration operation has aborted nova.compute.manager instance: swapping old allocation on held by migration for instance"
    },
    {
        "url": "https://stackoverflow.com/questions/57967958",
        "text": "nova.virt.libvirt.imagecache image at checking os_vif successfully plugged vif vifbridge nova.virt.libvirt.imagecache active base files:"
    },
    {
        "url": "https://stackoverflow.com/questions/58169598",
        "text": "t debug o.e.a.a.c.h.transportclusterhealthaction node timed out while retrying cluster:monitor health after failure timeout t warn r.suppressed node path: params: org.elasticsearch.discovery.masternotdiscoveredexception: null at org.elasticsearch.action.support.master.transportmasternodeaction$asyncsingleaction$3.ontimeout transportmasternodeaction.java:251 elasticsearch at org.elasticsearch.cluster.clusterstateobserver$contextpreservinglistener.ontimeout clusterstateobserver.java:325 elasticsearch at org.elasticsearch.cluster.clusterstateobserver$observerclusterstatelistener.ontimeout clusterstateobserver.java:252 elasticsearch at org.elasticsearch.cluster.service.clusterapplierservice$notifytimeout.run clusterapplierservice.java:572 elasticsearch at org.elasticsearch.common.util.concurrent.threadcontext$contextpreservingrunnable.run threadcontext.java:688 elasticsearch at java.util.concurrent.threadpoolexecutor.runworker threadpoolexecutor.java:1128 ?:? at java.util.concurrent.threadpoolexecutor$worker.run threadpoolexecutor.java:628 ?:? at java.lang.thread.run thread.java:835 ?:? t warn o.e.c.c.clusterformationfailurehelper node master not discovered yet: have discovered node ip ip di ml.machine_memory=33728778240, xpack.installed=true, ml.max_open_jobs=20 , node ip ip dim ml.machine_memory=33728778240, ml.max_open_jobs=20, xpack.installed=true discovery will continue using ip from hosts providers and from last known cluster state; node term last accepted version in term"
    },
    {
        "url": "https://stackoverflow.com/questions/58479457",
        "text": "openstack_dashboard.api.rest.utils invoking apiclient traceback most recent call last : file line in _wrapped data function self, request, file line in get nodes ironic.node_list request file line in node_list node_manager ironicclient request .node file line in wrapped value func file line in ironicclient file line in client return client_class *args, file line in __init__ self.http_client http._construct_http_client *args, typeerror: _construct_http_client takes at least argument given"
    },
    {
        "url": "https://stackoverflow.com/questions/58507130",
        "text": "octavia.controller.queue.endpoint creating load balancer octavia.network.drivers.neutron.allowed_address_pairs port already exists. nothing to be done. octavia.controller.worker.tasks.database_tasks created amphora in db with id octavia.certificates.generator.local signing a certificate request using openssl locally. octavia.certificates.generator.local using ca certificate from config. octavia.certificates.generator.local using ca private key from config. octavia.certificates.generator.local using ca private key passphrase from config. octavia.compute.drivers.nova_driver nova failed to build the instance due to: invalid key_name provided. http request id: : badrequest: invalid key_name provided. http request id:"
    },
    {
        "url": "https://stackoverflow.com/questions/58534100",
        "text": "neutron.agent.metadata.agent neutron.agent.metadata.agent unexpected error.: remoteerror: remote error: timeouterror queuepool limit of size overflow reached, connection timed out, timeout background on this at: neutron.agent.metadata.agent traceback most recent call last : neutron.agent.metadata.agent file line in __call__ neutron.agent.metadata.agent instance_id, tenant_id self._get_instance_and_tenant_id neutron.agent.metadata.agent file line in _get_instance_and_tenant_id neutron.agent.metadata.agent ports self._get_ports remote_address, network_id, router_id neutron.agent.metadata.agent file line in _get_ports neutron.agent.metadata.agent return self._get_ports_for_remote_address remote_address, networks neutron.agent.metadata.agent file line in __call__ neutron.agent.metadata.agent return self.func target_self, neutron.agent.metadata.agent file line in _get_ports_for_remote_address neutron.agent.metadata.agent ip_address=remote_address neutron.agent.metadata.agent file line in _get_ports_from_server neutron.agent.metadata.agent return self.plugin_rpc.get_ports self.context, filters neutron.agent.metadata.agent file line in get_ports neutron.agent.metadata.agent return cctxt.call context, neutron.agent.metadata.agent file line in call neutron.agent.metadata.agent return self._original_context.call ctxt, method, neutron.agent.metadata.agent file line in call neutron.agent.metadata.agent neutron.agent.metadata.agent file line in _send neutron.agent.metadata.agent neutron.agent.metadata.agent file line in send neutron.agent.metadata.agent call_monitor_timeout, neutron.agent.metadata.agent file line in _send neutron.agent.metadata.agent raise result neutron.agent.metadata.agent remoteerror: remote error: timeouterror queuepool limit of size overflow reached, connection timed out, timeout background on this at: neutron.agent.metadata.agent u traceback most recent call last , u file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , u file line in return self._do_dispatch endpoint, method, ctxt, args , u file line in _do_dispatch\\n result func ctxt, \\n u file line in get_ports\\n return self.plugin.get_ports context, , u file line in return method , u file line in setattr e, true , u file line in __exit__\\n self.force_reraise \\n , u file line in force_reraise\\n six.reraise self.type_, self.value, self.tb \\n u file line in return f , u file line in ectxt.value e.inner_exc\\n , u file line in __exit__\\n self.force_reraise \\n , u file line in force_reraise\\n six.reraise self.type_, self.value, self.tb \\n u file line in return f , u file line in log.debug retry wrapper got retriable exception: , e , u file line in __exit__\\n self.force_reraise \\n , u file line in force_reraise\\n six.reraise self.type_, self.value, self.tb \\n u file line in return f \\n u file line in get_ports\\n items self._make_port_dict c, fields for c in query , u file line in __iter__\\n return self._execute_and_instances context \\n , u file line in _execute_and_instances\\n close_with_result=true \\n u file line in _get_bind_args\\n , u file line in _connection_from_session\\n conn self.session.connection **kw \\n u file line in execution_options=execution_options , u file line in _connection_for_bind\\n conn engine.contextual_connect **kw \\n u file line in contextual_connect\\n self._wrap_pool_connect self.pool.connect, none , u file line in _wrap_pool_connect\\n return fn , u file line in return _connectionfairy._checkout self \\n u file line in fairy _connectionrecord.checkout pool \\n u file line in rec pool._do_get \\n , u file line in _do_get\\n self.size , self.overflow , self._timeout , u timeouterror: queuepool limit of size overflow reached, connection timed out, timeout background on this at: neutron.agent.metadata.agent oslo_messaging._drivers.amqpdriver number of call queues is greater than threshold: there could be a leak. increasing threshold to: oslo_messaging._drivers.amqpdriver number of call queues is greater than threshold: there could be a leak. increasing threshold to: oslo_messaging._drivers.amqpdriver number of call queues is greater than threshold: there could be a leak. increasing threshold to:"
    },
    {
        "url": "https://stackoverflow.com/questions/58744548",
        "text": "journalctl xe ... nov polkitd : registered authentication agent for unix process:3279:123978 system bus name usr nov systemd : starting the apache http server... subject: unit httpd.service has begun start up defined by: systemd support: unit httpd.service has begun starting up. ... nov python : mod importlib.import_module self.settings_module nov python : file line in import_module nov python : __import__ name nov python : file line in modul nov python : from local.local_settings import noqa: nov python : file line nov python : nov python : nov python : indentationerror: unexpected indent nov systemd : httpd.service: control process exited, nov systemd : failed to start the apache http server. subject: unit httpd.service has failed defined by: systemd support: unit httpd.service has failed. the result is failed. nov systemd : unit httpd.service entered failed state. nov systemd : httpd.service failed. nov polkitd : unregistered authentication agent for unix process:3279:123978 system bus name obj lines end"
    },
    {
        "url": "https://stackoverflow.com/questions/59432955",
        "text": "info: info: os refresh config aborting... debug: an exception occurred traceback most recent call last : file line in install _run_orc instack_env file line in _run_orc _run_live_command args, instack_env, file line in _run_live_command raise runtimeerror failed. see log for details. name runtimeerror: os refresh config failed. see log for details. error: undercloud upgrade failed. reason: os refresh config failed. see log for details."
    },
    {
        "url": "https://stackoverflow.com/questions/59823588",
        "text": "mod_wsgi : target wsgi script cannot be loaded as python module. mod_wsgi : exception occurred processing wsgi script traceback most recent call last : file line in module from django.core.wsgi import get_wsgi_application importerror: no module named django.core.wsgi"
    },
    {
        "url": "https://stackoverflow.com/questions/60372066",
        "text": "microstack.init auto microstack_init configuring networking ... microstack_init setting up forwarding... microstack_init opening horizon dashboard up to microstack_init waiting for rabbitmq to start ... waiting for microstack_init rabbitmq started! microstack_init configuring rabbitmq ... microstack_init rabbitmq configured! microstack_init waiting for mysql server to start ... waiting for microstack_init mysql server started! creating databases ... warning: using grant for creating new user is deprecated and will be removed in future release. create new user with create user statement. result self._query query warning: using grant statement to modify existing user s properties other than privileges is deprecated and will be removed in future release. use alter user statement for this operation. result self._query query microstack_init configuring keystone fernet keys ... microstack_init bootstrapping keystone ... microstack_init creating service project ... microstack_init keystone configured! microstack_init configuring nova compute hypervisor ... microstack_init configuring nova control plane services ... waiting for microstack_init creating flavors... microstack_init configuring neutron waiting for microstack_init configuring glance ... waiting for microstack_init adding cirros image ... microstack_init downloading cirros image ... ........................................................................ microstack_init creating microstack keypair microstack_init creating security group rules ... microstack_init restarting libvirt and virtlogd ... microstack_init complete. marked microstack as initialized!"
    },
    {
        "url": "https://stackoverflow.com/questions/61050114",
        "text": "multipass exec microstack vm sudo microstack.init do you want to setup clustering? yes no yes microstack_init configuring clustering ... what is this machines role? control compute compute please enter a cluster password please re enter password please enter the ip address of the control node please enter the ip address of this node microstack_init i am a compute node."
    },
    {
        "url": "https://stackoverflow.com/questions/61115156",
        "text": "neutron openvswitch agent.log.1:1022: neutron.agent.linux.async_process received from ovsdb client monitor tcp:127.0.0.1:6640 bridge name : ovsdb client: tcp:127.0.0.1:6640: open_vswitch database was removed neutron openvswitch agent.log.1:1023: neutron.agent.linux.async_process process ovsdb client monitor tcp:127.0.0.1:6640 bridge name dies due to the error: ovsdb client: tcp:127.0.0.1:6640: open_vswitch database was removed neutron openvswitch agent.log.1:1024: neutron.agent.linux.async_process received from ovsdb client monitor tcp:127.0.0.1:6640 interface name,ofport,external_ids : ovsdb client: tcp:127.0.0.1:6640: open_vswitch database was removed neutron openvswitch agent.log.1:1025: neutron.agent.linux.async_process process ovsdb client monitor tcp:127.0.0.1:6640 interface name,ofport,external_ids dies due to the error: ovsdb client: tcp:127.0.0.1:6640: open_vswitch database was removed neutron openvswitch agent.log.1:1027: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ofswitch ofctl request cookie=0,cookie_mask=0,flags=0,match=ofpmatch ,out_group=4294967295,out_port=4294967295,table_id=23,type=1 timed out: timeout: seconds neutron openvswitch agent.log.1:1028: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int failed to communicate with the switch: runtimeerror: ofctl request cookie=0,cookie_mask=0,flags=0,match=ofpmatch ,out_group=4294967295,out_port=4294967295,table_id=23,type=1 timed out neutron openvswitch agent.log.1:1029: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int traceback most recent call last : neutron openvswitch agent.log.1:1030: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int file line in check_canary_table neutron openvswitch agent.log.1:1031: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int flows self.dump_flows constants.canary_table neutron openvswitch agent.log.1:1032: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int file line in dump_flows neutron openvswitch agent.log.1:1033: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int reply_multi=true neutron openvswitch agent.log.1:1034: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int file line in _send_msg neutron openvswitch agent.log.1:1035: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int raise runtimeerror m neutron openvswitch agent.log.1:1036: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int runtimeerror: ofctl request cookie=0,cookie_mask=0,flags=0,match=ofpmatch ,out_group=4294967295,out_port=4294967295,table_id=23,type=1 timed out neutron openvswitch agent.log.1:1037: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.br_int neutron openvswitch agent.log.1:1044: neutron.agent.linux.async_process received from ovsdb client monitor tcp:127.0.0.1:6640 interface name,ofport,external_ids : with signal terminated neutron openvswitch agent.log.1:1045: neutron.agent.linux.async_process process ovsdb client monitor tcp:127.0.0.1:6640 interface name,ofport,external_ids dies due to the error: with signal terminated neutron openvswitch agent.log.1:1047: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp agent main thread died of an exception: asyncprocessexception: process is not running. neutron openvswitch agent.log.1:1048: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp traceback most recent call last : neutron openvswitch agent.log.1:1049: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in agent_main_wrapper neutron openvswitch agent.log.1:1050: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp ovs_agent.main bridge_classes neutron openvswitch agent.log.1:1051: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in main neutron openvswitch agent.log.1:1052: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp agent.daemon_loop neutron openvswitch agent.log.1:1053: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in wrapper neutron openvswitch agent.log.1:1054: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp result f neutron openvswitch agent.log.1:1055: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in daemon_loop neutron openvswitch agent.log.1:1056: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp self.rpc_loop polling_manager=pm, bridges_monitor=bm neutron openvswitch agent.log.1:1057: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in __exit__ neutron openvswitch agent.log.1:1058: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp self.gen.next neutron openvswitch agent.log.1:1059: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in get_bridges_monitor neutron openvswitch agent.log.1:1060: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp mon.stop neutron openvswitch agent.log.1:1061: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp file line in stop neutron openvswitch agent.log.1:1062: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp raise asyncprocessexception _ process is not running. neutron openvswitch agent.log.1:1063: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp asyncprocessexception: process is not running. neutron openvswitch agent.log.1:1064: neutron.plugins.ml2.drivers.openvswitch.agent.openflow.native.ovs_ryuapp neutron openvswitch agent.log.1:1065: critical neutron unhandled error: asyncprocessexception: process is not running. neutron openvswitch agent.log.1:1066: neutron traceback most recent call last : neutron openvswitch agent.log.1:1067: neutron file line in module neutron openvswitch agent.log.1:1068: neutron sys.exit main neutron openvswitch agent.log.1:1069: neutron file line in main neutron openvswitch agent.log.1:1070: neutron agent_main.main neutron openvswitch agent.log.1:1071: neutron file line in main neutron openvswitch agent.log.1:1072: neutron mod.main neutron openvswitch agent.log.1:1073: neutron file line in main neutron openvswitch agent.log.1:1074: neutron neutron openvswitch agent.log.1:1075: neutron file line in run_apps neutron openvswitch agent.log.1:1076: neutron hub.joinall services neutron openvswitch agent.log.1:1077: neutron file line in joinall neutron openvswitch agent.log.1:1078: neutron t.wait neutron openvswitch agent.log.1:1079: neutron file line in wait neutron openvswitch agent.log.1:1080: neutron return self._exit_event.wait neutron openvswitch agent.log.1:1081: neutron file line in wait neutron openvswitch agent.log.1:1082: neutron current.throw *self._exc neutron openvswitch agent.log.1:1083: neutron file line in main neutron openvswitch agent.log.1:1084: neutron result function neutron openvswitch agent.log.1:1085: neutron file line in _launch neutron openvswitch agent.log.1:1086: neutron raise e neutron openvswitch agent.log.1:1087: neutron asyncprocessexception: process is not running. neutron openvswitch agent.log.1:1088: neutron neutron openvswitch agent.log.1:1107: neutron.services.trunk.drivers.openvswitch.agent.ovsdb_handler no port present on trunk bridge tbr in seconds.: waittimeout: timed out after seconds neutron openvswitch agent.log.1:1108: neutron.services.trunk.drivers.openvswitch.agent.ovsdb_handler no port present on trunk bridge tbr in seconds.: waittimeout: timed out after seconds neutron openvswitch agent.log.1:1109: neutron.services.trunk.drivers.openvswitch.agent.ovsdb_handler no port present on trunk bridge tbr in seconds.: waittimeout: timed out after seconds neutron openvswitch agent.log.1:1110: neutron.services.trunk.drivers.openvswitch.agent.ovsdb_handler no port present on trunk bridge tbr in seconds.: waittimeout: timed out after seconds"
    },
    {
        "url": "https://stackoverflow.com/questions/61139735",
        "text": "neutron.agent.dhcp.agent starting network dhcp configuration debug neutron.agent.dhcp.agent calling driver for network: action: enable call_driver debug neutron.agent.linux.utils unable to access get_value_from_file oslo.privsep.daemon running privsep helper: critical oslo.privsep.daemon privsep helper command exited non zero neutron.agent.dhcp.agent unable to enable dhcp for oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron.agent.dhcp.agent traceback most recent call last : neutron.agent.dhcp.agent file line in call_driver neutron.agent.dhcp.agent getattr driver, action neutron.agent.dhcp.agent file line in enable neutron.agent.dhcp.agent common_utils.wait_until_true self._enable, neutron.agent.dhcp.agent file line in wait_until_true neutron.agent.dhcp.agent while not predicate : neutron.agent.dhcp.agent file line in _enable neutron.agent.dhcp.agent interface_name self.device_manager.setup self.network neutron.agent.dhcp.agent file line in setup neutron.agent.dhcp.agent ip_lib.ipwrapper .ensure_namespace network.namespace neutron.agent.dhcp.agent file line in ensure_namespace neutron.agent.dhcp.agent if not self.netns.exists name : neutron.agent.dhcp.agent file line in exists neutron.agent.dhcp.agent return network_namespace_exists name neutron.agent.dhcp.agent file line in network_namespace_exists neutron.agent.dhcp.agent output list_network_namespaces **kwargs neutron.agent.dhcp.agent file line in list_network_namespaces neutron.agent.dhcp.agent return privileged.list_netns **kwargs neutron.agent.dhcp.agent file line in _wrap neutron.agent.dhcp.agent self.start neutron.agent.dhcp.agent file line in start neutron.agent.dhcp.agent channel daemon.rootwrapclientchannel context=self neutron.agent.dhcp.agent file line in __init__ neutron.agent.dhcp.agent raise failedtodropprivileges msg neutron.agent.dhcp.agent oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron.agent.dhcp.agent"
    },
    {
        "url": "https://stackoverflow.com/questions/61139735",
        "text": "neutron.common.config logging enabled! neutron.common.config version debug neutron.common.config command line: config config log setup_logging neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent interface mappings: neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent bridge mappings: oslo.privsep.daemon running privsep helper: critical oslo.privsep.daemon privsep helper command exited non zero critical neutron unhandled error: oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron traceback most recent call last : neutron file line in module neutron sys.exit main neutron file line in main neutron agent_main.main neutron file line in main neutron manager linuxbridgemanager bridge_mappings, interface_mappings neutron file line in __init__ neutron self.validate_interface_mappings neutron file line in validate_interface_mappings neutron if not ip_lib.device_exists interface : neutron file line in device_exists neutron return ipdevice device_name, .exists neutron file line in exists neutron return privileged.interface_exists self.name, self.namespace neutron file line in sync_inner neutron return input_func *args, neutron file line in _wrap neutron self.start neutron file line in start neutron channel daemon.rootwrapclientchannel context=self neutron file line in __init__ neutron raise failedtodropprivileges msg neutron oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron"
    },
    {
        "url": "https://stackoverflow.com/questions/61286693",
        "text": "neutron.plugins.ml2.plugin attempt to bind port neutron.plugins.ml2.managers port does not have an ip address assigned and there are no driver with the port cannot be bound. neutron.plugins.ml2.managers failed to bind port on host dev openstack controller.ershandc.org for vnic_type normal using segments none, u"
    },
    {
        "url": "https://stackoverflow.com/questions/61286693",
        "text": "neutron.plugins.ml2.plugin attempt to bind port neutron.plugins.ml2.managers port does not have an ip address assigned and there are no driver with the port cannot be bound. neutron.plugins.ml2.managers failed to bind port on host dev openstack controller.ershandc.org for vnic_type normal using segments none, u u"
    },
    {
        "url": "https://stackoverflow.com/questions/61303668",
        "text": "namespace: heat priority: node: kind control plane start time: sun, apr labels: pod template annotations: none status: running ip: ips: ip: controlled by: replicaset keystone containers: keystone: container id: containerd: image: openio openstack keystone image id: docker.io ports: tcp, tcp host ports: tcp, tcp state: running started: sun, apr ready: true restart count: readiness: tcp socket environment: os_identity_admin_passwd: password ipaddr: mounts: from token ro conditions: type status initialized true ready true containersready true podscheduled true volumes: token type: secret a volume populated by a secret secretname: token optional: false qos class: besteffort node selectors: none tolerations: node.kubernetes.io not ready:noexecute for node.kubernetes.io unreachable:noexecute for events: none kubectl log podname logs: post http keystone.common.wsgi get keystone.common.wsgi could not find service: heat.: servicenotfound: could not find service: heat. get http keystone.common.wsgi get get http keystone.common.wsgi post post http get http keystone.common.wsgi post post http keystone.common.wsgi post post http keystone.common.wsgi get keystone.common.wsgi could not find service: heat.: servicenotfound: could not find service: heat. get http keystone.common.wsgi get get http keystone.common.wsgi post post http"
    },
    {
        "url": "https://stackoverflow.com/questions/61652081",
        "text": "keystone.server.flask.application authorization failed. the request you have made requires authentication. from unauthorized: the request you have made requires authentication. keystone.common.rbac_enforcer.enforcer deprecated policy rules found. use oslopolicy policy generator and oslopolicy policy upgrade to detect and resolve deprecated policies in your configuration."
    },
    {
        "url": "https://stackoverflow.com/questions/61761669",
        "text": "e main org.gaul.s3proxy.main:238 exception in thread e main org.gaul.s3proxy.main:238 org.jclouds.rest.authorizationexception: request: post http sensitive data in payload, use jclouds.wire.log.sensitive override to enable logging this data. failed with response: http unauthorized e main org.gaul.s3proxy.main:238 at org.jclouds.openstack.swift.v1.handlers.swifterrorhandler.handleerror swifterrorhandler.java:51 e main org.gaul.s3proxy.main:238 at org.jclouds.http.handlers.delegatingerrorhandler.handleerror delegatingerrorhandler.java:65 e main org.gaul.s3proxy.main:238 at org.jclouds.http.internal.basehttpcommandexecutorservice.shouldcontinue basehttpcommandexecutorservice.java:138 e main org.gaul.s3proxy.main:238 at org.jclouds.http.internal.basehttpcommandexecutorservice.invoke basehttpcommandexecutorservice.java:107 e main org.gaul.s3proxy.main:238 at org.jclouds.rest.internal.invokehttpmethod.invoke invokehttpmethod.java:91 e main org.gaul.s3proxy.main:238 at org.jclouds.rest.internal.invokehttpmethod.apply invokehttpmethod.java:74 e main org.gaul.s3proxy.main:238 at org.jclouds.rest.internal.invokehttpmethod.apply invokehttpmethod.java:45 e main org.gaul.s3proxy.main:238 at org.jclouds.rest.internal.delegatestoinvocationfunction.handle delegatestoinvocationfunction.java:156 e main org.gaul.s3proxy.main:238 at org.jclouds.rest.internal.delegatestoinvocationfunction.invoke delegatestoinvocationfunction.java:123"
    },
    {
        "url": "https://stackoverflow.com/questions/62179322",
        "text": "debug config changed net.netfilter.nf_conntrack_max debug config changed net.ipv4.neigh.default.gc_thresh2 debug config changed net.ipv6.neigh.default.gc_thresh1 debug config changed net.nf_conntrack_max debug config changed sysctl: setting key debug config changed net.ipv4.neigh.default.gc_thresh3 debug config changed net.ipv4.neigh.default.gc_thresh1 debug config changed net.ipv6.neigh.default.gc_thresh2 debug config changed net.ipv6.neigh.default.gc_thresh3 debug config changed active juju log creating bridge br int juju log creating bridge br ex juju log support for use of upstream module in conjunctionwith charm helpers is deprecated since juju log creating bridge br data debug juju log interface is not a linux bridge juju log adding port to bridge br data debug config changed failed to restart os charm phy nic mtu.service: unit os charm phy nic mtu.service not found."
    },
    {
        "url": "https://stackoverflow.com/questions/62179322",
        "text": "juju.api monitor.go:59 health ping timed out after juju.worker.dependency engine.go:551 manifold worker returned unexpected error: api connection broken unexpectedly juju log loaded template from templates juju log rendering from template: juju log wrote template debug juju log generating template context for amqp debug config changed traceback most recent call last : debug config changed file line in module debug config changed main debug config changed file line in main debug config changed hooks.execute sys.argv debug config changed file line in execute debug config changed self._hooks hook_name debug config changed file line in wrapped_f debug config changed stopstart, restart_functions debug config changed file line in restart_on_change_helper debug config changed r lambda_f debug config changed file line in lambda debug config changed lambda: f , __restart_map_cache debug config changed file line in config_changed debug config changed configs.write_all debug config changed file line in write_all debug config changed self.write k for k in six.iterkeys self.templates debug config changed file line in listcomp debug config changed self.write k for k in six.iterkeys self.templates debug config changed file line in write debug config changed _out self.render config_file debug config changed file line in render debug config changed ctxt ostmpl.context debug config changed file line in context debug config changed _ctxt context debug config changed file line in __call__ debug config changed host_ip get_relation_ip debug config changed file line in get_relation_ip debug config changed address network_get_primary_address interface debug config changed file line in inner_translate_exc2 debug config changed return f debug config changed file line in network_get_primary_address debug config changed .decode debug config changed file line in check_output debug config changed .stdout debug config changed file line in run debug config changed debug config changed subprocess.calledprocesserror: command returned non zero exit status juju.worker.uniter.operation runhook.go:113 hook failed: exit status juju log registered config file: juju log registered config file:"
    },
    {
        "url": "https://stackoverflow.com/questions/62251808",
        "text": "tail sun jun mod_ssl does not seem to be enabled sun jun mpm_event:notice apache centos mod_wsgi 4.6.4 python configured resuming normal operations sun jun core:notice command line: d foreground sun jun wsgi:error warning:root:use of this file has been deprecated since the rocky release in favor of in the module. this file is a legacy naming from before django and an importable is now the default. this file will be removed in the t release cycle. sun jun wsgi:error deprecationwarning: inspect.getargspec is deprecated since python use inspect.signature or inspect.getfullargspec sun jun wsgi:error argspec inspect.getargspec function sun jun authz_core:error client client denied by server configuration: sun jun wsgi:error remote openstack_auth.forms login failed for user using domain remote address sun jun authz_core:error client client denied by server configuration: sun jun wsgi:error remote openstack_auth.forms login failed for user using domain remote address"
    },
    {
        "url": "https://stackoverflow.com/questions/62680866",
        "text": "oslo_service.service starting thread. oslo_service.service traceback most recent call last : oslo_service.service file line in run_service oslo_service.service service.start oslo_service.service file line in start oslo_service.service self.rpcserver.start oslo_service.service file line in wrapper oslo_service.service log_after, timeout_timer oslo_service.service file line in run_once oslo_service.service post_fn fn oslo_service.service file line in lambda oslo_service.service states state .run_once lambda: fn self, , oslo_service.service file line in start oslo_service.service self.listener self._create_listener oslo_service.service file line in _create_listener oslo_service.service return self.transport._listen self._target, none oslo_service.service file line in _listen oslo_service.service target oslo_service.service invalidtarget: a server s target must have topic and server names specified: target oslo_service.service traceback most recent call last : file line in switch self.greenlet.switch value file line in main result function file line in run_service raise systemexit systemexit:"
    },
    {
        "url": "https://stackoverflow.com/questions/63650069",
        "text": "nova.conductor.manager nova.conductor.manager nova.scheduler.utils failed to compute_task_build_instances: no valid host was found. traceback most recent call last : file line in inner return func file line in select_destinations raise exception.novalidhost reason= nova.exception.novalidhost: no valid host was found. : nova.exception_remote.novalidhost_remote: no valid host was found. nova.scheduler.utils instance: setting instance to state.: nova.exception_remote.novalidhost_remote: no valid host was found."
    },
    {
        "url": "https://stackoverflow.com/questions/63737573",
        "text": "oslo_messaging.rpc.server exception during message handling: oslo_messaging.rpc.dispatcher.nosuchmethod: endpoint does not support rpc method pod_health_probe_method_ignore_errors oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in _process_incoming oslo_messaging.rpc.server res self.dispatcher.dispatch message oslo_messaging.rpc.server file line in dispatch oslo_messaging.rpc.server raise nosuchmethod method oslo_messaging.rpc.server oslo_messaging.rpc.dispatcher.nosuchmethod: endpoint does not support rpc method pod_health_probe_method_ignore_errors"
    },
    {
        "url": "https://stackoverflow.com/questions/63766864",
        "text": "nova.compute.manager instance: attaching volume eddc to os_brick.initiator.connectors.iscsi trying to connect to iscsi portal os_brick.initiator.connectors.iscsi couldn t find iscsi nodes because iscsiadm err: iscsiadm: no records found: oslo_concurrency.processutils.processexecutionerror: unexpected while running command. os_brick.initiator.connectors.iscsi iscsiadm stderr output when getting sessions: iscsiadm: no active sessions.: oslo_concurrency.processutils.processexecutionerror: unexpected while running command. nova.virt.block_device instance: driver failed to attach volume eddc at oslo_concurrency.processutils.processexecutionerror: unexpected while running command. command: iscsiadm m node t iqn.2010 eddc 4d39 b47d p interface op new exit code: stdout: stderr: iscsiadm: could not open permission while adding record: encountered iscsi database nova.virt.block_device instance: traceback most recent call last : nova.virt.block_device instance: file line in _volume_attach nova.virt.block_device instance: device_type=self nova.virt.block_device instance: file line in attach_volume nova.virt.block_device instance: nova.virt.block_device instance: file line in _connect_volume nova.virt.block_device instance: vol_driver.connect_volume connection_info, instance nova.virt.block_device instance: file line in connect_volume nova.virt.block_device instance: device_info self.connector.connect_volume connection_info nova.virt.block_device instance: file line in trace_logging_wrapper nova.virt.block_device instance: return f nova.virt.block_device instance: file line in inner nova.virt.block_device instance: return f nova.virt.block_device instance: file line in connect_volume nova.virt.block_device instance: self._cleanup_connection connection_properties, nova.virt.block_device instance: file line in __exit__ nova.virt.block_device instance: self.force_reraise nova.virt.block_device instance: file line in force_reraise nova.virt.block_device instance: six.reraise self.type_, self.value, self.tb nova.virt.block_device instance: file line in reraise nova.virt.block_device instance: raise value nova.virt.block_device instance: file line in connect_volume nova.virt.block_device instance: return self._connect_single_volume connection_properties nova.virt.block_device instance: file line in _wrapper nova.virt.block_device instance: return r.call f, nova.virt.block_device instance: file line in call nova.virt.block_device instance: return attempt.get self._wrap_exception nova.virt.block_device instance: file line in get nova.virt.block_device instance: six.reraise self.value 0 , self.value 1 self.value 2 nova.virt.block_device instance: file line in reraise nova.virt.block_device instance: raise value nova.virt.block_device instance: file line in call nova.virt.block_device instance: attempt attempt fn , attempt_number, false nova.virt.block_device instance: file line in _connect_single_volume nova.virt.block_device instance: self._connect_vol self.device_scan_attempts, props, data nova.virt.block_device instance: file line in _connect_vol nova.virt.block_device instance: session, manual_scan self._connect_to_iscsi_portal props nova.virt.block_device instance: file line in _connect_to_iscsi_portal nova.virt.block_device instance: nova.virt.block_device instance: file line in _run_iscsiadm nova.virt.block_device instance: delay_on_retry=delay_on_retry nova.virt.block_device instance: file line in _execute nova.virt.block_device instance: result self.__execute *args, nova.virt.block_device instance: file line in execute nova.virt.block_device instance: return execute_root *cmd, nova.virt.block_device instance: file line in _wrap nova.virt.block_device instance: return self.channel.remote_call name, args, kwargs nova.virt.block_device instance: file line in remote_call nova.virt.block_device instance: raise exc_type *result nova.virt.block_device instance: oslo_concurrency.processutils.processexecutionerror: unexpected while running command. nova.virt.block_device instance: command: iscsiadm m node t iqn.2010 eddc 4d39 b47d p interface op new nova.virt.block_device instance: exit code: nova.virt.block_device instance: stdout: nova.virt.block_device instance: stderr: iscsiadm: could not open permission while adding record: encountered iscsi database"
    },
    {
        "url": "https://stackoverflow.com/questions/64187294",
        "text": "oct controller octavia server container octavia housekeeping : octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connectionerror: httpsconnectionpool : max retries exceeded with url: caused by newconnectionerror object at : failed to establish a new connection: errno no route to host oct controller octavia server container octavia housekeeping : octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connectionerror: httpsconnectionpool : max retries exceeded with url: caused by newconnectionerror object at : failed to establish a new connection: errno no route to host"
    },
    {
        "url": "https://stackoverflow.com/questions/64386419",
        "text": "ironic_inspector.node_cache introspection for nodes u u has timed out debug ironic_inspector.node_cache node: state waiting attempting to acquire lock acquire_lock debug ironic_inspector.node_cache node: state waiting successfully acquired lock acquire_lock debug ironic_inspector.node_cache node: state waiting successfully released lock release_lock debug ironic_inspector.node_cache node: state waiting executing fsm waiting .process_event timeout fsm_event ironic_inspector.node_cache node: state waiting updating node state: waiting debug ironic_inspector.node_cache node: state waiting committing fields: _commit debug ironic_inspector.node_cache node: state committing fields: datetime.datetime 2020, , introspection timeout _commit debug ironic_inspector.node_cache node: state waiting attempting to acquire lock acquire_lock debug ironic_inspector.node_cache node: state waiting successfully acquired lock acquire_lock debug ironic_inspector.node_cache node: state waiting successfully released lock release_lock debug ironic_inspector.node_cache node: state waiting executing fsm waiting .process_event timeout fsm_event ironic_inspector.node_cache node: state waiting updating node state: waiting debug ironic_inspector.node_cache node: state waiting committing fields: _commit debug ironic_inspector.node_cache node: state committing fields: datetime.datetime 2020, , introspection timeout _commit"
    },
    {
        "url": "https://stackoverflow.com/questions/64460175",
        "text": "oct ha_proxy haproxy : alert : starting frontend haproxynode: cannot bind socket oct ha_proxy haproxy : proxy backendnodes started. oct ha_proxy haproxy : proxy backendnodes started. oct ha_proxy haproxy : proxy stats started. oct ha_proxy haproxy : proxy stats started. oct ha_proxy haproxy : proxy haproxynode started. oct ha_proxy haproxy : proxy haproxynode started. oct ha_proxy haproxy : proxy backendnodes started. oct ha_proxy haproxy : proxy backendnodes started. oct ha_proxy haproxy : proxy stats started. oct ha_proxy haproxy : server backendnodes is down, reason: connection problem, info: connection refused , check duration: active and backup servers left. sessions"
    },
    {
        "url": "https://stackoverflow.com/questions/64470323",
        "text": "debug nova.virt.libvirt.driver instance: about to invoke the migrate api _live_migration_operation nova.virt.libvirt.driver instance: live migration failure: not all arguments converted during string formatting debug nova.virt.libvirt.driver instance: migration operation thread notification thread_finished debug nova.virt.libvirt.migration instance: vm running on src, migration failed _log debug nova.virt.libvirt.driver instance: fixed incorrect job type to be _live_migration_monitor nova.virt.libvirt.driver instance: migration operation has aborted"
    },
    {
        "url": "https://stackoverflow.com/questions/64711382",
        "text": "debug ceilometer.publisher.gnocchi resource u u u u publish_samples"
    },
    {
        "url": "https://stackoverflow.com/questions/64711382",
        "text": "debug ceilometer.publisher.gnocchi resource u u u publish_samples"
    },
    {
        "url": "https://stackoverflow.com/questions/64711382",
        "text": "debug ceilometer.publisher.gnocchi resource u u u u publish_samples"
    },
    {
        "url": "https://stackoverflow.com/questions/66071988",
        "text": "neutron.common.config logging enabled! neutron.common.config version critical neutron unhandled error: valueerror: no closing quotation neutron traceback most recent call last : neutron file line in module neutron sys.exit main neutron file line in main neutron dhcp_agent.main neutron file line in main neutron config.setup_privsep neutron file line in setup_privsep neutron priv_context.init root_helper=shlex.split get_root_helper cfg.conf neutron file line in split neutron return list lex neutron file line in __next__ neutron token self.get_token neutron file line in get_token neutron raw self.read_token neutron file line in read_token neutron raise valueerror no closing quotation neutron valueerror: no closing quotation neutron"
    },
    {
        "url": "https://stackoverflow.com/questions/66486612",
        "text": "nova.scheduler.utils instance: from last host: kolla ceph node kolla ceph : traceback most recent call last , file line in _build_and_run_instance\\n block_device_info=block_device_info \\n file line in , file line in _get_guest_xml\\n xml conf.to_xml \\n , file line in to_xml\\n root self.format_dom \\n , file line in format_dom\\n self._format_devices root \\n , file line in _format_devices\\n devices.append dev.format_dom \\n , file line in format_dom\\n auth.set self.auth_username \\n , file line in lxml.etree._element.set\\n , file line in lxml.etree._setattributevalue\\n , file line in lxml.etree._utf8\\n , typeerror: argument must be bytes or unicode, got handling of the above exception, another exception occurred:\\n\\n traceback most recent call last , file line in _do_build_and_run_instance\\n filter_properties, request_spec \\n file line in _build_and_run_instance\\n instance_uuid=instance.uuid, e \\n , nova.exception.rescheduledexception: build of instance was re scheduled: argument must be bytes or unicode, got nova.scheduler.utils instance: from last host: kolla ceph node kolla ceph : traceback most recent call last , file line in _build_and_run_instance\\n block_device_info=block_device_info \\n file line in , file line in _get_guest_xml\\n xml conf.to_xml \\n , file line in to_xml\\n root self.format_dom \\n , file line in format_dom\\n self._format_devices root \\n , file line in _format_devices\\n devices.append dev.format_dom \\n , file line in format_dom\\n auth.set self.auth_username \\n , file line in lxml.etree._element.set\\n , file line in lxml.etree._setattributevalue\\n , file line in lxml.etree._utf8\\n , typeerror: argument must be bytes or unicode, got handling of the above exception, another exception occurred:\\n\\n traceback most recent call last , file line in _do_build_and_run_instance\\n filter_properties, request_spec \\n file line in _build_and_run_instance\\n instance_uuid=instance.uuid, e \\n , nova.exception.rescheduledexception: build of instance was re scheduled: argument must be bytes or unicode, got cat egrep i"
    },
    {
        "url": "https://stackoverflow.com/questions/66486612",
        "text": "magnum.drivers.heat.k8s_fedora_template_def failed to load keystone auth policy: filenotfounderror: errno no such file or directory: magnum.drivers.heat.k8s_fedora_template_def debd failed to load keystone auth policy: filenotfounderror: errno no such file or directory: magnum.drivers.heat.k8s_fedora_template_def failed to load keystone auth policy: filenotfounderror: errno no such file or directory: magnum.drivers.heat.driver nodegroup error, stack status: create_failed, stack_id: reason: resource create failed: resourceinerror: resources.kube_masters.resources 0 .resources.kube master: went to status due to message: exceeded maximum number of retries. exceeded max scheduling attempts for instance last exception: argument must be bytes or unicode, got code: magnum.drivers.heat.driver nodegroup error, stack status: create_failed, stack_id: reason: resource create failed: resourceinerror: resources.kube_masters.resources 0 .resources.kube master: went to status due to message: exceeded maximum number of retries. exceeded max scheduling attempts for instance last exception: argument must be bytes or unicode, got code:"
    },
    {
        "url": "https://stackoverflow.com/questions/66486612",
        "text": "heat.engine.resource traceback most recent call last : heat.engine.resource file line in _action_recorder heat.engine.resource yield heat.engine.resource file line in _do_action heat.engine.resource yield self.action_handler_task action, heat.engine.resource file line in wrapper heat.engine.resource step next subtask heat.engine.resource file line in action_handler_task heat.engine.resource done check handler_data heat.engine.resource file line in check_create_complete heat.engine.resource return self._check_status_complete self.create heat.engine.resource file line in _check_status_complete heat.engine.resource heat.engine.resource heat.common.exception.resourcefailure: resourceinerror: resources .resources.kube master: went to status due to message: exceeded maximum number of retries. exceeded max scheduling attempts for instance last exception: argument must be bytes or unicode, got code: heat.engine.resource heat.engine.stack admin stack create failed test cluster kube_masters 5w5xewm2u4c3 resource create failed: resourceinerror: resources .resources.kube master: went to status due to message: exceeded maximum number of retries. exceeded max scheduling attempts for instance last exception: argument must be bytes or unicode, got code:"
    },
    {
        "url": "https://stackoverflow.com/questions/66819800",
        "text": "oslo_messaging.rpc.server cinder.exception.imagecopyfailure: failed to copy image to volume: qemu img: while writing at byte no space left on device"
    },
    {
        "url": "https://stackoverflow.com/questions/66819800",
        "text": "cinder.volume.volume_utils beed failed to copy image to volume: bdcc oslo_concurrency.processutils.processexecutionerror: unexpected while running command. command: sudo cinder rootwrap qemu img convert o raw t none f exit code: stdout: stderr: qemu img: while writing at byte no space left on cinder.volume.volume_utils traceback most recent call last : cinder.volume.volume_utils file line in copy_image_to_volume cinder.volume.volume_utils context, volume, image_service, image_id cinder.volume.volume_utils file line in copy_image_to_volume cinder.volume.volume_utils cinder.volume.volume_utils file line in fetch_to_raw cinder.volume.volume_utils run_as_root=run_as_root cinder.volume.volume_utils file line in fetch_to_volume_format cinder.volume.volume_utils run_as_root=run_as_root cinder.volume.volume_utils file line in convert_image cinder.volume.volume_utils src_passphrase_file=src_passphrase_file cinder.volume.volume_utils file line in _convert_image cinder.volume.volume_utils utils.execute *cmd, run_as_root=run_as_root cinder.volume.volume_utils file line in execute cinder.volume.volume_utils return processutils.execute *cmd, cinder.volume.volume_utils file line in execute cinder.volume.volume_utils cinder.volume.volume_utils oslo_concurrency.processutils.processexecutionerror: unexpected while running command. cinder.volume.volume_utils command: sudo cinder rootwrap qemu img convert o raw t none f cinder.volume.volume_utils exit code: cinder.volume.volume_utils stdout: cinder.volume.volume_utils stderr: qemu img: while writing at byte no space left on cinder.volume.volume_utils cinder.volume.manager beed task transitioned into state from state cinder.volume.manager traceback most recent call last : cinder.volume.manager file line in copy_image_to_volume cinder.volume.manager context, volume, image_service, image_id cinder.volume.manager file line in copy_image_to_volume cinder.volume.manager cinder.volume.manager file line in fetch_to_raw cinder.volume.manager run_as_root=run_as_root cinder.volume.manager file line in fetch_to_volume_format cinder.volume.manager run_as_root=run_as_root cinder.volume.manager file line in convert_image cinder.volume.manager src_passphrase_file=src_passphrase_file cinder.volume.manager file line in _convert_image cinder.volume.manager utils.execute *cmd, run_as_root=run_as_root cinder.volume.manager file line in execute cinder.volume.manager return processutils.execute *cmd, cinder.volume.manager file line in execute cinder.volume.manager cinder.volume.manager oslo_concurrency.processutils.processexecutionerror: unexpected while running command. cinder.volume.manager command: sudo cinder rootwrap qemu img convert o raw t none f cinder.volume.manager exit code: cinder.volume.manager stdout: cinder.volume.manager stderr: qemu img: while writing at byte no space left on cinder.volume.manager cinder.volume.manager during handling of the above exception, another exception occurred: cinder.volume.manager cinder.volume.manager traceback most recent call last : cinder.volume.manager file line in _execute_task cinder.volume.manager result task.execute **arguments cinder.volume.manager file line in execute cinder.volume.manager cinder.volume.manager file line in _wrapper cinder.volume.manager return r.call f, cinder.volume.manager file line in call cinder.volume.manager do self.iter retry_state=retry_state cinder.volume.manager file line in iter cinder.volume.manager return fut.result cinder.volume.manager file line in result cinder.volume.manager return self.__get_result cinder.volume.manager file line in __get_result cinder.volume.manager raise self._exception cinder.volume.manager file line in call cinder.volume.manager result fn cinder.volume.manager file line in _create_from_image cinder.volume.manager image_service cinder.volume.manager file line in _create_from_image_cache_or_download cinder.volume.manager image_service cinder.volume.manager file line in _create_from_image_download cinder.volume.manager image_service cinder.volume.manager file line in copy_image_to_volume cinder.volume.manager raise exception.imagecopyfailure reason=ex.stderr cinder.volume.manager cinder.exception.imagecopyfailure: failed to copy image to volume: qemu img: while writing at byte no space left on device cinder.volume.manager cinder.volume.manager cinder.volume.manager beed task transitioned into state from state cinder.volume.manager beed task cinder.volume.flows.manager.create_volume.notifyvolumeactiontask;volume:create, create.start transitioned into state from state cinder.volume.manager beed task transitioned into state from state cinder.volume.flows.manager.create_volume beed volume bdcc create failed cinder.volume.manager beed task transitioned into state from state cinder.volume.manager beed task transitioned into state from state cinder.volume.manager beed flow transitioned into state from state oslo_messaging.rpc.server beed exception during message handling: cinder.exception.imagecopyfailure: failed to copy image to volume: qemu img: while writing at byte no space left on device oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in copy_image_to_volume oslo_messaging.rpc.server context, volume, image_service, image_id oslo_messaging.rpc.server file line in copy_image_to_volume oslo_messaging.rpc.server oslo_messaging.rpc.server file line in fetch_to_raw oslo_messaging.rpc.server run_as_root=run_as_root oslo_messaging.rpc.server file line in fetch_to_volume_format oslo_messaging.rpc.server run_as_root=run_as_root oslo_messaging.rpc.server file line in convert_image oslo_messaging.rpc.server src_passphrase_file=src_passphrase_file oslo_messaging.rpc.server file line in _convert_image oslo_messaging.rpc.server utils.execute *cmd, run_as_root=run_as_root oslo_messaging.rpc.server file line in execute oslo_messaging.rpc.server return processutils.execute *cmd, oslo_messaging.rpc.server file line in execute oslo_messaging.rpc.server oslo_messaging.rpc.server oslo_concurrency.processutils.processexecutionerror: unexpected while running command. oslo_messaging.rpc.server command: sudo cinder rootwrap qemu img convert o raw t none f oslo_messaging.rpc.server exit code: oslo_messaging.rpc.server stdout: oslo_messaging.rpc.server stderr: qemu img: while writing at byte no space left on oslo_messaging.rpc.server oslo_messaging.rpc.server during handling of the above exception, another exception occurred: oslo_messaging.rpc.server oslo_messaging.rpc.server traceback most recent call last : oslo_messaging.rpc.server file line in _process_incoming oslo_messaging.rpc.server res self.dispatcher.dispatch message oslo_messaging.rpc.server file line in dispatch oslo_messaging.rpc.server return self._do_dispatch endpoint, method, ctxt, args oslo_messaging.rpc.server file line in _do_dispatch oslo_messaging.rpc.server result func ctxt, oslo_messaging.rpc.server file , line in create_volume oslo_messaging.rpc.server file line in wrapper oslo_messaging.rpc.server result f oslo_messaging.rpc.server file line in create_volume oslo_messaging.rpc.server _run_flow oslo_messaging.rpc.server file line in _run_flow oslo_messaging.rpc.server flow_engine.run oslo_messaging.rpc.server file line in run oslo_messaging.rpc.server for _state in self.run_iter timeout=timeout : oslo_messaging.rpc.server file line in run_iter oslo_messaging.rpc.server failure.failure.reraise_if_any er_failures oslo_messaging.rpc.server file line in reraise_if_any oslo_messaging.rpc.server failures .reraise oslo_messaging.rpc.server file line in reraise oslo_messaging.rpc.server six.reraise *self._exc_info oslo_messaging.rpc.server file line in reraise oslo_messaging.rpc.server raise value oslo_messaging.rpc.server file line in _execute_task oslo_messaging.rpc.server result task.execute **arguments oslo_messaging.rpc.server file line in execute oslo_messaging.rpc.server oslo_messaging.rpc.server file line in _wrapper oslo_messaging.rpc.server return r.call f, oslo_messaging.rpc.server file line in call oslo_messaging.rpc.server do self.iter retry_state=retry_state oslo_messaging.rpc.server file line in iter oslo_messaging.rpc.server return fut.result oslo_messaging.rpc.server file line in result oslo_messaging.rpc.server return self.__get_result oslo_messaging.rpc.server file line in __get_result oslo_messaging.rpc.server raise self._exception oslo_messaging.rpc.server file line in call oslo_messaging.rpc.server result fn oslo_messaging.rpc.server file line in _create_from_image oslo_messaging.rpc.server image_service oslo_messaging.rpc.server file line in _create_from_image_cache_or_download oslo_messaging.rpc.server image_service oslo_messaging.rpc.server file line in _create_from_image_download oslo_messaging.rpc.server image_service oslo_messaging.rpc.server file line in copy_image_to_volume oslo_messaging.rpc.server raise exception.imagecopyfailure reason=ex.stderr oslo_messaging.rpc.server cinder.exception.imagecopyfailure: failed to copy image to volume: qemu img: while writing at byte no space left on device oslo_messaging.rpc.server oslo_messaging.rpc.server"
    },
    {
        "url": "https://stackoverflow.com/questions/66868330",
        "text": "i python : error: none res: b i python : client initialized.. i python : client created.. i python : base unknown android provider i python : base start application main loop i python : base leaving application in progress... i python : traceback most recent call last : i python : file line in module i python : file line in run i python : file line in runtouchapp i python : file line in mainloop i python : file line in idle i python : file line in dispatch_input i python : file line in post_dispatch_input i python : file line in kivy._event.eventdispatcher.dispatch i python : file line in on_touch_up i python : file line in kivy._event.eventdispatcher.dispatch i python : file line in kivy._event.eventobservers.dispatch i python : file line in kivy._event.eventobservers._dispatch i python : file line in custom_callback i python : file line in module i python : on_release: root.save_to_database i python : file line in save_to_database i python : file line in push_data i python : file line in push_api i python : file line in put_object i python : file line in _retry i python : file line in get_auth i python : file line in get_auth i python : file line in get_auth_keystone i python : swiftclient.exceptions.clientexception: i python : auth versions and require python keystoneclient, install it or use auth i python : version which requires st_auth, st_user, and st_key environment i python : variables to be set or overridden with a, u, or k. i python : python for android ended."
    },
    {
        "url": "https://stackoverflow.com/questions/66888926",
        "text": "property error: resources.server.properties.key_name: validating value the key could not be found. : create_failed resource create failed: resources : resources.single.resources.heal_group.property error: resources.server.properties.key_name: validating value the key could not be found."
    },
    {
        "url": "https://stackoverflow.com/questions/66953569",
        "text": "critical oslo.privsep.daemon privsep helper command exited non zero neutron.agent.dhcp.agent unable to enable dhcp for oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron.agent.dhcp.agent traceback most recent call last : neutron.agent.dhcp.agent file line in call_driver neutron.agent.dhcp.agent getattr driver, action neutron.agent.dhcp.agent file line in enable neutron.agent.dhcp.agent common_utils.wait_until_true self._enable, neutron.agent.dhcp.agent file line in wait_until_true neutron.agent.dhcp.agent while not predicate : neutron.agent.dhcp.agent file line in _enable neutron.agent.dhcp.agent interface_name self.device_manager.setup self.network neutron.agent.dhcp.agent file line in setup neutron.agent.dhcp.agent ip_lib.ipwrapper .ensure_namespace network.namespace neutron.agent.dhcp.agent file line in ensure_namespace neutron.agent.dhcp.agent if not self.netns.exists name : neutron.agent.dhcp.agent file line in exists neutron.agent.dhcp.agent return network_namespace_exists name neutron.agent.dhcp.agent file line in network_namespace_exists neutron.agent.dhcp.agent output list_network_namespaces **kwargs neutron.agent.dhcp.agent file line in list_network_namespaces neutron.agent.dhcp.agent return privileged.list_netns **kwargs neutron.agent.dhcp.agent file line in _wrap neutron.agent.dhcp.agent self.start neutron.agent.dhcp.agent file line in start neutron.agent.dhcp.agent channel daemon.rootwrapclientchannel context=self neutron.agent.dhcp.agent file line in __init__ neutron.agent.dhcp.agent raise failedtodropprivileges msg neutron.agent.dhcp.agent oslo_privsep.daemon.failedtodropprivileges: privsep helper command exited non zero neutron.agent.dhcp.agent neutron.agent.dhcp.agent finished network dhcp configuration"
    },
    {
        "url": "https://stackoverflow.com/questions/67175628",
        "text": "compute cat boot_image= 5.4.0 ro maybe ubiquity intel_iommu=on default_hugepagesz=1g transparent_hugepage=never lspci v grep mel ethernet controller: mellanox technologies family connectx pro ethernet controller: mellanox technologies family connectx pro ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function ethernet controller: mellanox technologies family connectx connectx pro virtual function cat grep v sed my_ip state_path enabled_apis osapi_compute,metadata log_dir transport_url rabbit: use_neutron true linuxnet_interface_driver nova.network.linux_net.linuxbridgeinterfacedriver firewall_driver nova.virt.firewall.noopfirewalldriver vif_plugging_is_fatal true vif_plugging_timeout cpu_allocation_ratio reserved_huge_pages node:0,size:1gb,count:1 reserved_huge_pages node:1,size:1gb,count:1 vcpu_pin_set filter_scheduler available_filters=nova.scheduler.filters.all_filters enabled_filters=availabilityzonefilter,computefilter,computecapabilitiesfilter,imagepropertiesfilter,servergroupantiaffinityfilter,servergroupaffinityfilter,pcipassthroughfilter,numatopologyfilter,aggregateinstanceextraspecsfilter api auth_strategy keystone vnc enabled true server_listen server_proxyclient_address novncproxy_base_url glance api_servers oslo_concurrency lock_path tmp keystone_authtoken www_authenticate_uri auth_url memcached_servers auth_type password project_domain_name user_domain_name project_name service username password placement auth_url os_region_name regionone auth_type password project_domain_name user_domain_name project_name service username password wsgi api_paste_config neutron auth_url auth_type password project_domain_name user_domain_name region_name regionone project_name service username password service_metadata_proxy true metadata_proxy_shared_secret cinder os_region_name regionone pci passthrough_whitelist cat grep v sed path_mtu physical_network_mtus type_drivers flat,vlan,vxlan tenant_network_types vxlan mechanism_drivers linuxbridge extension_drivers port_security path_mtu segment_mtu physical_network_mtus flat_networks network_vlan_ranges vni_ranges ovs_driver securitygroup sriov_driver cat grep v sed agent sriov_nic physical_device_mappings exclude_devices securitygroup firewall_driver neutron.agent.firewall.noopfirewalldriver cat grep v sed core_plugin service_plugins router auth_strategy keystone state_path allow_overlapping_ips true transport_url rabbit: global_physnet_mtu agent root_helper sudo keystone_authtoken www_authenticate_uri auth_url memcached_servers auth_type password project_domain_name user_domain_name project_name service username password oslo_concurrency lock_path lock controller openstack network agent list host id agent type host availability zone alive state binary metadata agent none : up neutron metadata agent linux bridge agent none : up neutron linuxbridge agent ecfe nic switch agent none : up neutron sriov nic agent dhcp agent nova : up neutron dhcp agent openstack network agent list host id agent type host availability zone alive state binary linux bridge agent none : up neutron linuxbridge agent agent nova : up neutron agent cat grep v sed core_plugin service_plugins router auth_strategy keystone state_path dhcp_agent_notification true allow_overlapping_ips true notify_nova_on_port_status_changes true notify_nova_on_port_data_changes true transport_url rabbit: global_physnet_mtu agent root_helper sudo keystone_authtoken www_authenticate_uri auth_url memcached_servers auth_type password project_domain_name user_domain_name project_name service username password database connection neutron: nova auth_url auth_type password project_domain_name user_domain_name region_name regionone project_name service username nova password oslo_concurrency lock_path tmp cat grep v sed path_mtu physical_network_mtus type_drivers flat,vlan,vxlan tenant_network_types vxlan mechanism_drivers linuxbridge,sriovnicswitch extension_drivers port_security path_mtu segment_mtu physical_network_mtus flat_networks network_vlan_ranges vni_ranges ovs_driver securitygroup sriov_driver cat grep v sed my_ip state_path enabled_apis osapi_compute,metadata log_dir transport_url rabbit: use_neutron true linuxnet_interface_driver nova.network.linux_net.linuxbridgeinterfacedriver firewall_driver nova.virt.firewall.noopfirewalldriver vif_plugging_is_fatal true vif_plugging_timeout filter_scheduler available_filters=nova.scheduler.filters.all_filters enabled_filters=availabilityzonefilter,computefilter,computecapabilitiesfilter,imagepropertiesfilter,servergroupaffinityfilter,pcipassthroughfilter,numatopologyfilter,aggregateinstanceextraspecsfilter api auth_strategy keystone glance api_servers oslo_concurrency lock_path tmp api_database connection database connection keystone_authtoken www_authenticate_uri auth_url memcached_servers auth_type password project_domain_name user_domain_name project_name service username password placement auth_url os_region_name regionone auth_type password project_domain_name user_domain_name project_name service username password wsgi api_paste_config neutron auth_url auth_type password project_domain_name user_domain_name region_name regionone project_name service username password service_metadata_proxy true metadata_proxy_shared_secret openstack network show field value admin_state_up up availability_zone_hints availability_zones nova created_at t description dns_domain none id none none is_default none is_vlan_transparent none mtu name sriov net port_security_enabled true project_id provider:network_type vlan provider:physical_network provider:segmentation_id qos_policy_id none revision_number router:external internal segments none shared false status active subnets tags updated_at t nova scheduler.log debug nova.scheduler.filters.pci_passthrough_filter magma magma ram: disk: io_ops: instances: doesn t have the required pci devices instancepcirequests instance_uuid= ? instancepcirequest host_passes"
    },
    {
        "url": "https://stackoverflow.com/questions/67406980",
        "text": "octavia.controller.queue.v1.consumer starting consumer... octavia.controller.queue.v2.consumer starting consumer... octavia.controller.queue.v1.endpoints creating load balancer octavia.network.drivers.neutron.allowed_address_pairs port already exists. nothing to be done. octavia.controller.worker.v1.tasks.database_tasks created amphora in db with id octavia.certificates.generator.local signing a certificate request using openssl locally. octavia.certificates.generator.local using ca certificate from config. octavia.certificates.generator.local using ca private key from config. octavia.certificates.generator.local using ca private key passphrase from config. octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connecttimeout: httpsconnectionpool : max retries exceeded with url: caused by connecttimeouterror object at , connection to timed out. connect octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connecttimeout: httpsconnectionpool : max retries exceeded with url: caused by connecttimeouterror object at , connection to timed out. connect octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connecttimeout: httpsconnectionpool : max retries exceeded with url: caused by connecttimeouterror object at , connection to timed out. connect octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connecttimeout: httpsconnectionpool : max retries exceeded with url: caused by connecttimeouterror object at , connection to timed out. connect octavia.amphorae.drivers.haproxy.rest_api_driver could not connect to instance. retrying.: requests.exceptions.connecttimeout: httpsconnectionpool : max retries exceeded with url: caused by connecttimeouterror object at , connection to timed out. connect"
    },
    {
        "url": "https://stackoverflow.com/questions/67426087",
        "text": "sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart could not reliably determine the server s fully qualified domain name, using set the directive globally to suppress this message sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice caught sigterm, shutting down sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice caught sigterm, shutting down sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice caught sigterm, shutting down sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice caught sigterm, shutting down sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice received. doing graceful restart sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may mpm_event:notice caught sigterm, shutting down sun may mpm_event:notice apache ubuntu mod_wsgi 4.5.17 python configured resuming normal operations sun may core:notice command line: sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 mod_wsgi : target wsgi script cannot be loaded as python module. sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 mod_wsgi : exception occurred processing wsgi script sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 traceback most recent call last : sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in module sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 application get_wsgi_application sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in get_wsgi_application sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 django.setup set_prefix=false sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in setup sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 configure_logging settings.logging_config, settings.logging sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in __getattr__ sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 self._setup name sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in _setup sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 self._wrapped settings settings_module sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 file line in __init__ sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 raise valueerror incorrect timezone setting: self.time_zone sun may wsgi:error remote xxx.xxx.xxx.xxx:44268 valueerror: incorrect timezone setting: sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 mod_wsgi : target wsgi script cannot be loaded as python module. sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 mod_wsgi : exception occurred processing wsgi script sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 traceback most recent call last : sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in module sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 application get_wsgi_application sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in get_wsgi_application sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 django.setup set_prefix=false sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in setup sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 configure_logging settings.logging_config, settings.logging sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in __getattr__ sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 self._setup name sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in _setup sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 self._wrapped settings settings_module sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 file line in __init__ sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 raise valueerror incorrect timezone setting: self.time_zone sun may wsgi:error remote xxx.xxx.xxx.xxx:40061 valueerror: incorrect timezone setting:"
    },
    {
        "url": "https://stackoverflow.com/questions/67450359",
        "text": "diskimage_builder.block_device.blockdevice getting value for image path diskimage_builder.block_device.level3.mount called for mount_mkfs_root diskimage_builder.block_device.utils calling sudo sync diskimage_builder.block_device.utils calling sudo fstrim verbose diskimage_builder.block_device.utils calling sudo umount diskimage_builder.block_device.utils calling sudo kpartx d traceback most recent call last : file line in module sys.exit main file line in main return bdc.main file line in main self.args.func file line in cmd_umount self.bd.cmd_umount file line in cmd_umount node.umount file line in umount self.partitioning.umount file line in umount self.state file line in exec_sudo raise e diskimage_builder.block_device.exception.blockdevicesetupexception: exec_sudo failed diskimage_builder.block_device.level3.mount called for mount_mkfs_root diskimage_builder.block_device.utils calling sudo sync diskimage_builder.block_device.utils calling sudo fstrim verbose traceback most recent call last : file line in module sys.exit main file line in main return bdc.main file line in main self.args.func file line in cmd_umount self.bd.cmd_umount file line in cmd_umount node.umount file line in umount self.state file line in exec_sudo raise e diskimage_builder.block_device.exception.blockdevicesetupexception: exec_sudo failed"
    },
    {
        "url": "https://stackoverflow.com/questions/67550020",
        "text": ":info::shell::100::root:: localhost executing script: rm rf :info::shell::100::root:: localhost executing script: mkdir p .ssh chmod .ssh grep ssh rsa echo ssh rsa chmod restorecon r .ssh :info::shell::100::root:: executing script: rpm q whatprovides yum utils yum install y yum utils :info::shell::49::root:: executing command: rpm qa name version release arch grep centos release openstack :info::shell::100::root:: executing script: rpm q yum y install centos release openstack ussuri true :info::shell::49::root:: executing command: rpm q rdo release version release arch :info::shell::100::root:: executing script: rpm q whatprovides yum utils yum install y yum utils yum clean metadata :info::shell::100::root:: executing script: yum install y puppet hiera openssh clients tar nc rubygem json yum update y puppet hiera openssh clients tar nc rubygem json rpm q whatprovides puppet rpm q whatprovides hiera rpm q whatprovides openssh clients rpm q whatprovides tar rpm q whatprovides nc rpm q whatprovides rubygem json :info::shell::100::root:: executing script: mkdir p mkdir mode mkdir mode mkdir mode :info::shell::100::root:: executing script: facter p :info::shell::100::root:: executing script: f ! l ln s echo skipping creation of hiera.yaml symlink sed i s;:datadir:.*;:datadir: puppet config print hiera_config :info::shell::100::root:: executing script: vgdisplay cinder volumes :info::shell::100::root:: localhost executing script: ssh keygen t rsa b f n :info::shell::100::root:: localhost executing script: ssh keyscan :info::shell::100::root:: executing script: systemctl :info::shell::100::root:: executing script: systemctl is enabled networkmanager :info::shell::100::root:: executing script: systemctl is active networkmanager :info::shell::100::root:: executing script: echo :info::shell::100::root:: localhost executing script: cd tar dereference cpzf .. hieradata ssh o o tar c xpzf cd cd tar dereference cpzf .. manifests ssh o o tar c xpzf cd tar dereference cpzf aodh apache ceilometer certmonger cinder concat firewall glance gnocchi heat horizon inifile ironic keystone magnum manila memcached mysql neutron nova nssdb openstack openstacklib oslo ovn packstack panko placement rabbitmq redis remote rsync sahara ssh stdlib swift sysctl systemd tempest trove vcsrepo vswitch xinetd ssh o o tar c xpzf :error::run_setup::1062::root:: traceback most recent call last : file line in main _main options, conffile, logfile file line in _main runsequences file line in runsequences controller.runallsequences file line in runallsequences sequence.run config=self.conf, file line in run step.run config=config, file line in run self.function config, messages file line in apply_puppet_manifest wait_for_puppet currently_running, messages file line in wait_for_puppet validate_logfile log file line in validate_logfile raise puppeterror message packstack.installer.exceptions.puppeterror: appeared during puppet run: error: facter: while resolving custom fact undefined method for nil:nilclass you will find full trace in log :info::shell::100::root:: executing script: rm rf"
    },
    {
        "url": "https://stackoverflow.com/questions/67550020",
        "text": "error: facter: while resolving custom fact undefined method for nil:nilclass warning: the function is deprecated in favor of using see file line not available warning: use of version is deprecated. it should be converted to version file: ... ... notice: value changed to notice: value changed to error: systemd start for rabbitmq server failed! journalctl log for rabbitmq server: logs begin at sat cdt, end at sat cdt. may openstack systemd : starting rabbitmq broker... may openstack rabbitmq server : both old .config and new .conf format config files exist. may openstack rabbitmq server : using the old format config file: may openstack rabbitmq server : please update your config files to the new format and remove the old file. may openstack rabbitmq server : error: epmd for host openstack: timeout timed out may openstack systemd : rabbitmq server.service: main process exited, failure may openstack systemd : rabbitmq server.service: failed with result may openstack systemd : failed to start rabbitmq broker. error: change from to failed: systemd start for rabbitmq server failed! journalctl log for rabbitmq server: logs begin at sat cdt, end at sat cdt. may openstack systemd : starting rabbitmq broker... may openstack rabbitmq server : both old .config and new .conf format config files exist. may openstack rabbitmq server : using the old format config file: may openstack rabbitmq server : please update your config files to the new format and remove the old file. may openstack rabbitmq server : error: epmd for host openstack: timeout timed out may openstack systemd : rabbitmq server.service: main process exited, failure may openstack systemd : rabbitmq server.service: failed with result may openstack systemd : failed to start rabbitmq broker. notice: triggered from events notice: triggered from events notice: triggered from event notice: fernet_setup : triggered from event notice: changed password notice: changed password notice: triggered from event notice: triggered from event notice: db_sync : triggered from events notice: triggered from event notice: bootstrap : triggered from event notice: triggered from events warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: fernet_rotate : skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies warning: skipping because of failed dependencies error: could not prefetch cinder_type provider could not authenticate error: failed to apply catalog: could not authenticate"
    },
    {
        "url": "https://stackoverflow.com/questions/67738414",
        "text": "task rabbitmq : check if each rabbit hostname resolves uniquely to the proper ip address localhost stream dgram raw stream dgram raw stream dgram raw , , , false, getent localhost , true, false, true, true, none, none, none, none, none, none , , false, stream localhost false, false, , false, getent localhost , false, null, null, null, null, null, null, true, true, true , , , stream dgram raw stream dgram raw stream dgram raw , stream localhost , hostname has to resolve uniquely to the ip address of api_interface"
    },
    {
        "url": "https://stackoverflow.com/questions/68355789",
        "text": "task keystone : include_tasks included: ... for task keystone : waiting for keystone ssh port to be up ok: task keystone : initialise fernet key authentication ok: node task keystone : run key distribution fatal: node : failed! true, , non zero return code , , , warning: permanently added ecdsa to the list of known hosts.\\r\\r\\nssh: connect to host node.17 port no route to connection unexpectedly closed bytes received so far sender error: unexplained code at io.c 235 warning: permanently added ecdsa to the list of known hosts. , ssh: connect to host node.17 port no route to host , rsync: connection unexpectedly closed bytes received so far sender , rsync error: unexplained code at io.c 235 no more hosts left play recap :"
    },
    {
        "url": "https://stackoverflow.com/questions/68377168",
        "text": "oslo_db.sqlalchemy.engines database connection was found disconnected; reconnecting: dbconnectionerror: pymysql.err.operationalerror lost connection to mysql server during query sql: u select background on this at: oslo_db.sqlalchemy.engines traceback most recent call last : oslo_db.sqlalchemy.engines file line in _connect_ping_listener oslo_db.sqlalchemy.engines connection.scalar select 1 oslo_db.sqlalchemy.engines file line in scalar oslo_db.sqlalchemy.engines return self.execute object_, .scalar oslo_db.sqlalchemy.engines file line in execute oslo_db.sqlalchemy.engines return meth self, multiparams, params oslo_db.sqlalchemy.engines file line in _execute_on_connection oslo_db.sqlalchemy.engines return connection._execute_clauseelement self, multiparams, params oslo_db.sqlalchemy.engines file line in _execute_clauseelement oslo_db.sqlalchemy.engines distilled_params, oslo_db.sqlalchemy.engines file line in _execute_context oslo_db.sqlalchemy.engines e, statement, parameters, cursor, context oslo_db.sqlalchemy.engines file line in _handle_dbapi_exception oslo_db.sqlalchemy.engines util.raise_from_cause newraise, exc_info oslo_db.sqlalchemy.engines file line in raise_from_cause oslo_db.sqlalchemy.engines reraise type exception , exception, oslo_db.sqlalchemy.engines file line in _execute_context oslo_db.sqlalchemy.engines cursor, statement, parameters, context oslo_db.sqlalchemy.engines file line in do_execute oslo_db.sqlalchemy.engines cursor.execute statement, parameters oslo_db.sqlalchemy.engines file line in execute oslo_db.sqlalchemy.engines result self._query query oslo_db.sqlalchemy.engines file line in _query oslo_db.sqlalchemy.engines conn.query q oslo_db.sqlalchemy.engines file line in query oslo_db.sqlalchemy.engines self._affected_rows self._read_query_result unbuffered=unbuffered oslo_db.sqlalchemy.engines file line in _read_query_result oslo_db.sqlalchemy.engines result.read oslo_db.sqlalchemy.engines file line in read oslo_db.sqlalchemy.engines first_packet self.connection._read_packet oslo_db.sqlalchemy.engines file line in _read_packet oslo_db.sqlalchemy.engines packet_header self._read_bytes 4 oslo_db.sqlalchemy.engines file line in _read_bytes oslo_db.sqlalchemy.engines cr.cr_server_lost, lost connection to mysql server during query oslo_db.sqlalchemy.engines dbconnectionerror: pymysql.err.operationalerror lost connection to mysql server during query sql: u select background on this at: oslo_db.sqlalchemy.engines oslo_db.sqlalchemy.engines database connection was found disconnected; reconnecting: dbconnectionerror: pymysql.err.operationalerror lost connection to mysql server during query sql: u select background on this at: oslo_db.sqlalchemy.engines traceback most recent call last : oslo_db.sqlalchemy.engines file line in _connect_ping_listener oslo_db.sqlalchemy.engines connection.scalar select 1 oslo_db.sqlalchemy.engines file line in scalar oslo_db.sqlalchemy.engines return self.execute object_, .scalar oslo_db.sqlalchemy.engines file line in execute oslo_db.sqlalchemy.engines return meth self, multiparams, params oslo_db.sqlalchemy.engines file line in _execute_on_connection oslo_db.sqlalchemy.engines return connection._execute_clauseelement self, multiparams, params oslo_db.sqlalchemy.engines file line in _execute_clauseelement oslo_db.sqlalchemy.engines distilled_params, oslo_db.sqlalchemy.engines file line in _execute_context oslo_db.sqlalchemy.engines e, statement, parameters, cursor, context oslo_db.sqlalchemy.engines file line in _handle_dbapi_exception oslo_db.sqlalchemy.engines util.raise_from_cause newraise, exc_info oslo_db.sqlalchemy.engines file line in raise_from_cause oslo_db.sqlalchemy.engines reraise type exception , exception, oslo_db.sqlalchemy.engines file line in _execute_context oslo_db.sqlalchemy.engines cursor, statement, parameters, context oslo_db.sqlalchemy.engines file line in do_execute oslo_db.sqlalchemy.engines cursor.execute statement, parameters oslo_db.sqlalchemy.engines file line in execute oslo_db.sqlalchemy.engines result self._query query oslo_db.sqlalchemy.engines file line in _query oslo_db.sqlalchemy.engines conn.query q oslo_db.sqlalchemy.engines file line in query oslo_db.sqlalchemy.engines self._affected_rows self._read_query_result unbuffered=unbuffered oslo_db.sqlalchemy.engines file line in _read_query_result oslo_db.sqlalchemy.engines result.read oslo_db.sqlalchemy.engines file line in read oslo_db.sqlalchemy.engines first_packet self.connection._read_packet oslo_db.sqlalchemy.engines file line in _read_packet oslo_db.sqlalchemy.engines lost connection to mysql server during query oslo_db.sqlalchemy.engines dbconnectionerror: pymysql.err.operationalerror lost connection to mysql server during query sql: u select background on this at: oslo_db.sqlalchemy.engines debug ironic_inspector.common.service_utils logging_exception_prefix asctime s.% 03d process d name s instance s log_opt_values debug oslo_service.service logging_exception_prefix asctime s.% 03d process d name s instance s log_opt_values debug oslo_db.sqlalchemy.engines mysql server mode set to strict_trans_tables,strict_all_tables,no_zero_in_date,no_zero_date,error_for_division_by_zero,traditional,no_auto_create_user,no_engine_substitution _check_effective_sql_mode"
    },
    {
        "url": "https://stackoverflow.com/questions/69452667",
        "text": "__main__ welcome to oslo logging __main__ a occurred __main__ an occurred asctime s.% 03d process d levelname s name s request_id s user_identity s instance message s"
    },
    {
        "url": "https://stackoverflow.com/questions/69452667",
        "text": "__main__ welcome to oslo logging __main__ a occurred __main__ an occurred asctime s.% 03d process d levelname s name s request_id s user_identity s instance message s"
    },
    {
        "url": "https://stackoverflow.com/questions/69715050",
        "text": "oct controller google_guest_agent 146448 : main.go:190 network when requesting metadata, make sure your instance has an active network and can reach the metadata server. oct controller google_guest_agent 146448 : main.go:193 watching metadata: get wait_for_change=true timeout_sec=60 last_etag=6f06fe6d055dd9f5: dial tcp connect: no route to host oct controller osconfigagent : tz osconfigagent main.go:218: get wait_for_change=true last_etag=6f06fe6d055dd9f5 timeout_sec=60: dial tcp connect: no route to host oct controller osconfigagent : tz osconfigagent main.go:218: network when requesting metadata, make sure your instance has an active network and can reach the metadata server: get wait_for_change=true last_etag=6f06fe6d055dd9f5 timeout_sec=60: dial tcp connect: no route to host oct controller osconfigagent : tz osconfigagent main.go:218: network when requesting metadata, make sure your instance has an active network and can reach the metadata server: get wait_for_change=true last_etag=6f06fe6d055dd9f5 timeout_sec=60: dial tcp connect: no route to host oct controller osconfigagent : tz osconfigagent main.go:218: network when requesting metadata, make sure your instance has an active network and can reach the metadata server: get wait_for_change=true last_etag=6f06fe6d055dd9f5 timeout_sec=60: dial tcp connect: no route to host"
    },
    {
        "url": "https://stackoverflow.com/questions/69790288",
        "text": "ovn metadata agent.service devstack ovn metadata agent.service loaded: loaded enabled; vendor preset: enabled active: failed result: exit code since sun cet; ago main pid: failure cpu: oct devstack systemd : ovn metadata agent.service: failed with result oct devstack systemd : ovn metadata agent.service: unit process neutron ovn met remains ru oct devstack systemd : ovn metadata agent.service: unit process neutron ovn met remains ru oct devstack systemd : ovn metadata agent.service: unit process privsep helper remains run oct devstack systemd : ovn metadata agent.service: consumed cpu time. oct devstack neutron ovn metadata agent : eventlet.wsgi.server wsgi exited, is_accepting oct devstack neutron ovn metadata agent : debug oslo_concurrency.lockutils acquired lock singleton_l oct devstack neutron ovn metadata agent : debug oslo_concurrency.lockutils acquired lock singleton_l oct devstack neutron ovn metadata agent : debug oslo_concurrency.lockutils releasing lock singleton_ oct devstack neutron ovn metadata agent : debug oslo_concurrency.lockutils releasing lock singleton_ m exit_trap m local m jobs p m m n m n m true m echo exit_trap: cleaning up child processes exit_trap: cleaning up child processes m kill m f m rm m kill_spinner m z m ne m echo on exit on exit m type p generate subunit m generate subunit fail m z m d m exit"
    },
    {
        "url": "https://stackoverflow.com/questions/69850132",
        "text": "critical root unhandled error: modulenotfounderror: no module named root traceback most recent call last : root file line in module root sys.exit main root file line in main root rpc_api_version=guest_api.api.api_latest_version root file line in __init__ root _manager importutils.import_object manager root file line in import_object root return import_class import_str *args, root file line in import_class root __import__ mod_str root modulenotfounderror: no module named"
    },
    {
        "url": "https://stackoverflow.com/questions/69971322",
        "text": "nova.virt.libvirt.driver instance: creating image nova.image.glance writing to object is not iterable nova.compute.manager instance: instance failed to spawn nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _build_resources nova.compute.manager instance: yield resources nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: block_device_info=block_device_info nova.compute.manager instance: file line in spawn nova.compute.manager instance: block_device_info=block_device_info nova.compute.manager instance: file line in _create_image nova.compute.manager instance: fallback_from_host nova.compute.manager instance: file line in _create_and_inject_local_root nova.compute.manager instance: instance, size, fallback_from_host nova.compute.manager instance: file line in _try_fetch_image_cache nova.compute.manager instance: nova.compute.manager instance: file line in cache nova.compute.manager instance: nova.compute.manager instance: file line in create_image nova.compute.manager instance: prepare_template target=base, nova.compute.manager instance: file line in inner nova.compute.manager instance: return f nova.compute.manager instance: file line in fetch_func_sync nova.compute.manager instance: fetch_func target=target, nova.compute.manager instance: file line in fetch_image nova.compute.manager instance: images.fetch_to_raw context, image_id, target nova.compute.manager instance: file line in fetch_to_raw nova.compute.manager instance: fetch context, image_href, path_tmp nova.compute.manager instance: file line in fetch nova.compute.manager instance: image_api.download context, image_href, dest_path=path nova.compute.manager instance: file line in download nova.compute.manager instance: dst_path=dest_path nova.compute.manager instance: file line in download nova.compute.manager instance: dst_path, ex nova.compute.manager instance: file line in __exit__ nova.compute.manager instance: self.force_reraise nova.compute.manager instance: file line in force_reraise nova.compute.manager instance: six.reraise self.type_, self.value, self.tb nova.compute.manager instance: file line in download nova.compute.manager instance: for chunk in image_chunks: nova.compute.manager instance: typeerror: object is not iterable"
    },
    {
        "url": "https://stackoverflow.com/questions/70606923",
        "text": "debug eventlet.wsgi.server accepted server debug glance.api.middleware.version_negotiation determining version of request: get accept: process_request debug glance.api.middleware.version_negotiation using url versioning process_request debug glance.api.middleware.version_negotiation matched version: process_request debug glance.api.middleware.version_negotiation new path process_request keystonemiddleware.auth_token authorization failed for token: invalidtoken: token authorization failed"
    },
    {
        "url": "https://stackoverflow.com/questions/70606923",
        "text": "keystone.server.flask.application could not recognize fernet token: tokennotfound: could not recognize fernet token"
    },
    {
        "url": "https://stackoverflow.com/questions/70617008",
        "text": "jan openstack object server : object replication complete. minutes jan openstack container server : skipping: is not mounted jan openstack container server : beginning replication run jan openstack container server : replication run over jan openstack container server : attempted to replicate dbs in seconds jan openstack container server : removed dbs jan openstack container server : successes, failures jan openstack container server : diff:0 diff_capped:0 empty:0 hashmatch:0 no_change:0 remote_merge:0 rsync:0 ts_repl:0 jan openstack object server : begin object audit mode zbf jan openstack object server : object audit zbf mode completed: total quarantined: total errors: total files sec: total bytes sec: auditing time: rate: jan openstack object server : begin object audit mode all jan openstack object server : object audit all mode completed: total quarantined: total errors: total files sec: total bytes sec: auditing time: rate: jan openstack object server : starting object reconstruction pass. jan openstack object server : nothing reconstructed for seconds. jan openstack object server : object reconstruction complete. minutes jan openstack account server : skipping: is not mounted jan openstack account server : beginning replication run jan openstack account server : replication run over jan openstack account server : attempted to replicate dbs in seconds jan openstack account server : removed dbs jan openstack account server : successes, failures jan openstack account server : diff:0 diff_capped:0 empty:0 hashmatch:0 no_change:0 remote_merge:0 rsync:0 ts_repl:0 jan openstack object server : starting object replication pass. jan openstack object server : is not mounted jan openstack object server : nothing replicated for seconds. jan openstack object server : object replication complete. minutes jan openstack container server : skipping: is not mounted jan openstack container server : beginning replication run jan openstack container server : replication run over jan openstack container server : attempted to replicate dbs in seconds jan openstack container server : removed dbs jan openstack container server : successes, failures jan openstack container server : diff:0 diff_capped:0 empty:0 hashmatch:0 no_change:0 remote_merge:0 rsync:0 ts_repl:0 jan openstack object server : begin object audit mode zbf jan openstack object server : object audit zbf mode completed: total quarantined: total errors: total files sec: total bytes sec: auditing time: rate: jan openstack object server : starting object reconstruction pass. jan openstack object server : nothing reconstructed for seconds. jan openstack object server : object reconstruction complete. minutes"
    },
    {
        "url": "https://stackoverflow.com/questions/70661495",
        "text": "closing amqp connection xx.xx.xx.36:40530 xx.xx.xx.36:5672 nova compute:6:f8xxxxxxxxxx58 accepting amqp connection xx.xx.xx.36:33010 xx.xx.xx.36:5672 connection xx.xx.xx.36:33010 xx.xx.xx.36:5672 has a client provided name: nova compute:6:f8xxxxxxxxxx58 connection xx.xx.xx.36:33010 xx.xx.xx.36:5672 nova compute:6:f8xxxxxxxxxx58 user authenticated and granted access to vhost vm_memory_high_watermark set. memory used:81022431232 allowed:81003041177 memory resource limit alarm set on node publishers will be blocked until this alarm clears closing amqp connection xx.xx.xx.35:57244 xx.xx.xx.36:5672 nova compute:6:c5xxxxxxxxxx63 accepting amqp connection xx.xx.xx.35:49700 xx.xx.xx.36:5672 connection xx.xx.xx.35:49700 xx.xx.xx.36:5672 has a client provided name: nova compute:6:c5xxxxxxxxxx63 lager_file_backend dropped messages in the last second that exceeded the limit of messages sec closing amqp connection xx.xx.xx.35:45454 xx.xx.xx.36:5672 nova compute:6:a7xxxxxxxxxxee accepting amqp connection xx.xx.xx.35:37474 xx.xx.xx.36:5672"
    },
    {
        "url": "https://stackoverflow.com/questions/70671180",
        "text": "cloud init : cloud init v. running at tue, jan up seconds. cloud init : stages.py : failed to rename devices: nic not present cannot rename to not available. ok finished cloud init job pre networking ok reached target pre starting service ok started service starting for network to be configured starting name resolution ok finished for network to be configured starting cloud metadata service crawler ok started name resolution ok reached target ok reached target and network name lookups cloud init : cloud init v. running at tue, jan up seconds. cloud init : ci info: device cloud init : ci info: cloud init : ci info: device up address mask scope hw address cloud init : ci info: cloud init : ci info: false . . . fa:16:3e:db:2c:9b cloud init : ci info: lo true host . cloud init : ci info: lo true . host . cloud init : ci info: cloud init : ci info: cloud init : ci info: cloud init : ci info: route destination gateway interface flags cloud init : ci info: cloud init : ci info: cloud init : stages.py : failed to rename devices: nic not present cannot rename to not available. ok finished cloud metadata service crawler ok reached target availability ok reached target is online ok reached target initialization"
    },
    {
        "url": "https://stackoverflow.com/questions/70722495",
        "text": "watcher api get http status: len: time: watcher api traceback most recent call last : file line in handle_one_response result self.application self.environ, start_response file line in __call__ return self.v1 start_response file line in __call__ return super authtokenmiddleware, self .__call__ env, start_response file line in __call__ resp self.call_func req, file line in call_func return self.func req, file line in __call__ response self.process_request file line in process_request resp super authprotocol, self .process_request request file line in process_request data, user_auth_ref self._do_fetch_token file line in _do_fetch_token data self.fetch_token token, file line in fetch_token data self._identity_server.verify_token file line in verify_token auth_ref self._request_strategy.verify_token file line in _request_strategy strategy_class self._get_strategy_class file line in _get_strategy_class if self._adapter.get_endpoint version=klass.auth_version : file line in get_endpoint return self.session.get_endpoint auth or self.auth, file line in get_endpoint return auth.get_endpoint self, file line in get_endpoint endpoint_data self.get_endpoint_data file line in get_endpoint_data endpoint_data service_catalog.endpoint_data_for file line in endpoint_data_for raise exceptions.endpointnotfound msg internal endpoint for identity service in regionone region not found"
    },
    {
        "url": "https://stackoverflow.com/questions/70722495",
        "text": "keystonemiddleware.auth_token identity response: request you have made requires authentication. : the request you have made requires authentication. http request id: critical keystonemiddleware.auth_token unable to validate token: identity server rejected authorization necessary to fetch token data: keystonemiddleware.auth_token._exceptions.serviceerror: identity server rejected authorization necessary to fetch token data"
    },
    {
        "url": "https://stackoverflow.com/questions/70858855",
        "text": "oslo_messaging._drivers.amqpdriver reply_2f4eca3967574c7791341bbd5471b2a9 doesn t exists, drop reply to amqpdestinationnotfound: exchange reply_2f4eca3967574c7791341bbd5471b2a9 doesn t exists oslo_messaging._drivers.amqpdriver the reply cannot be sent reply_2f4eca3967574c7791341bbd5471b2a9 reply queue don t exist after sec abandoning."
    },
    {
        "url": "https://stackoverflow.com/questions/71330477",
        "text": "cloud init : util.py : no active metadata service found"
    },
    {
        "url": "https://stackoverflow.com/questions/71330477",
        "text": "keypairs status: len: microversion: time: oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: server unexpectedly closed connection nova.api.openstack.requestlog .... get status: len: microversion: time:"
    },
    {
        "url": "https://stackoverflow.com/questions/71343605",
        "text": "nova.scheduler.utils instance: from last host: node : traceback most recent call last , file line in _build_and_run_instance\\n self.driver.spawn context, instance, image_meta,\\n , file line in self._create_guest_with_network \\n , file line in _create_guest_with_network\\n self. cleanup_failed_start \\n , file line in exit self.force_reraise \\n , file line in force_reraise\\n six.reraise self.type , self.value, self.tb \\n file line in raise , file line in _create_guest_with_network\\n guest self._create_guest \\n , file line in _create_guest\\n guest libvirt_guest.guest.create xml, self. host , file line in log.error defining a guest with xml: file line in exit self.force_reraise \\n , file line in force_reraise\\n six.reraise self.type , self.value, self.tb \\n file line in raise , file line in guest host.write_instance_config xml \\n , file line in write_instance_config\\n domain self.get_connection .definexml xml \\n , file line in result proxy_call self._autowrap, f, , file line in proxy_call\\n rv execute f, , file line in six.reraise c, e, tb , file line in raise , file line in rv meth , file line in if ret is none:raise libvirterror virdomaindefinexml failed , , libvirt.libvirterror: unsupported configuration: emulator does not support virt type handling of the above exception, another exception occurred:\\n\\n traceback most recent call last , file line in _do_build_and_run_instance\\n self._build_and_run_instance context, instance, image,\\n file line in _build_and_run_instance\\n raise exception.rescheduledexception \\n nova.exception.rescheduledexception: build of instance was re scheduled: unsupported configuration: emulator does not support virt type nova.scheduler.utils failed to compute_task_build_instances: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance nova.exception.maxretriesexceeded: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance"
    },
    {
        "url": "https://stackoverflow.com/questions/71343605",
        "text": ": vircpugethost:439 : this function is not supported by the connection driver: cannot detect host cpu model for architecture : vircpugethost:439 : this function is not supported by the connection driver: cannot detect host cpu model for architecture : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : virqemucapscachelookupdefault:5577 : invalid argument: kvm is not supported by on this host : qemudomaindefvalidate:5725 : unsupported configuration: emulator does not support virt type"
    },
    {
        "url": "https://stackoverflow.com/questions/71397727",
        "text": "tail f cinder.scheduler.host_manager volume service is down. host: cinder.message.api deleted expired messages. cinder.scheduler.host_manager volume service is down. host: cinder.scheduler.host_manager volume service is down. host: cinder.scheduler.host_manager volume service is down. host: cinder.scheduler.host_manager volume service is down. host:"
    },
    {
        "url": "https://stackoverflow.com/questions/71397727",
        "text": "tail f cinder.service starting cinder volume node version cinder.volume.manager starting volume driver lvmvolumedriver cinder.volume.drivers.lvm enabling lvm thin provisioning by because a thin pool exists. cinder.volume.driver driver hasn t implemented _init_vendor_properties cinder.keymgr.migration not migrating encryption keys because the confkeymanager s fixed_key is not in use. cinder.volume.manager driver initialization completed successfully. cinder.manager initiating service cleanup cinder.manager service cleanup completed. cinder.volume.manager initializing rpc dependent components of volume driver lvmvolumedriver cinder.volume.manager driver post rpc initialization completed successfully."
    },
    {
        "url": "https://stackoverflow.com/questions/71513005",
        "text": "oslo_service.service file line in _on_close oslo_service.service class_id, method_id , connectionerror oslo_service.service amqp.exceptions.internalerror: connection.open: internal_error access to vhost refused for user vhost is down oslo_service.service neutron.plugins.ml2.drivers.agent._common_agent stopping linux bridge agent agent."
    },
    {
        "url": "https://stackoverflow.com/questions/71513005",
        "text": "on amqp connection neutron linuxbridge agent:7:11111111 vhost: user: state: opening , channel handshake_error,opening, amqp_error,internal_error, access to vhost refused for user vhost is down ,"
    },
    {
        "url": "https://stackoverflow.com/questions/71673864",
        "text": "kypo head stack : create_in_progress stack create started kypo head stack.kypo head port : create_in_progress state changed kypo head stack.kypo head port : create_complete state changed kypo head stack.kypo head : create_in_progress state changed kypo head stack.kypo head floating ip : create_in_progress state changed kypo head stack.kypo head floating ip : create_complete state changed kypo head stack.kypo head : create_failed resourceinerror: resources.kypo head: went to status due to message: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance code: kypo head stack.kypo head : delete_in_progress state changed kypo head stack.kypo head : delete_complete state changed kypo head stack.kypo head : create_in_progress state changed kypo head stack.kypo head : create_failed resourceinerror: resources.kypo head: went to status due to message: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance code: kypo head stack.kypo head : delete_in_progress state changed kypo head stack.kypo head : delete_complete state changed kypo head stack.kypo head : create_in_progress state changed kypo head stack.kypo head : create_failed resourceinerror: resources.kypo head: went to status due to message: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance fecb code: stack kypo head stack create_failed kypo proxy jump stack : create_in_progress stack create started kypo proxy jump stack.kypo proxy jump port : create_in_progress state changed kypo proxy jump stack.kypo proxy jump port : create_complete state changed kypo proxy jump stack.kypo proxy jump floating ip : create_in_progress state changed kypo proxy jump stack.kypo proxy jump : create_in_progress state changed kypo proxy jump stack.kypo proxy jump floating ip : create_complete state changed kypo proxy jump stack.kypo proxy jump : create_failed resourceinerror: resources.kypo proxy jump: went to status due to message: exceeded maximum number of retries. exhausted all hosts available for retrying build failures for instance code: ... stack kypo proxy jump stack create_failed"
    },
    {
        "url": "https://stackoverflow.com/questions/71706595",
        "text": "oslo_service.service libvirt.libvirterror: unable to connect to server at connection refused oslo_service.service oslo_service.service during handling of the above exception, another exception occurred: oslo_service.service oslo_service.service traceback most recent call last : oslo_service.service file line in run_service oslo_service.service service.start oslo_service.service file line in start oslo_service.service self.manager.init_host oslo_service.service file line in init_host oslo_service.service self.driver.init_host host=self.host oslo_service.service file line in init_host oslo_service.service self._update_host_specific_capabilities oslo_service.service file line in _update_host_specific_capabilities oslo_service.service self._host.supports_secure_boot, oslo_service.service file line in supports_secure_boot oslo_service.service arch self.get_capabilities .host.cpu.arch oslo_service.service file line in get_capabilities oslo_service.service xmlstr self.get_connection .getcapabilities oslo_service.service file line in get_connection oslo_service.service raise exception.hypervisorunavailable oslo_service.service nova.exception.hypervisorunavailable: connection to the hypervisor is broken on host oslo_service.service nova.virt.libvirt.driver connection event reason failed to connect to libvirt: unable to connect to server at connection refused os_vif loaded vif plugins: linux_bridge, noop, ovs nova.virt.driver loading compute driver"
    },
    {
        "url": "https://stackoverflow.com/questions/71955855",
        "text": "connection nova compute:7:1dab7694 user authenticated and granted access to vhost closing amqp connection nova compute:7:facf1224 vhost: user: closing amqp connection nova compute:7:9c706aca vhost: user: supervisor ,rabbit_channel_sup_sup had child channel_sup started with rabbit_channel_sup:start_link at undefined exit with reason shutdown in context shutdown_error closing amqp connection nova compute:7:5fdd2029 vhost: user: accepting amqp connection connection has a client provided name: nova conductor:24:71983386 connection nova conductor:24:71983386 user authenticated and granted access to vhost lager_error_logger_h dropped messages in the last second that exceeded the limit of messages sec accepting amqp connection connection has a client provided name: nova compute:7:be0a8525 connection nova compute:7:be0a8525 user authenticated and granted access to vhost closing amqp connection nova conductor:21:e037e12d missed heartbeats from client, timeout: accepting amqp connection connection has a client provided name: nova conductor:21:e037e12d connection nova conductor:21:e037e12d user authenticated and granted access to vhost closing amqp connection nova compute:7:e592f063 missed heartbeats from client, timeout: closing amqp connection cinder volume:32:2a7ba690 missed heartbeats from client, timeout: closing amqp connection cinder volume:32:c7fddb16 missed heartbeats from client, timeout: closing amqp connection nova conductor:24:71983386 missed heartbeats from client, timeout: a lot of above missed heartbeats log, until restart the node rabbitmq process connection nova scheduler:59:d19d2307 user authenticated and granted access to vhost mirrored queue in vhost master saw deaths of mirrors mirrored queue in vhost master saw deaths of mirrors"
    },
    {
        "url": "https://stackoverflow.com/questions/71955855",
        "text": "connection has a client provided name: nova compute:7:a480ff3a lager_file_backend dropped messages in the last second that exceeded the limit of messages sec closing amqp connection nova compute:7:310dcd66 vhost: user: accepting amqp connection connection has a client provided name: nova compute:7:012d81c4 connection nova compute:7:012d81c4 user authenticated and granted access to vhost closing amqp connection nova conductor:23:7f5f17b1 missed heartbeats from client, timeout: accepting amqp connection connection has a client provided name: nova conductor:23:7f5f17b1 connection nova conductor:23:7f5f17b1 user authenticated and granted access to vhost closing amqp connection nova conductor:22:6de6e8f9 missed heartbeats from client, timeout: accepting amqp connection connection has a client provided name: nova conductor:22:6de6e8f9 connection nova conductor:22:6de6e8f9 user authenticated and granted access to vhost closing amqp connection nova conductor:24:af6de2d2 missed heartbeats from client, timeout: a lot of above and missed heartbeats log, until restart the this node rabbitmq process accepting amqp connection connection has a client provided name: nova compute:7:355e2e03 connection nova compute:7:355e2e03 user authenticated and granted access to vhost closing amqp connection nova conductor:24:6341a486 missed heartbeats from client, timeout: accepting amqp connection connection has a client provided name: nova conductor:24:6341a486 connection nova conductor:24:6341a486 user authenticated and granted access to vhost closing amqp connection nova conductor:22:bbe54f83 vhost: user: client unexpectedly closed tcp connection accepting amqp connection connection has a client provided name: nova conductor:22:bbe54f83 connection nova conductor:22:bbe54f83 user authenticated and granted access to vhost closing amqp connection nova compute:7:37b137ea vhost: user: client unexpectedly closed tcp connection closing amqp connection nova conductor:21:7bd7d6dc vhost: user: client unexpectedly closed tcp connection closing amqp connection nova conductor:25:2e3dd6d2 vhost: user: client unexpectedly closed tcp connection lager_file_backend dropped messages in the last second that exceeded the limit of messages sec rabbitmq is asked to stop... accepting amqp connection connection has a client provided name: nova conductor:22:99215d08 connection nova conductor:22:99215d08 user authenticated and granted access to vhost stopping rabbitmq applications and their dependencies in the following order: rabbitmq_management amqp_client rabbitmq_web_dispatch cowboy cowlib rabbitmq_management_agent rabbit mnesia rabbit_common os_mon stopping application rabbitmq http listener registry could not find context rabbitmq_management_tls application rabbitmq_management exited with reason: stopped stopping application application amqp_client exited with reason: stopped stopping application application rabbitmq_web_dispatch exited with reason: stopped stopping application closing amqp connection nova conductor:25:b31b82a2 vhost: user: client unexpectedly closed tcp connection stopping application stopping application application cowboy exited with reason: stopped application cowlib exited with reason: stopped stopping application application rabbitmq_management_agent exited with reason: stopped peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping unregistration. stopped tcp listener on on amqp connection barbican keystone listener:7:b59ea871 vhost: user: state: running , channel operation none caused a connection exception connection_forced: broker forced connection closure with reason a lot of these and operation none caused log on amqp connection magnum conductor:112:0a1e307e vhost: user: state: running , channel operation none caused a connection exception connection_forced: broker forced connection closure with reason application lager started on node log file opened with lager application mnesia started on node application mnesia exited with reason: stopped application recon started on node application inets started on node application jsx started on node application os_mon started on node application crypto started on node application cowlib started on node application mnesia started on node application xmerl started on node application started on node application public_key started on node application ssl started on node application ranch started on node application cowboy started on node application rabbit_common started on node application amqp_client started on node starting rabbitmq on erlang copyright c pivotal software, inc. licensed under the mpl. see node : home dir : config file s : cookie hash : log s : : database dir : memory high watermark set to mib bytes of mib bytes total enabling free disk space monitoring disk free limit set to limiting to approx file handles sockets fhc read buffering: off fhc write buffering: on waiting for mnesia tables for ms, retries left waiting for mnesia tables for ms, retries left waiting for mnesia tables for ms, retries left peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping registration. priority queues enabled, real bq is rabbit_variable_queue starting rabbit_node_monitor management plugin: using rates mode making sure data directory for vhost exists starting message stores for vhost message store using rabbit_msg_store_ets_index to provide index started message store of type transient for vhost message store using rabbit_msg_store_ets_index to provide index message store rebuilding indices from scratch started message store of type persistent for vhost mirrored queue in vhost adding mirror on node mirrored queue in vhost adding mirror on node a lot of different mirrored queue log"
    },
    {
        "url": "https://stackoverflow.com/questions/71955855",
        "text": "closing amqp connection nova conductor:23:3ca11891 missed heartbeats from client, timeout: a lot of above missed heartbeats log, until restart the node rabbitmq process accepting amqp connection connection has a client provided name: nova conductor:25:53cc4527 connection nova conductor:25:53cc4527 user authenticated and granted access to vhost closing amqp connection nova conductor:21:b701bc54 missed heartbeats from client, timeout: closing amqp connection nova conductor:23:615ea986 missed heartbeats from client, timeout: less of but lot of log, until restart the node rabbitmq process connection mod_wsgi:32:06e32cf4 2d5f user authenticated and granted access to vhost accepting amqp connection mirrored queue in vhost slave saw deaths of mirrors mirrored queue in vhost slave saw deaths of mirrors"
    },
    {
        "url": "https://stackoverflow.com/questions/71955855",
        "text": "oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: server unexpectedly closed connection. trying again in seconds.: oserror: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port duplicate above log oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer oslo.messaging._drivers.impl_rabbit amqp server on is unreachable: errno connection reset by peer. trying again in seconds.: connectionreseterror: errno connection reset by peer duplicate above log oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit reconnected to amqp server on via amqp client with port oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: errno connection reset by peer oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: errno connection reset by peer oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit a recoverable connection channel occurred, trying to reconnect: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit failed to consume message from queue: server unexpectedly closed connection: kombu.exceptions.operationalerror: server unexpectedly closed connection oslo.messaging._drivers.impl_rabbit unable to connect to amqp server on after inf tries: server unexpectedly closed connection: kombu.exceptions.operationalerror: server unexpectedly closed connection oslo_messaging._drivers.amqpdriver failed to process incoming message, retrying..: oslo_messaging.exceptions.messagedeliveryfailure: unable to connect to amqp server on after inf tries: server unexpectedly closed connection"
    },
    {
        "url": "https://stackoverflow.com/questions/71998698",
        "text": ". . . oslo.messaging._drivers.impl_rabbit connection failed: timed out retrying in seconds : socket.timeout: timed out oslo.messaging._drivers.impl_rabbit connection failed: timed out retrying in seconds : socket.timeout: timed out oslo.messaging._drivers.impl_rabbit connection failed: timed out retrying in seconds : socket.timeout: timed out oslo.messaging._drivers.impl_rabbit connection failed: timed out retrying in seconds : socket.timeout: timed out oslo.messaging._drivers.impl_rabbit connection failed: timed out retrying in seconds : socket.timeout: timed out . . ."
    },
    {
        "url": "https://stackoverflow.com/questions/72060675",
        "text": "microstack_init setting up as a control node. microstack_init generating tls certificate and key microstack_init configuring networking ... microstack_init opening horizon dashboard up to microstack_init waiting for rabbitmq to start ... waiting for microstack_init rabbitmq started! microstack_init configuring rabbitmq ... microstack_init rabbitmq configured! microstack_init waiting for mysql server to start ... waiting for microstack_init mysql server started! creating databases ... microstack_init configuring keystone fernet keys ... traceback most recent call last : file line in module load_entry_point file line in main cmd file line in wrapper return func file line in init question.ask file line in ask self.yes awr file line in yes check file line in check raise subprocess.calledprocesserror proc.returncode, .join args subprocess.calledprocesserror: command snap openstack launch keystone manage db_sync returned non zero exit status"
    },
    {
        "url": "https://stackoverflow.com/questions/72171183",
        "text": "nova.exception.novalidhost: no valid host was found. : nova.exception_remote.novalidhost_remote: no valid host was found. nova.scheduler.utils instance: setting instance to active state.: nova.exception_remote.novalidhost_remote: no valid host was found. nova.scheduler.utils failed to compute_task_migrate_server: no valid host was found. traceback most recent call last : file line in inner return func file line in select_destinations raise exception.novalidhost reason= nova.exception.novalidhost: no valid host was found. : nova.exception_remote.novalidhost_remote: no valid host was found. nova.scheduler.utils instance: setting instance to active state.: nova.exception_remote.novalidhost_remote: no valid host was found."
    },
    {
        "url": "https://stackoverflow.com/questions/72303782",
        "text": "oslo_service.service starting thread.: attributeerror: object has no attribute oslo_service.service traceback most recent call last : oslo_service.service file line in run_service oslo_service.service service.start oslo_service.service file line in start oslo_service.service self.manager.init_host oslo_service.service file line in init_host oslo_service.service self.driver.init_host host=self.host oslo_service.service file line in init_host oslo_service.service self._check_cpu_compatibility oslo_service.service file line in _check_cpu_compatibility oslo_service.service self._compare_cpu cpu, self._get_cpu_info , none oslo_service.service file line in _compare_cpu oslo_service.service cpu self._vcpu_model_to_cpu_config guest_cpu oslo_service.service file line in _vcpu_model_to_cpu_config oslo_service.service if vcpu_model.topology: oslo_service.service attributeerror: object has no attribute oslo_service.service"
    },
    {
        "url": "https://stackoverflow.com/questions/72349021",
        "text": "nova.compute.claims instance: attempting claim on node memory mb, disk gb, vcpus cpu nova.compute.claims instance: total memory: mb, used: mb nova.compute.claims instance: memory limit not specified, defaulting to unlimited nova.compute.claims instance: total disk: gb, used: gb nova.compute.claims instance: disk limit not specified, defaulting to unlimited nova.compute.claims instance: total vcpu: vcpu, used: vcpu nova.compute.claims instance: vcpu limit not specified, defaulting to unlimited nova.compute.claims instance: claim successful on node"
    },
    {
        "url": "https://stackoverflow.com/questions/72430507",
        "text": "collector monasca_agent.collector.checks.collector collector.py:264 plugin prometheus still running after collector monasca_agent.collector.checks.collector collector.py:183 collection time for check prometheus is high:"
    },
    {
        "url": "https://stackoverflow.com/questions/72526356",
        "text": "neutron.agent.dhcp.agent neutron.agent.dhcp.agent unable to sync network state.: oslo_messaging.rpc.client.remoteerror: remote error: programmingerror pymysql.err.programmingerror table doesn t exist on this at: most recent call last , file line in _execute_context\\n cursor, statement, parameters, , file line in do_execute\\n cursor.execute statement, parameters , file line in result self._query query \\n file line in conn.query q \\n file line in self._affected_rows self._read_query_result unbuffered=unbuffered \\n , file line in _read_query_result\\n result.read \\n file line in first_packet self.connection._read_packet \\n , file line in _read_packet\\n packet.check_error \\n , file line in check_error\\n err.raise_mysql_exception self._data \\n , file line in raise_mysql_exception\\n raise errorclass errno, errval , pymysql.err.programmingerror: table t exist above exception was the direct cause of the following exception:\\n\\n traceback most recent call last , file line in _process_incoming\\n res self.dispatcher.dispatch message \\n , file line in return self._do_dispatch endpoint, method, ctxt, args , file line in _do_dispatch\\n result func ctxt, \\n file line in get_active_networks_info\\n networks self._get_active_networks context, , file line in _get_active_networks\\n plugin.auto_schedule_networks context, host , file line in auto_schedule_networks\\n self.network_scheduler.auto_schedule_networks self, context, host , file line in auto_schedule_networks\\n subnets plugin.get_subnets context, , file line in return method , file line in get_subnets\\n marker, page_reverse \\n file line in _get_subnets\\n , file line in get_objects\\n cls, context, kwargs \\n , file line in get_objects\\n _pager.to_kwargs context, obj_cls if _pager else , file line in get_collection\\n for c in , file line in __iter__\\n return self._execute_and_instances context \\n , file line in _execute_and_instances\\n result conn.execute querycontext.statement, self._params \\n file line in return meth self, multiparams, params , file line in _execute_on_connection\\n return connection._execute_clauseelement self, multiparams, params , file line in _execute_clauseelement\\n distilled_params,\\n , file line in _execute_context\\n e, statement, parameters, cursor, , file line in _handle_dbapi_exception\\n util.raise_from_cause newraise, exc_info \\n file line in raise_from_cause\\n reraise type exception , exception, , file line in raise value.with_traceback tb \\n , file line in _execute_context\\n cursor, statement, parameters, , file line in do_execute\\n cursor.execute statement, parameters , file line in result self._query query \\n file line in conn.query q \\n file line in self._affected_rows self._read_query_result unbuffered=unbuffered \\n , file line in _read_query_result\\n result.read \\n file line in first_packet self.connection._read_packet \\n , file line in _read_packet\\n packet.check_error \\n , file line in check_error\\n err.raise_mysql_exception self._data \\n , file line in raise_mysql_exception\\n raise errorclass errno, errval , sqlalchemy.exc.programmingerror: pymysql.err.programmingerror table t exist sql: select subnets.project_id as subnets_project_id, subnets.id as subnets_id, subnets.in_use as subnets_in_use, subnets.name as subnets_name, subnets.network_id as subnets_network_id, subnets.segment_id as subnets_segment_id, subnets.subnetpool_id as subnets_subnetpool_id, subnets.ip_version as subnets_ip_version, subnets.cidr as subnets_cidr, subnets.gateway_ip as subnets_gateway_ip, subnets.enable_dhcp as subnets_enable_dhcp, subnets.ipv6_ra_mode as subnets_ipv6_ra_mode, subnets.ipv6_address_mode as subnets_ipv6_address_mode, subnets.standard_attr_id as subnets_standard_attr_id, subnetpools_1.shared as subnetpools_1_shared, standardattributes_1.id as standardattributes_1_id, standardattributes_1.resource_type as standardattributes_1_resource_type, standardattributes_1.description as standardattributes_1_description, standardattributes_1.revision_number as standardattributes_1_revision_number, standardattributes_1.created_at as standardattributes_1_created_at, standardattributes_1.updated_at as standardattributes_1_updated_at, subnetpools_1.project_id as subnetpools_1_project_id, subnetpools_1.id as subnetpools_1_id, subnetpools_1.name as subnetpools_1_name, subnetpools_1.ip_version as subnetpools_1_ip_version, subnetpools_1.default_prefixlen as subnetpools_1_default_prefixlen, subnetpools_1.min_prefixlen as subnetpools_1_min_prefixlen, subnetpools_1.max_prefixlen as subnetpools_1_max_prefixlen, subnetpools_1.is_default as subnetpools_1_is_default, subnetpools_1.default_quota as subnetpools_1_default_quota, subnetpools_1.hash as subnetpools_1_hash, subnetpools_1.address_scope_id as subnetpools_1_address_scope_id, subnetpools_1.standard_attr_id as subnetpools_1_standard_attr_id, standardattributes_2.id as standardattributes_2_id, standardattributes_2.resource_type as standardattributes_2_resource_type, standardattributes_2.description as standardattributes_2_description, standardattributes_2.revision_number as standardattributes_2_revision_number, standardattributes_2.created_at as standardattributes_2_created_at, standardattributes_2.updated_at as standardattributes_2_updated_at, subnet_dns_publish_fixed_ips_1.subnet_id as subnet_dns_publish_fixed_ips_1_subnet_id, subnet_dns_publish_fixed_ips_1.dns_publish_fixed_ip as subnet_dns_publish_fixed_ips_1_dns_publish_fixed_ip subnets left outer join subnetpools as subnetpools_1 on subnets.subnetpool_id subnetpools_1.id left outer join standardattributes as standardattributes_1 on standardattributes_1.id subnetpools_1.standard_attr_id left outer join standardattributes as standardattributes_2 on standardattributes_2.id subnets.standard_attr_id left outer join subnet_dns_publish_fixed_ips as subnet_dns_publish_fixed_ips_1 on subnets.id subnet_dns_publish_fixed_ips_1.subnet_id \\n background on this at: neutron.agent.dhcp.agent traceback most recent call last : neutron.agent.dhcp.agent file line in sync_state neutron.agent.dhcp.agent enable_dhcp_filter=false neutron.agent.dhcp.agent file line in get_active_networks_info neutron.agent.dhcp.agent neutron.agent.dhcp.agent file line in call neutron.agent.dhcp.agent return self._original_context.call ctxt, method, neutron.agent.dhcp.agent file line in call neutron.agent.dhcp.agent transport_options=self.transport_options neutron.agent.dhcp.agent file line in _send neutron.agent.dhcp.agent transport_options=transport_options neutron.agent.dhcp.agent file line in send neutron.agent.dhcp.agent transport_options=transport_options neutron.agent.dhcp.agent file line in _send neutron.agent.dhcp.agent raise result neutron.agent.dhcp.agent oslo_messaging.rpc.client.remoteerror: remote error: programmingerror pymysql.err.programmingerror table doesn t exist neutron.agent.dhcp.agent sql: select subnets.project_id as subnets_project_id, subnets.id as subnets_id, subnets.in_use as subnets_in_use, subnets.name as subnets_name, subnets.network_id as subnets_network_id, subnets.segment_id as subnets_segment_id, subnets.subnetpool_id as subnets_subnetpool_id, subnets.ip_version as subnets_ip_version, subnets.cidr as subnets_cidr, subnets.gateway_ip as subnets_gateway_ip, subnets.enable_dhcp as subnets_enable_dhcp, subnets.ipv6_ra_mode as subnets_ipv6_ra_mode, subnets.ipv6_address_mode as subnets_ipv6_address_mode, subnets.standard_attr_id as subnets_standard_attr_id, subnetpools_1.shared as subnetpools_1_shared, standardattributes_1.id as standardattributes_1_id, standardattributes_1.resource_type as standardattributes_1_resource_type, standardattributes_1.description as standardattributes_1_description, standardattributes_1.revision_number as standardattributes_1_revision_number, standardattributes_1.created_at as standardattributes_1_cre^c"
    },
    {
        "url": "https://stackoverflow.com/questions/72608811",
        "text": "sudo microstack init auto compute join connection string microstack_init configuring clustering ... microstack_init setting up as a compute node. microstack_init tls certificates must be provided: config.tls.cacert path, config.tls.cer.path, and config.tls.key.path. traceback most recent call last file line in module load_entry_point file line in main cmd file line in wrapper return func file line in init question.ask file line in ask self.no awr file line in no restart file line in restart check file line in check raise subprocess.calledprocesserror proc.retunrcode, .join args subprocess.calledprocesserror: command snapctl restart microstack.nginx returned non zero exit status"
    },
    {
        "url": "https://stackoverflow.com/questions/72721636",
        "text": "juju.cmd.juju.commands bootstrap.go:884 failed to bootstrap model: cannot start bootstrap instance: no metadata for images in microstack with arch"
    },
    {
        "url": "https://stackoverflow.com/questions/72955135",
        "text": "cat nova.console.websocketproxy in exit nova.console.websocketproxy websocket server settings: nova.console.websocketproxy listen on nova.console.websocketproxy web server no directory listings . web root: nova.console.websocketproxy ssl tls support nova.console.websocketproxy proxying from to none:none nova.console.websocketproxy ssl tls wss: websocket connection nova.console.websocketproxy path: nova.compute.rpcapi automatically selected compute rpc version from minimum service version nova.console.websocketproxy connect info: consoleauthtoken access_url_base= nova.console.websocketproxy connecting to: nova.console.websocketproxy handler exception: errno econnrefused nova.console.websocketproxy handler exception: ssl: unexpected_eof_while_reading unexpected eof while reading _ssl.c:997 nova.console.websocketproxy ssl tls wss: websocket connection nova.console.websocketproxy path: nova.console.websocketproxy handler exception: the token is invalid or has expired"
    },
    {
        "url": "https://stackoverflow.com/questions/72969573",
        "text": "nova.compute.manager instance: took seconds to destroy the instance on the hypervisor. nova.compute.manager instance: failed to allocate network s : nova.exception.virtualinterfacecreateexception: virtual interface creation failed nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _create_guest_with_network nova.compute.manager instance: guest self._create_guest nova.compute.manager instance: file line in __exit__ nova.compute.manager instance: next self.gen nova.compute.manager instance: file line in wait_for_instance_event nova.compute.manager instance: self._wait_for_instance_events nova.compute.manager instance: file line in _wait_for_instance_events nova.compute.manager instance: actual_event event.wait nova.compute.manager instance: file line in wait nova.compute.manager instance: instance_event self.event.wait nova.compute.manager instance: file line in wait nova.compute.manager instance: result hub.switch nova.compute.manager instance: file line in switch nova.compute.manager instance: return self.greenlet.switch nova.compute.manager instance: eventlet.timeout.timeout: seconds nova.compute.manager instance: nova.compute.manager instance: during handling of the above exception, another exception occurred: nova.compute.manager instance: nova.compute.manager instance: traceback most recent call last : nova.compute.manager instance: file line in _build_and_run_instance nova.compute.manager instance: self.driver.spawn context, instance, image_meta, nova.compute.manager instance: file line in spawn nova.compute.manager instance: self._create_guest_with_network nova.compute.manager instance: file line in _create_guest_with_network nova.compute.manager instance: raise exception.virtualinterfacecreateexception nova.compute.manager instance: nova.exception.virtualinterfacecreateexception: virtual interface creation failed nova.compute.manager instance: nova.compute.manager instance: build of instance aborted: failed to allocate the network s , not rescheduling.: nova.exception.buildabortexception: build of instance aborted: failed to allocate the network s , not rescheduling. os_vif successfully unplugged vif vifbridge ? ,preserve_on_delete=false,vif_name= nova.compute.manager instance: took seconds to deallocate network for instance."
    },
    {
        "url": "https://stackoverflow.com/questions/74030350",
        "text": "ott ibisco nova conductor : debug oslo_concurrency.lockutils lock by nova.context.set_target_cell. locals .get_or_set_cached_cell_and_set_connections :: held inner ott ibisco nova conductor : debug oslo_concurrency.lockutils lock acquired by nova.context.set_target_cell. locals .get_or_set_cached_cell_and_set_connections :: waited inner ott ibisco nova conductor : debug oslo_concurrency.lockutils lock by nova.context.set_target_cell. locals .get_or_set_cached_cell_and_set_connections :: held inner ott ibisco nova conductor : debug oslo_concurrency.lockutils acquired lock lock ott ibisco nova conductor : debug oslo_concurrency.lockutils releasing lock lock ott ibisco nova conductor : oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused ott ibisco nova conductor : oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused ott ibisco nova conductor : oslo.messaging._drivers.impl_rabbit connection failed: errno econnrefused retrying in seconds : connectionrefusederror: errno econnrefused"
    },
    {
        "url": "https://stackoverflow.com/questions/74062923",
        "text": "i am seeing below when i try to spawn a instance on vpp with dpdk and openstack openstack server create flavor csr small image csr nic net vpp suresh oct nova compute : debug nova.virt.libvirt.vif none demo demo vif_type=vhostuser access_ip_v4=none,access_ip_v6=none,architecture=none,auto_disk_config=false,availability_zone= ? ? ,shutdown_terminate=false,system_metadata= null, , , , , , false, false , null, null, null, false, , false, unplug oct nova compute : nova.compute.manager none demo demo instance: failed to allocate network s : nova.exception.virtualinterfacecreateexception: virtual interface creation failed oct nova compute : nova.compute.manager instance: traceback most recent call last : oct nova compute : nova.compute.manager instance: file line in _create_guest_with_network oct nova compute : nova.compute.manager instance: guest self._create_guest oct nova compute : nova.compute.manager instance: file line in __exit__ oct nova compute : nova.compute.manager instance: next self.gen oct nova compute : nova.compute.manager instance: file line in wait_for_instance_event oct nova compute : nova.compute.manager instance: actual_event event.wait oct nova compute : nova.compute.manager instance: file line in wait oct nova compute : nova.compute.manager instance: result hub.switch oct nova compute : nova.compute.manager instance: file line in switch oct nova compute : nova.compute.manager instance: return self.greenlet.switch oct nova compute : nova.compute.manager instance: eventlet.timeout.timeout: seconds oct nova compute : nova.compute.manager instance: oct nova compute : nova.compute.manager instance: during handling of the above exception, another exception occurred: oct nova compute : nova.compute.manager instance: oct nova compute : nova.compute.manager instance: traceback most recent call last : oct nova compute : nova.compute.manager instance: file line in _build_and_run_instance oct nova compute : nova.compute.manager instance: self.driver.spawn context, instance, image_meta, oct nova compute : nova.compute.manager instance: file line in spawn oct nova compute : nova.compute.manager instance: self._create_guest_with_network oct nova compute : nova.compute.manager instance: file line in _create_guest_with_network oct nova compute : nova.compute.manager instance: raise exception.virtualinterfacecreateexception oct nova compute : nova.compute.manager instance: nova.exception.virtualinterfacecreateexception: virtual interface creation failed oct nova compute : nova.compute.manager instance: sudo vppctl show dpdk phy segment iova:0x7f683fe00000, len:2097152, virt:0x7f683fe00000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:365 segment iova:0x7f6840000000, len:2097152, virt:0x7f6840000000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:370 segment iova:0x7f6840200000, len:2097152, virt:0x7f6840200000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:371 segment iova:0x7f6840400000, len:2097152, virt:0x7f6840400000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:376 segment iova:0x7f6840600000, len:2097152, virt:0x7f6840600000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:377 segment iova:0x7f6840800000, len:2097152, virt:0x7f6840800000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:382 segment iova:0x7f6840a00000, len:2097152, virt:0x7f6840a00000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:383 segment iova:0x7f6840c00000, len:2097152, virt:0x7f6840c00000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:388 segment iova:0x7f6840e00000, len:2097152, virt:0x7f6840e00000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:389 segment iova:0x7f6841000000, len:2097152, virt:0x7f6841000000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:390 segment iova:0x7f6841200000, len:2097152, virt:0x7f6841200000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:393 segment iova:0x7f6841400000, len:2097152, virt:0x7f6841400000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:394 segment iova:0x7f6841600000, len:2097152, virt:0x7f6841600000, socket_id:0, hugepage_sz:2097152, nchannel:0, nrank:0 fd:395 segment iova:0x7f583f600000, len:2097152, virt:0x7f583f600000, socket_id:1, hugepage_sz:2097152, nchannel:0, nrank:0 fd:391 cat unix nodaemon log full coredump startup config cli listen gid vpp api trace on api segment gid vpp socksvr dpdk dev num rx queues num tx queues uio driver vfio pci dev dev dev dev socket mem memory main heap size main heap page size hugepage statseg size page size hugepage per node counters off grep huge anonhugepages: kb shmemhugepages: kb filehugepages: kb hugepages_total: hugepages_free: hugepages_rsvd: hugepages_surp: hugepagesize: kb hugetlb: kb"
    },
    {
        "url": "https://stackoverflow.com/questions/74110275",
        "text": "oslo.service.loopingcall raise exception.brickexception _ nfs mount failed for share sh s. oslo.service.loopingcall os_brick.exception.brickexception: nfs mount failed for share unexpected while running command.\\ncommand: mount t nfs o vers:3 code: mount.nfs: parsing on"
    },
    {
        "url": "https://stackoverflow.com/questions/74286142",
        "text": "warn org.ovirt.engine.core.bll.provider.network.openstack.basenetworkproviderproxy ee managedthreadfactory engine thread host binding id for external network vlan on host ovn comp nodes is null, using host id to allocate vnic instead. please provide an after_get_caps hook for the plugin type open_vswitch on host ovn comp nodes org.ovirt.engine.core.dal.dbbroker.auditloghandling.auditlogdirector forkjoinpool worker event_id: vm_down_error 119 , vm ovn comp is down with error. exit message: cannot get interface mtu on no such device."
    }
]